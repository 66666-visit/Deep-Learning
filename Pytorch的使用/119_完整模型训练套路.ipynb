{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CIFAR 10 model 网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 下面用 CIFAR 10 model网络来完成分类问题，网络模型如下图所示。"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DataLoader加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: D:\\深度学习\\100_土堆数据集\\dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10('D:\\\\深度学习\\\\100_土堆数据集\\\\dataset',train=True,transform=torchvision.transforms.ToTensor())       \n",
    "test_data = torchvision.datasets.CIFAR10('D:\\\\深度学习\\\\100_土堆数据集\\\\dataset',train=False,transform=torchvision.transforms.ToTensor())       \n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data_size, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data_size, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 测试网络正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ==========================================\n",
    "# 1. 定义模型类\n",
    "# ==========================================\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # nn.Sequential 是一个容器，它会按照我们在里面写的顺序，\n",
    "        # 自动把数据依次传给每一层。\n",
    "        # 好处：在 forward 函数里不用写一长串 x=conv1(x), x=pool1(x)...\n",
    "        self.model1 = nn.Sequential(\n",
    "            # --- 第1组：卷积 + 池化 ---\n",
    "            # 输入: (Batch, 3, 32, 32) -> RGB 3通道\n",
    "            # 卷积计算: H_out = (32 + 2*2 - 5) / 1 + 1 = 32\n",
    "            # 输出: (Batch, 32, 32, 32) -> 通道变32，尺寸不变\n",
    "            nn.Conv2d(3, 32, 5, 1, 2),  # in=3, out=32, k=5, s=1, p=2\n",
    "            \n",
    "            # 池化计算: 32 / 2 = 16\n",
    "            # 输出: (Batch, 32, 16, 16) -> 尺寸减半\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 第2组：卷积 + 池化 ---\n",
    "            # 输入: (Batch, 32, 16, 16)\n",
    "            # 卷积计算: (16 + 2*2 - 5) / 1 + 1 = 16\n",
    "            # 输出: (Batch, 32, 16, 16) -> 尺寸不变\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),\n",
    "            \n",
    "            # 池化计算: 16 / 2 = 8\n",
    "            # 输出: (Batch, 32, 8, 8) -> 尺寸减半\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 第3组：卷积 + 池化 ---\n",
    "            # 输入: (Batch, 32, 8, 8)\n",
    "            # 卷积计算: (8 + 2*2 - 5) / 1 + 1 = 8\n",
    "            # 输出: (Batch, 64, 8, 8) -> 通道变64 (特征变多了)，尺寸不变\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            \n",
    "            # 池化计算: 8 / 2 = 4\n",
    "            # 输出: (Batch, 64, 4, 4) -> 尺寸再次减半。这是进 Linear 前的最终特征图。\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 展平层 ---\n",
    "            # 作用: 把立体的特征图拍扁成一维向量，以便放入全连接层。\n",
    "            # 变化: (Batch, 64, 4, 4) -> (Batch, 64*4*4) 即 (Batch, 1024)\n",
    "            nn.Flatten(), \n",
    "            \n",
    "            # --- 全连接层 ---\n",
    "            # 输入维度: 1024 (必须和上面展平后的长度一致)\n",
    "            # 输出维度: 64 (中间层节点数)\n",
    "            nn.Linear(64*4*4, 64),\n",
    "            \n",
    "            # --- 输出层 ---\n",
    "            # 输出维度: 10 (对应 CIFAR10 的 10 个分类)\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 因为用了 Sequential，这里一行代码就搞定了所有层的前向传播\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "    \n",
    "# ==========================================\n",
    "# 2. 自测代码 (Sanity Check)\n",
    "# ==========================================\n",
    "if __name__ == '__main__':\n",
    "    # 实例化模型\n",
    "    tudui = Tudui()\n",
    "    \n",
    "    # 创建一个模拟的输入数据 (假数据)\n",
    "    # 形状: (Batch_Size=64, Channels=3, Height=32, Width=32)\n",
    "    # 作用: 用来验证网络能不能跑通，输出尺寸对不对。\n",
    "    input = torch.ones((64, 3, 32, 32))\n",
    "    \n",
    "    # 把假数据喂给模型\n",
    "    output = tudui(input)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    # 预期结果: torch.Size([64, 10])\n",
    "    # 解释: 64 是 batch_size (每张图对应一个结果)，10 是分类的概率/分数\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__name__ == '__main__' 这句话是 Python 中非常经典的一个**“开关”**，全称叫 “脚本入口判断”。\n",
    "\n",
    "简单来说，它的作用是：防止你写的测试代码在被别人调用（import）时意外运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 网络训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "训练次数：1,Loss:2.3087823390960693\n",
      "训练次数：2,Loss:2.315652847290039\n",
      "训练次数：3,Loss:2.3006820678710938\n",
      "训练次数：4,Loss:2.318735361099243\n",
      "训练次数：5,Loss:2.308865785598755\n",
      "训练次数：6,Loss:2.2973833084106445\n",
      "训练次数：7,Loss:2.299346446990967\n",
      "训练次数：8,Loss:2.306925058364868\n",
      "训练次数：9,Loss:2.301213264465332\n",
      "训练次数：10,Loss:2.3110625743865967\n",
      "训练次数：11,Loss:2.293025016784668\n",
      "训练次数：12,Loss:2.2931923866271973\n",
      "训练次数：13,Loss:2.306326389312744\n",
      "训练次数：14,Loss:2.316879987716675\n",
      "训练次数：15,Loss:2.2928855419158936\n",
      "训练次数：16,Loss:2.302217483520508\n",
      "训练次数：17,Loss:2.310936689376831\n",
      "训练次数：18,Loss:2.299715280532837\n",
      "训练次数：19,Loss:2.288522481918335\n",
      "训练次数：20,Loss:2.297100782394409\n",
      "训练次数：21,Loss:2.299164056777954\n",
      "训练次数：22,Loss:2.3035502433776855\n",
      "训练次数：23,Loss:2.299276828765869\n",
      "训练次数：24,Loss:2.2942616939544678\n",
      "训练次数：25,Loss:2.3115763664245605\n",
      "训练次数：26,Loss:2.295483350753784\n",
      "训练次数：27,Loss:2.3000893592834473\n",
      "训练次数：28,Loss:2.3025448322296143\n",
      "训练次数：29,Loss:2.2953429222106934\n",
      "训练次数：30,Loss:2.294612169265747\n",
      "训练次数：31,Loss:2.2944111824035645\n",
      "训练次数：32,Loss:2.298386573791504\n",
      "训练次数：33,Loss:2.299421548843384\n",
      "训练次数：34,Loss:2.3025379180908203\n",
      "训练次数：35,Loss:2.300135850906372\n",
      "训练次数：36,Loss:2.2988321781158447\n",
      "训练次数：37,Loss:2.2973878383636475\n",
      "训练次数：38,Loss:2.297076463699341\n",
      "训练次数：39,Loss:2.294724702835083\n",
      "训练次数：40,Loss:2.299271583557129\n",
      "训练次数：41,Loss:2.3092281818389893\n",
      "训练次数：42,Loss:2.2969954013824463\n",
      "训练次数：43,Loss:2.286691427230835\n",
      "训练次数：44,Loss:2.30863356590271\n",
      "训练次数：45,Loss:2.301312208175659\n",
      "训练次数：46,Loss:2.298949956893921\n",
      "训练次数：47,Loss:2.2943668365478516\n",
      "训练次数：48,Loss:2.2927298545837402\n",
      "训练次数：49,Loss:2.303239583969116\n",
      "训练次数：50,Loss:2.286595344543457\n",
      "训练次数：51,Loss:2.297335386276245\n",
      "训练次数：52,Loss:2.2911593914031982\n",
      "训练次数：53,Loss:2.293118953704834\n",
      "训练次数：54,Loss:2.293041706085205\n",
      "训练次数：55,Loss:2.2908151149749756\n",
      "训练次数：56,Loss:2.2919890880584717\n",
      "训练次数：57,Loss:2.2894089221954346\n",
      "训练次数：58,Loss:2.294560194015503\n",
      "训练次数：59,Loss:2.2983596324920654\n",
      "训练次数：60,Loss:2.3022851943969727\n",
      "训练次数：61,Loss:2.2943272590637207\n",
      "训练次数：62,Loss:2.296375274658203\n",
      "训练次数：63,Loss:2.295241355895996\n",
      "训练次数：64,Loss:2.297395944595337\n",
      "训练次数：65,Loss:2.3036861419677734\n",
      "训练次数：66,Loss:2.2930102348327637\n",
      "训练次数：67,Loss:2.2881546020507812\n",
      "训练次数：68,Loss:2.295994520187378\n",
      "训练次数：69,Loss:2.299988031387329\n",
      "训练次数：70,Loss:2.299372911453247\n",
      "训练次数：71,Loss:2.2922236919403076\n",
      "训练次数：72,Loss:2.2917559146881104\n",
      "训练次数：73,Loss:2.297339677810669\n",
      "训练次数：74,Loss:2.2799792289733887\n",
      "训练次数：75,Loss:2.2995197772979736\n",
      "训练次数：76,Loss:2.289031744003296\n",
      "训练次数：77,Loss:2.293022632598877\n",
      "训练次数：78,Loss:2.297321319580078\n",
      "训练次数：79,Loss:2.291590452194214\n",
      "训练次数：80,Loss:2.300152063369751\n",
      "训练次数：81,Loss:2.304978132247925\n",
      "训练次数：82,Loss:2.307929039001465\n",
      "训练次数：83,Loss:2.292079448699951\n",
      "训练次数：84,Loss:2.2831406593322754\n",
      "训练次数：85,Loss:2.2922229766845703\n",
      "训练次数：86,Loss:2.285994291305542\n",
      "训练次数：87,Loss:2.289889097213745\n",
      "训练次数：88,Loss:2.2939016819000244\n",
      "训练次数：89,Loss:2.2793445587158203\n",
      "训练次数：90,Loss:2.288538932800293\n",
      "训练次数：91,Loss:2.2946388721466064\n",
      "训练次数：92,Loss:2.2853078842163086\n",
      "训练次数：93,Loss:2.296200752258301\n",
      "训练次数：94,Loss:2.268657684326172\n",
      "训练次数：95,Loss:2.2952346801757812\n",
      "训练次数：96,Loss:2.283226728439331\n",
      "训练次数：97,Loss:2.291757345199585\n",
      "训练次数：98,Loss:2.2990245819091797\n",
      "训练次数：99,Loss:2.2995212078094482\n",
      "训练次数：100,Loss:2.2853689193725586\n",
      "训练次数：101,Loss:2.299600124359131\n",
      "训练次数：102,Loss:2.3006131649017334\n",
      "训练次数：103,Loss:2.2996973991394043\n",
      "训练次数：104,Loss:2.293529987335205\n",
      "训练次数：105,Loss:2.295356273651123\n",
      "训练次数：106,Loss:2.306271553039551\n",
      "训练次数：107,Loss:2.3009071350097656\n",
      "训练次数：108,Loss:2.3027987480163574\n",
      "训练次数：109,Loss:2.296907663345337\n",
      "训练次数：110,Loss:2.290172576904297\n",
      "训练次数：111,Loss:2.290727138519287\n",
      "训练次数：112,Loss:2.2940940856933594\n",
      "训练次数：113,Loss:2.2899060249328613\n",
      "训练次数：114,Loss:2.295118570327759\n",
      "训练次数：115,Loss:2.302767515182495\n",
      "训练次数：116,Loss:2.3023581504821777\n",
      "训练次数：117,Loss:2.286893606185913\n",
      "训练次数：118,Loss:2.279095411300659\n",
      "训练次数：119,Loss:2.29433274269104\n",
      "训练次数：120,Loss:2.28576397895813\n",
      "训练次数：121,Loss:2.3011202812194824\n",
      "训练次数：122,Loss:2.279143810272217\n",
      "训练次数：123,Loss:2.2973294258117676\n",
      "训练次数：124,Loss:2.2903401851654053\n",
      "训练次数：125,Loss:2.2842366695404053\n",
      "训练次数：126,Loss:2.2876625061035156\n",
      "训练次数：127,Loss:2.2803213596343994\n",
      "训练次数：128,Loss:2.288684129714966\n",
      "训练次数：129,Loss:2.2874279022216797\n",
      "训练次数：130,Loss:2.2743940353393555\n",
      "训练次数：131,Loss:2.2863168716430664\n",
      "训练次数：132,Loss:2.2803099155426025\n",
      "训练次数：133,Loss:2.2988829612731934\n",
      "训练次数：134,Loss:2.280926465988159\n",
      "训练次数：135,Loss:2.2976925373077393\n",
      "训练次数：136,Loss:2.2737574577331543\n",
      "训练次数：137,Loss:2.285353899002075\n",
      "训练次数：138,Loss:2.286916732788086\n",
      "训练次数：139,Loss:2.2707009315490723\n",
      "训练次数：140,Loss:2.2905735969543457\n",
      "训练次数：141,Loss:2.2936811447143555\n",
      "训练次数：142,Loss:2.2827935218811035\n",
      "训练次数：143,Loss:2.2879798412323\n",
      "训练次数：144,Loss:2.286802291870117\n",
      "训练次数：145,Loss:2.2899739742279053\n",
      "训练次数：146,Loss:2.2795705795288086\n",
      "训练次数：147,Loss:2.281233072280884\n",
      "训练次数：148,Loss:2.276254177093506\n",
      "训练次数：149,Loss:2.2793118953704834\n",
      "训练次数：150,Loss:2.299508571624756\n",
      "训练次数：151,Loss:2.275451898574829\n",
      "训练次数：152,Loss:2.267436981201172\n",
      "训练次数：153,Loss:2.3150484561920166\n",
      "训练次数：154,Loss:2.2944679260253906\n",
      "训练次数：155,Loss:2.271930456161499\n",
      "训练次数：156,Loss:2.2906665802001953\n",
      "训练次数：157,Loss:2.2833638191223145\n",
      "训练次数：158,Loss:2.2880961894989014\n",
      "训练次数：159,Loss:2.2760958671569824\n",
      "训练次数：160,Loss:2.29972767829895\n",
      "训练次数：161,Loss:2.2668516635894775\n",
      "训练次数：162,Loss:2.2650132179260254\n",
      "训练次数：163,Loss:2.2889809608459473\n",
      "训练次数：164,Loss:2.2840213775634766\n",
      "训练次数：165,Loss:2.2765913009643555\n",
      "训练次数：166,Loss:2.2758805751800537\n",
      "训练次数：167,Loss:2.261991024017334\n",
      "训练次数：168,Loss:2.2838594913482666\n",
      "训练次数：169,Loss:2.2640321254730225\n",
      "训练次数：170,Loss:2.272937536239624\n",
      "训练次数：171,Loss:2.28546142578125\n",
      "训练次数：172,Loss:2.266746997833252\n",
      "训练次数：173,Loss:2.2570626735687256\n",
      "训练次数：174,Loss:2.2886528968811035\n",
      "训练次数：175,Loss:2.27937650680542\n",
      "训练次数：176,Loss:2.254981756210327\n",
      "训练次数：177,Loss:2.2620840072631836\n",
      "训练次数：178,Loss:2.2924704551696777\n",
      "训练次数：179,Loss:2.3043196201324463\n",
      "训练次数：180,Loss:2.281409978866577\n",
      "训练次数：181,Loss:2.2750346660614014\n",
      "训练次数：182,Loss:2.2766432762145996\n",
      "训练次数：183,Loss:2.271796226501465\n",
      "训练次数：184,Loss:2.2829346656799316\n",
      "训练次数：185,Loss:2.2962045669555664\n",
      "训练次数：186,Loss:2.288925886154175\n",
      "训练次数：187,Loss:2.268627405166626\n",
      "训练次数：188,Loss:2.273634433746338\n",
      "训练次数：189,Loss:2.263110876083374\n",
      "训练次数：190,Loss:2.2813262939453125\n",
      "训练次数：191,Loss:2.267115831375122\n",
      "训练次数：192,Loss:2.2544732093811035\n",
      "训练次数：193,Loss:2.286741018295288\n",
      "训练次数：194,Loss:2.2724006175994873\n",
      "训练次数：195,Loss:2.2561914920806885\n",
      "训练次数：196,Loss:2.2670066356658936\n",
      "训练次数：197,Loss:2.292672634124756\n",
      "训练次数：198,Loss:2.2663936614990234\n",
      "训练次数：199,Loss:2.2932069301605225\n",
      "训练次数：200,Loss:2.271672248840332\n",
      "训练次数：201,Loss:2.257319211959839\n",
      "训练次数：202,Loss:2.263246536254883\n",
      "训练次数：203,Loss:2.2773735523223877\n",
      "训练次数：204,Loss:2.2658755779266357\n",
      "训练次数：205,Loss:2.2377703189849854\n",
      "训练次数：206,Loss:2.3036487102508545\n",
      "训练次数：207,Loss:2.2720727920532227\n",
      "训练次数：208,Loss:2.280848503112793\n",
      "训练次数：209,Loss:2.2544126510620117\n",
      "训练次数：210,Loss:2.2750802040100098\n",
      "训练次数：211,Loss:2.2635810375213623\n",
      "训练次数：212,Loss:2.2715179920196533\n",
      "训练次数：213,Loss:2.2379653453826904\n",
      "训练次数：214,Loss:2.2618513107299805\n",
      "训练次数：215,Loss:2.2549149990081787\n",
      "训练次数：216,Loss:2.2956831455230713\n",
      "训练次数：217,Loss:2.2528622150421143\n",
      "训练次数：218,Loss:2.2846968173980713\n",
      "训练次数：219,Loss:2.2870306968688965\n",
      "训练次数：220,Loss:2.265846014022827\n",
      "训练次数：221,Loss:2.284980297088623\n",
      "训练次数：222,Loss:2.262322425842285\n",
      "训练次数：223,Loss:2.2871694564819336\n",
      "训练次数：224,Loss:2.261331081390381\n",
      "训练次数：225,Loss:2.285703182220459\n",
      "训练次数：226,Loss:2.274123430252075\n",
      "训练次数：227,Loss:2.262260913848877\n",
      "训练次数：228,Loss:2.2452502250671387\n",
      "训练次数：229,Loss:2.2706966400146484\n",
      "训练次数：230,Loss:2.274610996246338\n",
      "训练次数：231,Loss:2.264843225479126\n",
      "训练次数：232,Loss:2.2610111236572266\n",
      "训练次数：233,Loss:2.265082597732544\n",
      "训练次数：234,Loss:2.286792755126953\n",
      "训练次数：235,Loss:2.281087875366211\n",
      "训练次数：236,Loss:2.274777889251709\n",
      "训练次数：237,Loss:2.2776923179626465\n",
      "训练次数：238,Loss:2.2771189212799072\n",
      "训练次数：239,Loss:2.2556512355804443\n",
      "训练次数：240,Loss:2.2583718299865723\n",
      "训练次数：241,Loss:2.2858846187591553\n",
      "训练次数：242,Loss:2.2754745483398438\n",
      "训练次数：243,Loss:2.2542309761047363\n",
      "训练次数：244,Loss:2.2690589427948\n",
      "训练次数：245,Loss:2.2467148303985596\n",
      "训练次数：246,Loss:2.2715766429901123\n",
      "训练次数：247,Loss:2.280000686645508\n",
      "训练次数：248,Loss:2.251328468322754\n",
      "训练次数：249,Loss:2.253912925720215\n",
      "训练次数：250,Loss:2.27280592918396\n",
      "训练次数：251,Loss:2.257122755050659\n",
      "训练次数：252,Loss:2.2345736026763916\n",
      "训练次数：253,Loss:2.245725631713867\n",
      "训练次数：254,Loss:2.2415716648101807\n",
      "训练次数：255,Loss:2.2560763359069824\n",
      "训练次数：256,Loss:2.2675766944885254\n",
      "训练次数：257,Loss:2.263721466064453\n",
      "训练次数：258,Loss:2.2476727962493896\n",
      "训练次数：259,Loss:2.2353577613830566\n",
      "训练次数：260,Loss:2.2678062915802\n",
      "训练次数：261,Loss:2.255251169204712\n",
      "训练次数：262,Loss:2.2447311878204346\n",
      "训练次数：263,Loss:2.2634832859039307\n",
      "训练次数：264,Loss:2.282944679260254\n",
      "训练次数：265,Loss:2.2577853202819824\n",
      "训练次数：266,Loss:2.239276885986328\n",
      "训练次数：267,Loss:2.2494025230407715\n",
      "训练次数：268,Loss:2.255157709121704\n",
      "训练次数：269,Loss:2.246438503265381\n",
      "训练次数：270,Loss:2.215705394744873\n",
      "训练次数：271,Loss:2.2739851474761963\n",
      "训练次数：272,Loss:2.2351629734039307\n",
      "训练次数：273,Loss:2.225470781326294\n",
      "训练次数：274,Loss:2.2370622158050537\n",
      "训练次数：275,Loss:2.284421920776367\n",
      "训练次数：276,Loss:2.266433000564575\n",
      "训练次数：277,Loss:2.2813005447387695\n",
      "训练次数：278,Loss:2.239582061767578\n",
      "训练次数：279,Loss:2.2587890625\n",
      "训练次数：280,Loss:2.2315876483917236\n",
      "训练次数：281,Loss:2.217210054397583\n",
      "训练次数：282,Loss:2.2508018016815186\n",
      "训练次数：283,Loss:2.249823570251465\n",
      "训练次数：284,Loss:2.245941638946533\n",
      "训练次数：285,Loss:2.2175915241241455\n",
      "训练次数：286,Loss:2.2383036613464355\n",
      "训练次数：287,Loss:2.2563295364379883\n",
      "训练次数：288,Loss:2.214890241622925\n",
      "训练次数：289,Loss:2.2616500854492188\n",
      "训练次数：290,Loss:2.2337591648101807\n",
      "训练次数：291,Loss:2.2265079021453857\n",
      "训练次数：292,Loss:2.2643048763275146\n",
      "训练次数：293,Loss:2.219703197479248\n",
      "训练次数：294,Loss:2.1965131759643555\n",
      "训练次数：295,Loss:2.231767177581787\n",
      "训练次数：296,Loss:2.2353506088256836\n",
      "训练次数：297,Loss:2.253988027572632\n",
      "训练次数：298,Loss:2.218229293823242\n",
      "训练次数：299,Loss:2.1751773357391357\n",
      "训练次数：300,Loss:2.235706329345703\n",
      "训练次数：301,Loss:2.240135908126831\n",
      "训练次数：302,Loss:2.2413952350616455\n",
      "训练次数：303,Loss:2.2249233722686768\n",
      "训练次数：304,Loss:2.2114648818969727\n",
      "训练次数：305,Loss:2.2268364429473877\n",
      "训练次数：306,Loss:2.233128070831299\n",
      "训练次数：307,Loss:2.189507484436035\n",
      "训练次数：308,Loss:2.220254421234131\n",
      "训练次数：309,Loss:2.1929056644439697\n",
      "训练次数：310,Loss:2.238978147506714\n",
      "训练次数：311,Loss:2.2187681198120117\n",
      "训练次数：312,Loss:2.2174816131591797\n",
      "训练次数：313,Loss:2.227849006652832\n",
      "训练次数：314,Loss:2.228398084640503\n",
      "训练次数：315,Loss:2.1984241008758545\n",
      "训练次数：316,Loss:2.2351150512695312\n",
      "训练次数：317,Loss:2.1669020652770996\n",
      "训练次数：318,Loss:2.22485089302063\n",
      "训练次数：319,Loss:2.166731119155884\n",
      "训练次数：320,Loss:2.1897127628326416\n",
      "训练次数：321,Loss:2.1884474754333496\n",
      "训练次数：322,Loss:2.2424304485321045\n",
      "训练次数：323,Loss:2.2013661861419678\n",
      "训练次数：324,Loss:2.197117567062378\n",
      "训练次数：325,Loss:2.2187998294830322\n",
      "训练次数：326,Loss:2.159738302230835\n",
      "训练次数：327,Loss:2.2250826358795166\n",
      "训练次数：328,Loss:2.163681983947754\n",
      "训练次数：329,Loss:2.173900604248047\n",
      "训练次数：330,Loss:2.231877088546753\n",
      "训练次数：331,Loss:2.1700801849365234\n",
      "训练次数：332,Loss:2.200104236602783\n",
      "训练次数：333,Loss:2.125561237335205\n",
      "训练次数：334,Loss:2.212307929992676\n",
      "训练次数：335,Loss:2.1720025539398193\n",
      "训练次数：336,Loss:2.1516036987304688\n",
      "训练次数：337,Loss:2.181425094604492\n",
      "训练次数：338,Loss:2.2026822566986084\n",
      "训练次数：339,Loss:2.1752729415893555\n",
      "训练次数：340,Loss:2.132033348083496\n",
      "训练次数：341,Loss:2.170919895172119\n",
      "训练次数：342,Loss:2.179232120513916\n",
      "训练次数：343,Loss:2.1770851612091064\n",
      "训练次数：344,Loss:2.211730480194092\n",
      "训练次数：345,Loss:2.1862711906433105\n",
      "训练次数：346,Loss:2.128304958343506\n",
      "训练次数：347,Loss:2.1978917121887207\n",
      "训练次数：348,Loss:2.191013813018799\n",
      "训练次数：349,Loss:2.1753153800964355\n",
      "训练次数：350,Loss:2.1651134490966797\n",
      "训练次数：351,Loss:2.1479732990264893\n",
      "训练次数：352,Loss:2.170407772064209\n",
      "训练次数：353,Loss:2.142184019088745\n",
      "训练次数：354,Loss:2.1589906215667725\n",
      "训练次数：355,Loss:2.0875635147094727\n",
      "训练次数：356,Loss:2.248972177505493\n",
      "训练次数：357,Loss:2.1304244995117188\n",
      "训练次数：358,Loss:2.144120454788208\n",
      "训练次数：359,Loss:2.1775224208831787\n",
      "训练次数：360,Loss:2.1750802993774414\n",
      "训练次数：361,Loss:2.143052577972412\n",
      "训练次数：362,Loss:2.056671380996704\n",
      "训练次数：363,Loss:2.1673500537872314\n",
      "训练次数：364,Loss:2.1742424964904785\n",
      "训练次数：365,Loss:2.1352362632751465\n",
      "训练次数：366,Loss:2.0966153144836426\n",
      "训练次数：367,Loss:2.1262173652648926\n",
      "训练次数：368,Loss:2.19944167137146\n",
      "训练次数：369,Loss:2.0663647651672363\n",
      "训练次数：370,Loss:2.1276683807373047\n",
      "训练次数：371,Loss:2.089689016342163\n",
      "训练次数：372,Loss:2.081376075744629\n",
      "训练次数：373,Loss:2.0724563598632812\n",
      "训练次数：374,Loss:2.1304337978363037\n",
      "训练次数：375,Loss:2.099827527999878\n",
      "训练次数：376,Loss:2.1349198818206787\n",
      "训练次数：377,Loss:2.0892977714538574\n",
      "训练次数：378,Loss:2.033017635345459\n",
      "训练次数：379,Loss:2.2013626098632812\n",
      "训练次数：380,Loss:2.120922088623047\n",
      "训练次数：381,Loss:2.1561334133148193\n",
      "训练次数：382,Loss:2.1169748306274414\n",
      "训练次数：383,Loss:2.0974180698394775\n",
      "训练次数：384,Loss:2.060875654220581\n",
      "训练次数：385,Loss:2.0954513549804688\n",
      "训练次数：386,Loss:2.087409496307373\n",
      "训练次数：387,Loss:2.037564516067505\n",
      "训练次数：388,Loss:2.1850404739379883\n",
      "训练次数：389,Loss:2.1222376823425293\n",
      "训练次数：390,Loss:2.061359167098999\n",
      "训练次数：391,Loss:2.1382534503936768\n",
      "训练次数：392,Loss:2.1586062908172607\n",
      "训练次数：393,Loss:2.0543696880340576\n",
      "训练次数：394,Loss:2.0800728797912598\n",
      "训练次数：395,Loss:2.0080349445343018\n",
      "训练次数：396,Loss:2.110103130340576\n",
      "训练次数：397,Loss:2.058732509613037\n",
      "训练次数：398,Loss:2.134138822555542\n",
      "训练次数：399,Loss:1.9966747760772705\n",
      "训练次数：400,Loss:2.148861885070801\n",
      "训练次数：401,Loss:2.190626621246338\n",
      "训练次数：402,Loss:2.088454246520996\n",
      "训练次数：403,Loss:2.054910659790039\n",
      "训练次数：404,Loss:2.0521957874298096\n",
      "训练次数：405,Loss:2.137673854827881\n",
      "训练次数：406,Loss:2.0619113445281982\n",
      "训练次数：407,Loss:1.9907442331314087\n",
      "训练次数：408,Loss:1.9702774286270142\n",
      "训练次数：409,Loss:2.0415756702423096\n",
      "训练次数：410,Loss:1.9753962755203247\n",
      "训练次数：411,Loss:2.193939447402954\n",
      "训练次数：412,Loss:2.0478994846343994\n",
      "训练次数：413,Loss:2.1541082859039307\n",
      "训练次数：414,Loss:2.114108085632324\n",
      "训练次数：415,Loss:2.0515193939208984\n",
      "训练次数：416,Loss:2.0257692337036133\n",
      "训练次数：417,Loss:2.0765602588653564\n",
      "训练次数：418,Loss:1.9631233215332031\n",
      "训练次数：419,Loss:2.0855798721313477\n",
      "训练次数：420,Loss:2.1157994270324707\n",
      "训练次数：421,Loss:2.1240010261535645\n",
      "训练次数：422,Loss:2.2185184955596924\n",
      "训练次数：423,Loss:2.080754041671753\n",
      "训练次数：424,Loss:2.101201295852661\n",
      "训练次数：425,Loss:2.1081504821777344\n",
      "训练次数：426,Loss:2.1689751148223877\n",
      "训练次数：427,Loss:2.194956064224243\n",
      "训练次数：428,Loss:1.993831992149353\n",
      "训练次数：429,Loss:2.1402993202209473\n",
      "训练次数：430,Loss:2.0851893424987793\n",
      "训练次数：431,Loss:1.99885094165802\n",
      "训练次数：432,Loss:2.04720139503479\n",
      "训练次数：433,Loss:2.1153206825256348\n",
      "训练次数：434,Loss:2.0775651931762695\n",
      "训练次数：435,Loss:2.055445671081543\n",
      "训练次数：436,Loss:2.0450656414031982\n",
      "训练次数：437,Loss:2.013974666595459\n",
      "训练次数：438,Loss:2.078874349594116\n",
      "训练次数：439,Loss:2.1032071113586426\n",
      "训练次数：440,Loss:2.084214925765991\n",
      "训练次数：441,Loss:2.127293348312378\n",
      "训练次数：442,Loss:2.1231160163879395\n",
      "训练次数：443,Loss:2.1060256958007812\n",
      "训练次数：444,Loss:1.978449821472168\n",
      "训练次数：445,Loss:2.043346643447876\n",
      "训练次数：446,Loss:2.0961575508117676\n",
      "训练次数：447,Loss:2.055422067642212\n",
      "训练次数：448,Loss:2.1031439304351807\n",
      "训练次数：449,Loss:1.9583678245544434\n",
      "训练次数：450,Loss:2.1238906383514404\n",
      "训练次数：451,Loss:2.123089075088501\n",
      "训练次数：452,Loss:1.9598238468170166\n",
      "训练次数：453,Loss:2.089505672454834\n",
      "训练次数：454,Loss:2.088548421859741\n",
      "训练次数：455,Loss:2.1059329509735107\n",
      "训练次数：456,Loss:2.017951488494873\n",
      "训练次数：457,Loss:2.1149587631225586\n",
      "训练次数：458,Loss:2.078737735748291\n",
      "训练次数：459,Loss:2.044419765472412\n",
      "训练次数：460,Loss:2.014467239379883\n",
      "训练次数：461,Loss:2.0364723205566406\n",
      "训练次数：462,Loss:2.0872914791107178\n",
      "训练次数：463,Loss:1.985643982887268\n",
      "训练次数：464,Loss:2.09503436088562\n",
      "训练次数：465,Loss:1.9863229990005493\n",
      "训练次数：466,Loss:2.0544722080230713\n",
      "训练次数：467,Loss:2.0547282695770264\n",
      "训练次数：468,Loss:1.9591829776763916\n",
      "训练次数：469,Loss:2.0829288959503174\n",
      "训练次数：470,Loss:2.0318925380706787\n",
      "训练次数：471,Loss:1.9552555084228516\n",
      "训练次数：472,Loss:2.149381399154663\n",
      "训练次数：473,Loss:2.0419435501098633\n",
      "训练次数：474,Loss:2.0369694232940674\n",
      "训练次数：475,Loss:1.967218279838562\n",
      "训练次数：476,Loss:2.147510290145874\n",
      "训练次数：477,Loss:2.0326426029205322\n",
      "训练次数：478,Loss:2.118378162384033\n",
      "训练次数：479,Loss:2.157567262649536\n",
      "训练次数：480,Loss:2.012936592102051\n",
      "训练次数：481,Loss:2.12945556640625\n",
      "训练次数：482,Loss:2.143261432647705\n",
      "训练次数：483,Loss:2.0243160724639893\n",
      "训练次数：484,Loss:2.026188611984253\n",
      "训练次数：485,Loss:2.1051666736602783\n",
      "训练次数：486,Loss:2.0844995975494385\n",
      "训练次数：487,Loss:2.2679874897003174\n",
      "训练次数：488,Loss:2.1211116313934326\n",
      "训练次数：489,Loss:2.057668447494507\n",
      "训练次数：490,Loss:2.0249812602996826\n",
      "训练次数：491,Loss:2.09346342086792\n",
      "训练次数：492,Loss:2.004180669784546\n",
      "训练次数：493,Loss:2.0714516639709473\n",
      "训练次数：494,Loss:2.0049328804016113\n",
      "训练次数：495,Loss:2.0959925651550293\n",
      "训练次数：496,Loss:2.0425283908843994\n",
      "训练次数：497,Loss:2.0677599906921387\n",
      "训练次数：498,Loss:2.044275999069214\n",
      "训练次数：499,Loss:2.0930681228637695\n",
      "训练次数：500,Loss:2.0540080070495605\n",
      "训练次数：501,Loss:2.0712296962738037\n",
      "训练次数：502,Loss:2.043323040008545\n",
      "训练次数：503,Loss:1.9506816864013672\n",
      "训练次数：504,Loss:2.1405410766601562\n",
      "训练次数：505,Loss:2.0163815021514893\n",
      "训练次数：506,Loss:1.8762915134429932\n",
      "训练次数：507,Loss:2.0061728954315186\n",
      "训练次数：508,Loss:2.0753142833709717\n",
      "训练次数：509,Loss:2.154433012008667\n",
      "训练次数：510,Loss:2.1804816722869873\n",
      "训练次数：511,Loss:2.1123151779174805\n",
      "训练次数：512,Loss:1.971488356590271\n",
      "训练次数：513,Loss:2.13731050491333\n",
      "训练次数：514,Loss:2.0679264068603516\n",
      "训练次数：515,Loss:2.2146825790405273\n",
      "训练次数：516,Loss:1.9939113855361938\n",
      "训练次数：517,Loss:2.091301202774048\n",
      "训练次数：518,Loss:1.977109432220459\n",
      "训练次数：519,Loss:1.9398335218429565\n",
      "训练次数：520,Loss:2.070756196975708\n",
      "训练次数：521,Loss:2.10129714012146\n",
      "训练次数：522,Loss:2.111252546310425\n",
      "训练次数：523,Loss:2.0302348136901855\n",
      "训练次数：524,Loss:2.0629539489746094\n",
      "训练次数：525,Loss:1.9903680086135864\n",
      "训练次数：526,Loss:2.13102126121521\n",
      "训练次数：527,Loss:2.0297951698303223\n",
      "训练次数：528,Loss:2.0065674781799316\n",
      "训练次数：529,Loss:2.0895304679870605\n",
      "训练次数：530,Loss:2.158785820007324\n",
      "训练次数：531,Loss:2.046788454055786\n",
      "训练次数：532,Loss:2.0872364044189453\n",
      "训练次数：533,Loss:2.097003936767578\n",
      "训练次数：534,Loss:1.891086220741272\n",
      "训练次数：535,Loss:2.0144355297088623\n",
      "训练次数：536,Loss:2.091132640838623\n",
      "训练次数：537,Loss:2.062559127807617\n",
      "训练次数：538,Loss:1.9909237623214722\n",
      "训练次数：539,Loss:2.077975034713745\n",
      "训练次数：540,Loss:1.9959899187088013\n",
      "训练次数：541,Loss:2.024174213409424\n",
      "训练次数：542,Loss:2.0900251865386963\n",
      "训练次数：543,Loss:1.9925148487091064\n",
      "训练次数：544,Loss:1.9683871269226074\n",
      "训练次数：545,Loss:2.0079233646392822\n",
      "训练次数：546,Loss:2.014444589614868\n",
      "训练次数：547,Loss:2.0922582149505615\n",
      "训练次数：548,Loss:2.103443145751953\n",
      "训练次数：549,Loss:2.0332131385803223\n",
      "训练次数：550,Loss:2.000675678253174\n",
      "训练次数：551,Loss:2.0251810550689697\n",
      "训练次数：552,Loss:2.1890318393707275\n",
      "训练次数：553,Loss:1.9625015258789062\n",
      "训练次数：554,Loss:2.0133042335510254\n",
      "训练次数：555,Loss:1.9937567710876465\n",
      "训练次数：556,Loss:1.91318941116333\n",
      "训练次数：557,Loss:2.1116931438446045\n",
      "训练次数：558,Loss:2.0186095237731934\n",
      "训练次数：559,Loss:2.07340407371521\n",
      "训练次数：560,Loss:1.9482614994049072\n",
      "训练次数：561,Loss:1.938754916191101\n",
      "训练次数：562,Loss:2.0912845134735107\n",
      "训练次数：563,Loss:2.134291410446167\n",
      "训练次数：564,Loss:2.1860313415527344\n",
      "训练次数：565,Loss:2.0597734451293945\n",
      "训练次数：566,Loss:1.9814821481704712\n",
      "训练次数：567,Loss:1.9768719673156738\n",
      "训练次数：568,Loss:2.0048067569732666\n",
      "训练次数：569,Loss:1.9750598669052124\n",
      "训练次数：570,Loss:2.059915781021118\n",
      "训练次数：571,Loss:2.049757242202759\n",
      "训练次数：572,Loss:1.9629138708114624\n",
      "训练次数：573,Loss:1.971577525138855\n",
      "训练次数：574,Loss:2.037856101989746\n",
      "训练次数：575,Loss:1.973879337310791\n",
      "训练次数：576,Loss:2.128434181213379\n",
      "训练次数：577,Loss:2.0802764892578125\n",
      "训练次数：578,Loss:2.18062686920166\n",
      "训练次数：579,Loss:2.019698143005371\n",
      "训练次数：580,Loss:2.035360813140869\n",
      "训练次数：581,Loss:1.9985496997833252\n",
      "训练次数：582,Loss:2.012078046798706\n",
      "训练次数：583,Loss:2.0107810497283936\n",
      "训练次数：584,Loss:2.1086490154266357\n",
      "训练次数：585,Loss:2.0994505882263184\n",
      "训练次数：586,Loss:2.078699827194214\n",
      "训练次数：587,Loss:2.0986716747283936\n",
      "训练次数：588,Loss:2.0898683071136475\n",
      "训练次数：589,Loss:2.1474499702453613\n",
      "训练次数：590,Loss:2.0575764179229736\n",
      "训练次数：591,Loss:2.083725929260254\n",
      "训练次数：592,Loss:1.980118989944458\n",
      "训练次数：593,Loss:1.9897398948669434\n",
      "训练次数：594,Loss:2.1583452224731445\n",
      "训练次数：595,Loss:2.026238203048706\n",
      "训练次数：596,Loss:2.1205546855926514\n",
      "训练次数：597,Loss:2.0163073539733887\n",
      "训练次数：598,Loss:2.011744737625122\n",
      "训练次数：599,Loss:2.0235259532928467\n",
      "训练次数：600,Loss:2.0068182945251465\n",
      "训练次数：601,Loss:2.0921900272369385\n",
      "训练次数：602,Loss:1.9717206954956055\n",
      "训练次数：603,Loss:1.848716378211975\n",
      "训练次数：604,Loss:2.026127338409424\n",
      "训练次数：605,Loss:2.0947465896606445\n",
      "训练次数：606,Loss:1.9081069231033325\n",
      "训练次数：607,Loss:1.9266085624694824\n",
      "训练次数：608,Loss:1.977708101272583\n",
      "训练次数：609,Loss:1.9946825504302979\n",
      "训练次数：610,Loss:1.9756819009780884\n",
      "训练次数：611,Loss:1.923727035522461\n",
      "训练次数：612,Loss:2.115605115890503\n",
      "训练次数：613,Loss:2.0110647678375244\n",
      "训练次数：614,Loss:2.0360519886016846\n",
      "训练次数：615,Loss:2.012835741043091\n",
      "训练次数：616,Loss:2.059804916381836\n",
      "训练次数：617,Loss:2.043628692626953\n",
      "训练次数：618,Loss:2.077899694442749\n",
      "训练次数：619,Loss:1.9873303174972534\n",
      "训练次数：620,Loss:2.1031312942504883\n",
      "训练次数：621,Loss:2.167335271835327\n",
      "训练次数：622,Loss:2.1570422649383545\n",
      "训练次数：623,Loss:1.9876750707626343\n",
      "训练次数：624,Loss:2.0586023330688477\n",
      "训练次数：625,Loss:2.094604730606079\n",
      "训练次数：626,Loss:1.942116141319275\n",
      "训练次数：627,Loss:1.899117350578308\n",
      "训练次数：628,Loss:2.1544995307922363\n",
      "训练次数：629,Loss:2.1372101306915283\n",
      "训练次数：630,Loss:1.974148154258728\n",
      "训练次数：631,Loss:2.060784339904785\n",
      "训练次数：632,Loss:2.05078125\n",
      "训练次数：633,Loss:1.919849157333374\n",
      "训练次数：634,Loss:1.9938644170761108\n",
      "训练次数：635,Loss:2.0189614295959473\n",
      "训练次数：636,Loss:1.8480254411697388\n",
      "训练次数：637,Loss:2.228254556655884\n",
      "训练次数：638,Loss:2.0356643199920654\n",
      "训练次数：639,Loss:2.103902578353882\n",
      "训练次数：640,Loss:2.18741512298584\n",
      "训练次数：641,Loss:2.1031441688537598\n",
      "训练次数：642,Loss:2.172743558883667\n",
      "训练次数：643,Loss:2.051445960998535\n",
      "训练次数：644,Loss:2.0509355068206787\n",
      "训练次数：645,Loss:1.9812238216400146\n",
      "训练次数：646,Loss:2.1732497215270996\n",
      "训练次数：647,Loss:1.9464516639709473\n",
      "训练次数：648,Loss:1.9793225526809692\n",
      "训练次数：649,Loss:2.2473342418670654\n",
      "训练次数：650,Loss:1.9839218854904175\n",
      "训练次数：651,Loss:1.9241584539413452\n",
      "训练次数：652,Loss:2.0782978534698486\n",
      "训练次数：653,Loss:1.9237433671951294\n",
      "训练次数：654,Loss:1.9448243379592896\n",
      "训练次数：655,Loss:1.9601198434829712\n",
      "训练次数：656,Loss:2.0674073696136475\n",
      "训练次数：657,Loss:2.056166172027588\n",
      "训练次数：658,Loss:1.9610083103179932\n",
      "训练次数：659,Loss:2.028365135192871\n",
      "训练次数：660,Loss:1.85745370388031\n",
      "训练次数：661,Loss:1.9794849157333374\n",
      "训练次数：662,Loss:2.091808795928955\n",
      "训练次数：663,Loss:1.8707209825515747\n",
      "训练次数：664,Loss:1.8603929281234741\n",
      "训练次数：665,Loss:2.0637311935424805\n",
      "训练次数：666,Loss:2.0409038066864014\n",
      "训练次数：667,Loss:2.0257375240325928\n",
      "训练次数：668,Loss:2.147299289703369\n",
      "训练次数：669,Loss:1.9864317178726196\n",
      "训练次数：670,Loss:2.0563395023345947\n",
      "训练次数：671,Loss:2.1286258697509766\n",
      "训练次数：672,Loss:1.9659028053283691\n",
      "训练次数：673,Loss:1.9013079404830933\n",
      "训练次数：674,Loss:2.0078017711639404\n",
      "训练次数：675,Loss:2.1077353954315186\n",
      "训练次数：676,Loss:2.033189535140991\n",
      "训练次数：677,Loss:1.9464722871780396\n",
      "训练次数：678,Loss:1.838951587677002\n",
      "训练次数：679,Loss:2.021066904067993\n",
      "训练次数：680,Loss:2.061840295791626\n",
      "训练次数：681,Loss:2.097787380218506\n",
      "训练次数：682,Loss:1.9028977155685425\n",
      "训练次数：683,Loss:1.985346794128418\n",
      "训练次数：684,Loss:1.8988518714904785\n",
      "训练次数：685,Loss:1.9551068544387817\n",
      "训练次数：686,Loss:2.111781120300293\n",
      "训练次数：687,Loss:1.977551817893982\n",
      "训练次数：688,Loss:1.830758810043335\n",
      "训练次数：689,Loss:1.9654159545898438\n",
      "训练次数：690,Loss:2.020293951034546\n",
      "训练次数：691,Loss:1.9875622987747192\n",
      "训练次数：692,Loss:2.006950855255127\n",
      "训练次数：693,Loss:2.098764419555664\n",
      "训练次数：694,Loss:2.123814821243286\n",
      "训练次数：695,Loss:2.0452373027801514\n",
      "训练次数：696,Loss:2.042259693145752\n",
      "训练次数：697,Loss:2.0272393226623535\n",
      "训练次数：698,Loss:2.0342626571655273\n",
      "训练次数：699,Loss:1.9679443836212158\n",
      "训练次数：700,Loss:2.0464861392974854\n",
      "训练次数：701,Loss:2.0033180713653564\n",
      "训练次数：702,Loss:2.128758192062378\n",
      "训练次数：703,Loss:2.015707015991211\n",
      "训练次数：704,Loss:1.9233100414276123\n",
      "训练次数：705,Loss:1.9481251239776611\n",
      "训练次数：706,Loss:1.9875500202178955\n",
      "训练次数：707,Loss:2.1238181591033936\n",
      "训练次数：708,Loss:1.9741467237472534\n",
      "训练次数：709,Loss:2.1264004707336426\n",
      "训练次数：710,Loss:1.9973236322402954\n",
      "训练次数：711,Loss:2.0705220699310303\n",
      "训练次数：712,Loss:2.0239620208740234\n",
      "训练次数：713,Loss:1.9481173753738403\n",
      "训练次数：714,Loss:1.9596376419067383\n",
      "训练次数：715,Loss:1.9126774072647095\n",
      "训练次数：716,Loss:2.110219955444336\n",
      "训练次数：717,Loss:1.9992334842681885\n",
      "训练次数：718,Loss:2.0043394565582275\n",
      "训练次数：719,Loss:1.9026068449020386\n",
      "训练次数：720,Loss:2.0150654315948486\n",
      "训练次数：721,Loss:1.9029630422592163\n",
      "训练次数：722,Loss:1.9942755699157715\n",
      "训练次数：723,Loss:1.92630934715271\n",
      "训练次数：724,Loss:2.0521860122680664\n",
      "训练次数：725,Loss:2.047297954559326\n",
      "训练次数：726,Loss:1.8279364109039307\n",
      "训练次数：727,Loss:2.105623960494995\n",
      "训练次数：728,Loss:2.053657054901123\n",
      "训练次数：729,Loss:1.8903692960739136\n",
      "训练次数：730,Loss:2.1307716369628906\n",
      "训练次数：731,Loss:1.8767664432525635\n",
      "训练次数：732,Loss:1.9890646934509277\n",
      "训练次数：733,Loss:2.0884509086608887\n",
      "训练次数：734,Loss:1.9563158750534058\n",
      "训练次数：735,Loss:1.874068021774292\n",
      "训练次数：736,Loss:2.2076826095581055\n",
      "训练次数：737,Loss:1.9674499034881592\n",
      "训练次数：738,Loss:2.037796974182129\n",
      "训练次数：739,Loss:1.8331029415130615\n",
      "训练次数：740,Loss:1.894176959991455\n",
      "训练次数：741,Loss:1.9795160293579102\n",
      "训练次数：742,Loss:1.9907828569412231\n",
      "训练次数：743,Loss:2.075253486633301\n",
      "训练次数：744,Loss:1.9475535154342651\n",
      "训练次数：745,Loss:1.9184361696243286\n",
      "训练次数：746,Loss:1.9778496026992798\n",
      "训练次数：747,Loss:1.8400719165802002\n",
      "训练次数：748,Loss:2.2481677532196045\n",
      "训练次数：749,Loss:2.0825648307800293\n",
      "训练次数：750,Loss:1.948749303817749\n",
      "训练次数：751,Loss:2.1958396434783936\n",
      "训练次数：752,Loss:2.188276767730713\n",
      "训练次数：753,Loss:1.930301308631897\n",
      "训练次数：754,Loss:1.974711298942566\n",
      "训练次数：755,Loss:1.988692045211792\n",
      "训练次数：756,Loss:1.8608354330062866\n",
      "训练次数：757,Loss:2.1024010181427\n",
      "训练次数：758,Loss:1.8576127290725708\n",
      "训练次数：759,Loss:1.922810673713684\n",
      "训练次数：760,Loss:2.03905987739563\n",
      "训练次数：761,Loss:1.9826064109802246\n",
      "训练次数：762,Loss:1.9598160982131958\n",
      "训练次数：763,Loss:2.13108229637146\n",
      "训练次数：764,Loss:2.002232074737549\n",
      "训练次数：765,Loss:1.9130206108093262\n",
      "训练次数：766,Loss:2.0615506172180176\n",
      "训练次数：767,Loss:2.1162145137786865\n",
      "训练次数：768,Loss:2.0295159816741943\n",
      "训练次数：769,Loss:1.8894789218902588\n",
      "训练次数：770,Loss:1.8942047357559204\n",
      "训练次数：771,Loss:1.9816807508468628\n",
      "训练次数：772,Loss:1.8939999341964722\n",
      "训练次数：773,Loss:2.000408411026001\n",
      "训练次数：774,Loss:2.0040814876556396\n",
      "训练次数：775,Loss:2.045999050140381\n",
      "训练次数：776,Loss:1.9161583185195923\n",
      "训练次数：777,Loss:2.0556118488311768\n",
      "训练次数：778,Loss:2.0129706859588623\n",
      "训练次数：779,Loss:2.0857651233673096\n",
      "训练次数：780,Loss:1.9706352949142456\n",
      "训练次数：781,Loss:2.0903749465942383\n",
      "训练次数：782,Loss:2.1237709522247314\n",
      "-----第 2 轮训练开始-----\n",
      "训练次数：783,Loss:2.061462640762329\n",
      "训练次数：784,Loss:1.861357569694519\n",
      "训练次数：785,Loss:2.022721767425537\n",
      "训练次数：786,Loss:1.7999354600906372\n",
      "训练次数：787,Loss:1.8388817310333252\n",
      "训练次数：788,Loss:1.8955169916152954\n",
      "训练次数：789,Loss:1.9155852794647217\n",
      "训练次数：790,Loss:1.920918345451355\n",
      "训练次数：791,Loss:1.9443107843399048\n",
      "训练次数：792,Loss:2.020171880722046\n",
      "训练次数：793,Loss:1.994672417640686\n",
      "训练次数：794,Loss:1.9053757190704346\n",
      "训练次数：795,Loss:2.1004300117492676\n",
      "训练次数：796,Loss:2.042854070663452\n",
      "训练次数：797,Loss:1.8821359872817993\n",
      "训练次数：798,Loss:1.8896408081054688\n",
      "训练次数：799,Loss:1.8504087924957275\n",
      "训练次数：800,Loss:1.8898845911026\n",
      "训练次数：801,Loss:1.9803777933120728\n",
      "训练次数：802,Loss:1.9128644466400146\n",
      "训练次数：803,Loss:1.9206970930099487\n",
      "训练次数：804,Loss:1.9023873805999756\n",
      "训练次数：805,Loss:1.8725167512893677\n",
      "训练次数：806,Loss:1.9761542081832886\n",
      "训练次数：807,Loss:1.9288215637207031\n",
      "训练次数：808,Loss:1.8772209882736206\n",
      "训练次数：809,Loss:2.0239908695220947\n",
      "训练次数：810,Loss:2.063687562942505\n",
      "训练次数：811,Loss:1.9644920825958252\n",
      "训练次数：812,Loss:1.989028811454773\n",
      "训练次数：813,Loss:2.103630304336548\n",
      "训练次数：814,Loss:2.2172961235046387\n",
      "训练次数：815,Loss:2.1718056201934814\n",
      "训练次数：816,Loss:1.8334109783172607\n",
      "训练次数：817,Loss:1.883176326751709\n",
      "训练次数：818,Loss:2.108705759048462\n",
      "训练次数：819,Loss:1.9650251865386963\n",
      "训练次数：820,Loss:1.8153201341629028\n",
      "训练次数：821,Loss:1.850237488746643\n",
      "训练次数：822,Loss:2.03244686126709\n",
      "训练次数：823,Loss:2.0076699256896973\n",
      "训练次数：824,Loss:1.986601710319519\n",
      "训练次数：825,Loss:1.9570262432098389\n",
      "训练次数：826,Loss:1.94169020652771\n",
      "训练次数：827,Loss:1.8289380073547363\n",
      "训练次数：828,Loss:1.9305483102798462\n",
      "训练次数：829,Loss:2.0759847164154053\n",
      "训练次数：830,Loss:1.8468234539031982\n",
      "训练次数：831,Loss:2.0420334339141846\n",
      "训练次数：832,Loss:2.0294363498687744\n",
      "训练次数：833,Loss:1.9859243631362915\n",
      "训练次数：834,Loss:1.931949496269226\n",
      "训练次数：835,Loss:1.8552792072296143\n",
      "训练次数：836,Loss:2.1388094425201416\n",
      "训练次数：837,Loss:1.911011815071106\n",
      "训练次数：838,Loss:1.9446948766708374\n",
      "训练次数：839,Loss:1.9797601699829102\n",
      "训练次数：840,Loss:2.050539970397949\n",
      "训练次数：841,Loss:1.7961077690124512\n",
      "训练次数：842,Loss:1.851048231124878\n",
      "训练次数：843,Loss:2.011507987976074\n",
      "训练次数：844,Loss:2.072652578353882\n",
      "训练次数：845,Loss:1.9242993593215942\n",
      "训练次数：846,Loss:1.9663152694702148\n",
      "训练次数：847,Loss:1.931788682937622\n",
      "训练次数：848,Loss:1.858518123626709\n",
      "训练次数：849,Loss:1.887473702430725\n",
      "训练次数：850,Loss:2.00728178024292\n",
      "训练次数：851,Loss:1.9743292331695557\n",
      "训练次数：852,Loss:1.9623380899429321\n",
      "训练次数：853,Loss:2.0085673332214355\n",
      "训练次数：854,Loss:1.9127651453018188\n",
      "训练次数：855,Loss:1.9260013103485107\n",
      "训练次数：856,Loss:1.9030170440673828\n",
      "训练次数：857,Loss:1.8770283460617065\n",
      "训练次数：858,Loss:1.9790903329849243\n",
      "训练次数：859,Loss:1.9207308292388916\n",
      "训练次数：860,Loss:1.9047653675079346\n",
      "训练次数：861,Loss:1.8859326839447021\n",
      "训练次数：862,Loss:2.061039924621582\n",
      "训练次数：863,Loss:2.0405328273773193\n",
      "训练次数：864,Loss:2.1095798015594482\n",
      "训练次数：865,Loss:1.974810242652893\n",
      "训练次数：866,Loss:1.8085048198699951\n",
      "训练次数：867,Loss:1.8600248098373413\n",
      "训练次数：868,Loss:2.0075523853302\n",
      "训练次数：869,Loss:2.0005292892456055\n",
      "训练次数：870,Loss:1.7408479452133179\n",
      "训练次数：871,Loss:1.843179702758789\n",
      "训练次数：872,Loss:1.9829068183898926\n",
      "训练次数：873,Loss:1.98997962474823\n",
      "训练次数：874,Loss:1.8874986171722412\n",
      "训练次数：875,Loss:1.987251877784729\n",
      "训练次数：876,Loss:1.8167610168457031\n",
      "训练次数：877,Loss:2.1397438049316406\n",
      "训练次数：878,Loss:2.081003427505493\n",
      "训练次数：879,Loss:1.8477038145065308\n",
      "训练次数：880,Loss:1.9397387504577637\n",
      "训练次数：881,Loss:1.8813917636871338\n",
      "训练次数：882,Loss:1.9627180099487305\n",
      "训练次数：883,Loss:1.9678936004638672\n",
      "训练次数：884,Loss:1.8153069019317627\n",
      "训练次数：885,Loss:1.9394451379776\n",
      "训练次数：886,Loss:2.016477584838867\n",
      "训练次数：887,Loss:1.9962230920791626\n",
      "训练次数：888,Loss:1.8539295196533203\n",
      "训练次数：889,Loss:1.9743729829788208\n",
      "训练次数：890,Loss:2.0932281017303467\n",
      "训练次数：891,Loss:1.9299029111862183\n",
      "训练次数：892,Loss:1.81393301486969\n",
      "训练次数：893,Loss:1.8587743043899536\n",
      "训练次数：894,Loss:1.9504568576812744\n",
      "训练次数：895,Loss:1.9644180536270142\n",
      "训练次数：896,Loss:1.7308062314987183\n",
      "训练次数：897,Loss:1.9538280963897705\n",
      "训练次数：898,Loss:2.0166258811950684\n",
      "训练次数：899,Loss:1.9135106801986694\n",
      "训练次数：900,Loss:1.8648968935012817\n",
      "训练次数：901,Loss:2.0814452171325684\n",
      "训练次数：902,Loss:2.0482852458953857\n",
      "训练次数：903,Loss:1.9348334074020386\n",
      "训练次数：904,Loss:1.9452099800109863\n",
      "训练次数：905,Loss:2.0123941898345947\n",
      "训练次数：906,Loss:1.9446625709533691\n",
      "训练次数：907,Loss:1.8498479127883911\n",
      "训练次数：908,Loss:1.8588064908981323\n",
      "训练次数：909,Loss:1.839796543121338\n",
      "训练次数：910,Loss:1.898018479347229\n",
      "训练次数：911,Loss:1.8845428228378296\n",
      "训练次数：912,Loss:1.8795554637908936\n",
      "训练次数：913,Loss:2.077793836593628\n",
      "训练次数：914,Loss:2.023719310760498\n",
      "训练次数：915,Loss:2.2137269973754883\n",
      "训练次数：916,Loss:1.8401083946228027\n",
      "训练次数：917,Loss:1.9071385860443115\n",
      "训练次数：918,Loss:1.9424803256988525\n",
      "训练次数：919,Loss:1.9488811492919922\n",
      "训练次数：920,Loss:1.930158257484436\n",
      "训练次数：921,Loss:1.9205477237701416\n",
      "训练次数：922,Loss:2.087259292602539\n",
      "训练次数：923,Loss:2.1395699977874756\n",
      "训练次数：924,Loss:1.9845324754714966\n",
      "训练次数：925,Loss:1.8834880590438843\n",
      "训练次数：926,Loss:1.8075252771377563\n",
      "训练次数：927,Loss:1.833526849746704\n",
      "训练次数：928,Loss:2.0486209392547607\n",
      "训练次数：929,Loss:1.8903752565383911\n",
      "训练次数：930,Loss:2.116001605987549\n",
      "训练次数：931,Loss:1.8026996850967407\n",
      "训练次数：932,Loss:2.0212275981903076\n",
      "训练次数：933,Loss:1.8819069862365723\n",
      "训练次数：934,Loss:1.8877720832824707\n",
      "训练次数：935,Loss:2.027292490005493\n",
      "训练次数：936,Loss:1.8646247386932373\n",
      "训练次数：937,Loss:1.7988464832305908\n",
      "训练次数：938,Loss:1.963632345199585\n",
      "训练次数：939,Loss:1.952272891998291\n",
      "训练次数：940,Loss:1.93580961227417\n",
      "训练次数：941,Loss:2.026944637298584\n",
      "训练次数：942,Loss:2.142744779586792\n",
      "训练次数：943,Loss:2.024552583694458\n",
      "训练次数：944,Loss:1.8752673864364624\n",
      "训练次数：945,Loss:1.9421560764312744\n",
      "训练次数：946,Loss:2.0416741371154785\n",
      "训练次数：947,Loss:1.902300477027893\n",
      "训练次数：948,Loss:1.9910778999328613\n",
      "训练次数：949,Loss:1.8385064601898193\n",
      "训练次数：950,Loss:1.8541791439056396\n",
      "训练次数：951,Loss:1.9304028749465942\n",
      "训练次数：952,Loss:1.8829582929611206\n",
      "训练次数：953,Loss:1.9273617267608643\n",
      "训练次数：954,Loss:1.8518749475479126\n",
      "训练次数：955,Loss:1.8577314615249634\n",
      "训练次数：956,Loss:2.0127620697021484\n",
      "训练次数：957,Loss:1.9351019859313965\n",
      "训练次数：958,Loss:1.8838160037994385\n",
      "训练次数：959,Loss:1.8928241729736328\n",
      "训练次数：960,Loss:1.97494637966156\n",
      "训练次数：961,Loss:1.9408642053604126\n",
      "训练次数：962,Loss:1.922666311264038\n",
      "训练次数：963,Loss:1.837470293045044\n",
      "训练次数：964,Loss:1.7651197910308838\n",
      "训练次数：965,Loss:1.8533073663711548\n",
      "训练次数：966,Loss:1.7948276996612549\n",
      "训练次数：967,Loss:1.991849422454834\n",
      "训练次数：968,Loss:2.0886659622192383\n",
      "训练次数：969,Loss:1.9021879434585571\n",
      "训练次数：970,Loss:1.8594434261322021\n",
      "训练次数：971,Loss:2.04822039604187\n",
      "训练次数：972,Loss:1.9518346786499023\n",
      "训练次数：973,Loss:1.734424352645874\n",
      "训练次数：974,Loss:1.9620610475540161\n",
      "训练次数：975,Loss:1.9852687120437622\n",
      "训练次数：976,Loss:1.7730430364608765\n",
      "训练次数：977,Loss:1.868833065032959\n",
      "训练次数：978,Loss:1.9654618501663208\n",
      "训练次数：979,Loss:1.9051084518432617\n",
      "训练次数：980,Loss:1.8093171119689941\n",
      "训练次数：981,Loss:1.899296760559082\n",
      "训练次数：982,Loss:1.9602513313293457\n",
      "训练次数：983,Loss:1.750191569328308\n",
      "训练次数：984,Loss:2.0045664310455322\n",
      "训练次数：985,Loss:1.8981565237045288\n",
      "训练次数：986,Loss:1.9721629619598389\n",
      "训练次数：987,Loss:1.8088185787200928\n",
      "训练次数：988,Loss:1.9884165525436401\n",
      "训练次数：989,Loss:1.9096218347549438\n",
      "训练次数：990,Loss:1.8707032203674316\n",
      "训练次数：991,Loss:1.9216554164886475\n",
      "训练次数：992,Loss:2.0550897121429443\n",
      "训练次数：993,Loss:1.8375099897384644\n",
      "训练次数：994,Loss:2.002788543701172\n",
      "训练次数：995,Loss:1.8498034477233887\n",
      "训练次数：996,Loss:1.8794808387756348\n",
      "训练次数：997,Loss:1.8989745378494263\n",
      "训练次数：998,Loss:1.9877351522445679\n",
      "训练次数：999,Loss:1.812152624130249\n",
      "训练次数：1000,Loss:1.9409455060958862\n",
      "训练次数：1001,Loss:1.9301551580429077\n",
      "训练次数：1002,Loss:1.8629385232925415\n",
      "训练次数：1003,Loss:2.0023245811462402\n",
      "训练次数：1004,Loss:1.8640183210372925\n",
      "训练次数：1005,Loss:1.9015157222747803\n",
      "训练次数：1006,Loss:2.0613739490509033\n",
      "训练次数：1007,Loss:2.2055435180664062\n",
      "训练次数：1008,Loss:2.0137884616851807\n",
      "训练次数：1009,Loss:1.796984314918518\n",
      "训练次数：1010,Loss:1.8259631395339966\n",
      "训练次数：1011,Loss:1.8721740245819092\n",
      "训练次数：1012,Loss:2.100924015045166\n",
      "训练次数：1013,Loss:1.9514738321304321\n",
      "训练次数：1014,Loss:1.789261817932129\n",
      "训练次数：1015,Loss:1.8860893249511719\n",
      "训练次数：1016,Loss:1.966529130935669\n",
      "训练次数：1017,Loss:1.9712815284729004\n",
      "训练次数：1018,Loss:2.0773112773895264\n",
      "训练次数：1019,Loss:2.1463239192962646\n",
      "训练次数：1020,Loss:1.900356650352478\n",
      "训练次数：1021,Loss:1.8445147275924683\n",
      "训练次数：1022,Loss:1.9300042390823364\n",
      "训练次数：1023,Loss:2.0300798416137695\n",
      "训练次数：1024,Loss:1.8552721738815308\n",
      "训练次数：1025,Loss:1.8505170345306396\n",
      "训练次数：1026,Loss:2.0996179580688477\n",
      "训练次数：1027,Loss:1.7966705560684204\n",
      "训练次数：1028,Loss:1.793411135673523\n",
      "训练次数：1029,Loss:1.949562668800354\n",
      "训练次数：1030,Loss:1.9075002670288086\n",
      "训练次数：1031,Loss:1.8731356859207153\n",
      "训练次数：1032,Loss:2.002326726913452\n",
      "训练次数：1033,Loss:1.849205493927002\n",
      "训练次数：1034,Loss:1.831937313079834\n",
      "训练次数：1035,Loss:1.9085617065429688\n",
      "训练次数：1036,Loss:1.903896450996399\n",
      "训练次数：1037,Loss:1.946480393409729\n",
      "训练次数：1038,Loss:1.8650856018066406\n",
      "训练次数：1039,Loss:1.9321857690811157\n",
      "训练次数：1040,Loss:1.797486424446106\n",
      "训练次数：1041,Loss:2.007174253463745\n",
      "训练次数：1042,Loss:2.068847894668579\n",
      "训练次数：1043,Loss:1.9753018617630005\n",
      "训练次数：1044,Loss:2.0246527194976807\n",
      "训练次数：1045,Loss:1.9276162385940552\n",
      "训练次数：1046,Loss:1.988503336906433\n",
      "训练次数：1047,Loss:1.7431358098983765\n",
      "训练次数：1048,Loss:1.7387876510620117\n",
      "训练次数：1049,Loss:1.9655225276947021\n",
      "训练次数：1050,Loss:1.9682974815368652\n",
      "训练次数：1051,Loss:1.9866071939468384\n",
      "训练次数：1052,Loss:1.8404771089553833\n",
      "训练次数：1053,Loss:2.052504539489746\n",
      "训练次数：1054,Loss:1.9685841798782349\n",
      "训练次数：1055,Loss:1.800412654876709\n",
      "训练次数：1056,Loss:1.883544683456421\n",
      "训练次数：1057,Loss:1.9173259735107422\n",
      "训练次数：1058,Loss:1.9160680770874023\n",
      "训练次数：1059,Loss:2.044119358062744\n",
      "训练次数：1060,Loss:1.8926950693130493\n",
      "训练次数：1061,Loss:1.950277328491211\n",
      "训练次数：1062,Loss:1.7537388801574707\n",
      "训练次数：1063,Loss:1.7830129861831665\n",
      "训练次数：1064,Loss:1.8350921869277954\n",
      "训练次数：1065,Loss:1.872481107711792\n",
      "训练次数：1066,Loss:1.9302711486816406\n",
      "训练次数：1067,Loss:1.8104791641235352\n",
      "训练次数：1068,Loss:1.8365256786346436\n",
      "训练次数：1069,Loss:1.994842290878296\n",
      "训练次数：1070,Loss:1.8726931810379028\n",
      "训练次数：1071,Loss:1.9053573608398438\n",
      "训练次数：1072,Loss:1.8710135221481323\n",
      "训练次数：1073,Loss:1.822303295135498\n",
      "训练次数：1074,Loss:1.9597721099853516\n",
      "训练次数：1075,Loss:1.866551399230957\n",
      "训练次数：1076,Loss:1.7463104724884033\n",
      "训练次数：1077,Loss:1.9079962968826294\n",
      "训练次数：1078,Loss:1.7773709297180176\n",
      "训练次数：1079,Loss:2.0622150897979736\n",
      "训练次数：1080,Loss:1.9013872146606445\n",
      "训练次数：1081,Loss:1.8100473880767822\n",
      "训练次数：1082,Loss:1.8921316862106323\n",
      "训练次数：1083,Loss:1.941869854927063\n",
      "训练次数：1084,Loss:2.0051631927490234\n",
      "训练次数：1085,Loss:1.8695622682571411\n",
      "训练次数：1086,Loss:1.7349876165390015\n",
      "训练次数：1087,Loss:1.732292652130127\n",
      "训练次数：1088,Loss:1.924865484237671\n",
      "训练次数：1089,Loss:1.746621012687683\n",
      "训练次数：1090,Loss:1.8587393760681152\n",
      "训练次数：1091,Loss:1.782112717628479\n",
      "训练次数：1092,Loss:1.796358585357666\n",
      "训练次数：1093,Loss:1.8058139085769653\n",
      "训练次数：1094,Loss:1.7180631160736084\n",
      "训练次数：1095,Loss:1.9467219114303589\n",
      "训练次数：1096,Loss:1.9420160055160522\n",
      "训练次数：1097,Loss:1.767691731452942\n",
      "训练次数：1098,Loss:2.009875535964966\n",
      "训练次数：1099,Loss:1.7886251211166382\n",
      "训练次数：1100,Loss:1.9536964893341064\n",
      "训练次数：1101,Loss:1.6728417873382568\n",
      "训练次数：1102,Loss:1.8381725549697876\n",
      "训练次数：1103,Loss:1.819427490234375\n",
      "训练次数：1104,Loss:1.9303722381591797\n",
      "训练次数：1105,Loss:1.8057029247283936\n",
      "训练次数：1106,Loss:1.974159598350525\n",
      "训练次数：1107,Loss:1.7852909564971924\n",
      "训练次数：1108,Loss:1.6700804233551025\n",
      "训练次数：1109,Loss:1.8148910999298096\n",
      "训练次数：1110,Loss:1.777614712715149\n",
      "训练次数：1111,Loss:1.8316504955291748\n",
      "训练次数：1112,Loss:1.9220446348190308\n",
      "训练次数：1113,Loss:1.8176571130752563\n",
      "训练次数：1114,Loss:1.8114339113235474\n",
      "训练次数：1115,Loss:1.688125491142273\n",
      "训练次数：1116,Loss:1.9130492210388184\n",
      "训练次数：1117,Loss:1.784074068069458\n",
      "训练次数：1118,Loss:1.8981916904449463\n",
      "训练次数：1119,Loss:1.8999056816101074\n",
      "训练次数：1120,Loss:1.7649695873260498\n",
      "训练次数：1121,Loss:1.7509132623672485\n",
      "训练次数：1122,Loss:1.6344399452209473\n",
      "训练次数：1123,Loss:1.8860044479370117\n",
      "训练次数：1124,Loss:1.9813653230667114\n",
      "训练次数：1125,Loss:1.812267541885376\n",
      "训练次数：1126,Loss:1.8853085041046143\n",
      "训练次数：1127,Loss:1.8162070512771606\n",
      "训练次数：1128,Loss:1.7160314321517944\n",
      "训练次数：1129,Loss:1.8444993495941162\n",
      "训练次数：1130,Loss:1.8841882944107056\n",
      "训练次数：1131,Loss:1.9387929439544678\n",
      "训练次数：1132,Loss:1.7547402381896973\n",
      "训练次数：1133,Loss:1.7942066192626953\n",
      "训练次数：1134,Loss:1.8113704919815063\n",
      "训练次数：1135,Loss:1.7191879749298096\n",
      "训练次数：1136,Loss:1.923161506652832\n",
      "训练次数：1137,Loss:1.724258303642273\n",
      "训练次数：1138,Loss:1.9034483432769775\n",
      "训练次数：1139,Loss:1.8085105419158936\n",
      "训练次数：1140,Loss:1.8664798736572266\n",
      "训练次数：1141,Loss:1.842179298400879\n",
      "训练次数：1142,Loss:1.9314192533493042\n",
      "训练次数：1143,Loss:1.8429794311523438\n",
      "训练次数：1144,Loss:1.666643500328064\n",
      "训练次数：1145,Loss:2.068253517150879\n",
      "训练次数：1146,Loss:1.8571349382400513\n",
      "训练次数：1147,Loss:1.5495558977127075\n",
      "训练次数：1148,Loss:1.6851454973220825\n",
      "训练次数：1149,Loss:1.7974305152893066\n",
      "训练次数：1150,Loss:1.9514113664627075\n",
      "训练次数：1151,Loss:1.5846301317214966\n",
      "训练次数：1152,Loss:1.7120544910430908\n",
      "训练次数：1153,Loss:1.7230463027954102\n",
      "训练次数：1154,Loss:1.9209918975830078\n",
      "训练次数：1155,Loss:1.8162163496017456\n",
      "训练次数：1156,Loss:1.7591129541397095\n",
      "训练次数：1157,Loss:1.9718613624572754\n",
      "训练次数：1158,Loss:1.8238794803619385\n",
      "训练次数：1159,Loss:1.861208200454712\n",
      "训练次数：1160,Loss:1.799019694328308\n",
      "训练次数：1161,Loss:1.8570289611816406\n",
      "训练次数：1162,Loss:1.853925347328186\n",
      "训练次数：1163,Loss:1.9018187522888184\n",
      "训练次数：1164,Loss:1.741498589515686\n",
      "训练次数：1165,Loss:1.7807645797729492\n",
      "训练次数：1166,Loss:1.690651774406433\n",
      "训练次数：1167,Loss:1.784407377243042\n",
      "训练次数：1168,Loss:1.813295841217041\n",
      "训练次数：1169,Loss:1.7397019863128662\n",
      "训练次数：1170,Loss:2.018404245376587\n",
      "训练次数：1171,Loss:1.9148786067962646\n",
      "训练次数：1172,Loss:1.7298328876495361\n",
      "训练次数：1173,Loss:1.8267385959625244\n",
      "训练次数：1174,Loss:1.7448508739471436\n",
      "训练次数：1175,Loss:1.739546298980713\n",
      "训练次数：1176,Loss:1.8034400939941406\n",
      "训练次数：1177,Loss:1.751187801361084\n",
      "训练次数：1178,Loss:1.7851260900497437\n",
      "训练次数：1179,Loss:1.7584065198898315\n",
      "训练次数：1180,Loss:1.869307041168213\n",
      "训练次数：1181,Loss:1.6496554613113403\n",
      "训练次数：1182,Loss:1.8370296955108643\n",
      "训练次数：1183,Loss:2.0149002075195312\n",
      "训练次数：1184,Loss:1.8775864839553833\n",
      "训练次数：1185,Loss:1.8881654739379883\n",
      "训练次数：1186,Loss:1.7463440895080566\n",
      "训练次数：1187,Loss:1.930755376815796\n",
      "训练次数：1188,Loss:1.7113114595413208\n",
      "训练次数：1189,Loss:1.7606544494628906\n",
      "训练次数：1190,Loss:1.556782841682434\n",
      "训练次数：1191,Loss:1.7223637104034424\n",
      "训练次数：1192,Loss:1.6988292932510376\n",
      "训练次数：1193,Loss:1.9206370115280151\n",
      "训练次数：1194,Loss:1.7749602794647217\n",
      "训练次数：1195,Loss:1.8985830545425415\n",
      "训练次数：1196,Loss:1.8093860149383545\n",
      "训练次数：1197,Loss:1.9521434307098389\n",
      "训练次数：1198,Loss:1.8748137950897217\n",
      "训练次数：1199,Loss:1.9028444290161133\n",
      "训练次数：1200,Loss:1.7170038223266602\n",
      "训练次数：1201,Loss:1.7481443881988525\n",
      "训练次数：1202,Loss:1.8428754806518555\n",
      "训练次数：1203,Loss:1.8364183902740479\n",
      "训练次数：1204,Loss:1.8879567384719849\n",
      "训练次数：1205,Loss:1.95026695728302\n",
      "训练次数：1206,Loss:1.775743007659912\n",
      "训练次数：1207,Loss:1.7345540523529053\n",
      "训练次数：1208,Loss:1.8854455947875977\n",
      "训练次数：1209,Loss:1.9300163984298706\n",
      "训练次数：1210,Loss:1.690312147140503\n",
      "训练次数：1211,Loss:1.8916670083999634\n",
      "训练次数：1212,Loss:1.9141242504119873\n",
      "训练次数：1213,Loss:1.8414231538772583\n",
      "训练次数：1214,Loss:1.8102636337280273\n",
      "训练次数：1215,Loss:1.9908404350280762\n",
      "训练次数：1216,Loss:1.7970010042190552\n",
      "训练次数：1217,Loss:1.7406127452850342\n",
      "训练次数：1218,Loss:2.1173291206359863\n",
      "训练次数：1219,Loss:2.2663156986236572\n",
      "训练次数：1220,Loss:1.881974458694458\n",
      "训练次数：1221,Loss:1.8574845790863037\n",
      "训练次数：1222,Loss:1.8644298315048218\n",
      "训练次数：1223,Loss:1.9708889722824097\n",
      "训练次数：1224,Loss:1.8513386249542236\n",
      "训练次数：1225,Loss:1.8357547521591187\n",
      "训练次数：1226,Loss:1.67264723777771\n",
      "训练次数：1227,Loss:1.8478798866271973\n",
      "训练次数：1228,Loss:1.8783594369888306\n",
      "训练次数：1229,Loss:1.8291865587234497\n",
      "训练次数：1230,Loss:1.8427215814590454\n",
      "训练次数：1231,Loss:1.6617498397827148\n",
      "训练次数：1232,Loss:1.9238117933273315\n",
      "训练次数：1233,Loss:1.691772699356079\n",
      "训练次数：1234,Loss:1.6034331321716309\n",
      "训练次数：1235,Loss:1.8025585412979126\n",
      "训练次数：1236,Loss:1.8110781908035278\n",
      "训练次数：1237,Loss:1.8881573677062988\n",
      "训练次数：1238,Loss:1.712686538696289\n",
      "训练次数：1239,Loss:1.8754703998565674\n",
      "训练次数：1240,Loss:1.7064892053604126\n",
      "训练次数：1241,Loss:1.7932614088058472\n",
      "训练次数：1242,Loss:1.7358064651489258\n",
      "训练次数：1243,Loss:1.8439656496047974\n",
      "训练次数：1244,Loss:1.8668535947799683\n",
      "训练次数：1245,Loss:1.82295823097229\n",
      "训练次数：1246,Loss:1.785617709159851\n",
      "训练次数：1247,Loss:1.7572792768478394\n",
      "训练次数：1248,Loss:1.7135210037231445\n",
      "训练次数：1249,Loss:1.9308679103851318\n",
      "训练次数：1250,Loss:1.6663299798965454\n",
      "训练次数：1251,Loss:1.74172043800354\n",
      "训练次数：1252,Loss:1.9053499698638916\n",
      "训练次数：1253,Loss:1.7922916412353516\n",
      "训练次数：1254,Loss:1.9258182048797607\n",
      "训练次数：1255,Loss:1.6851451396942139\n",
      "训练次数：1256,Loss:1.8774021863937378\n",
      "训练次数：1257,Loss:1.6854755878448486\n",
      "训练次数：1258,Loss:1.9058345556259155\n",
      "训练次数：1259,Loss:1.8131228685379028\n",
      "训练次数：1260,Loss:2.0654923915863037\n",
      "训练次数：1261,Loss:2.0574166774749756\n",
      "训练次数：1262,Loss:1.8502368927001953\n",
      "训练次数：1263,Loss:1.8556894063949585\n",
      "训练次数：1264,Loss:1.9284948110580444\n",
      "训练次数：1265,Loss:1.9253801107406616\n",
      "训练次数：1266,Loss:1.7157151699066162\n",
      "训练次数：1267,Loss:1.717574119567871\n",
      "训练次数：1268,Loss:1.6876461505889893\n",
      "训练次数：1269,Loss:2.0344505310058594\n",
      "训练次数：1270,Loss:1.9913699626922607\n",
      "训练次数：1271,Loss:1.8506675958633423\n",
      "训练次数：1272,Loss:1.678916335105896\n",
      "训练次数：1273,Loss:1.9862029552459717\n",
      "训练次数：1274,Loss:1.8080017566680908\n",
      "训练次数：1275,Loss:1.794998288154602\n",
      "训练次数：1276,Loss:1.6818928718566895\n",
      "训练次数：1277,Loss:1.7623343467712402\n",
      "训练次数：1278,Loss:1.7638747692108154\n",
      "训练次数：1279,Loss:1.8496370315551758\n",
      "训练次数：1280,Loss:1.9277557134628296\n",
      "训练次数：1281,Loss:2.062465190887451\n",
      "训练次数：1282,Loss:1.804016351699829\n",
      "训练次数：1283,Loss:1.7805622816085815\n",
      "训练次数：1284,Loss:1.856104850769043\n",
      "训练次数：1285,Loss:1.7032382488250732\n",
      "训练次数：1286,Loss:1.9704740047454834\n",
      "训练次数：1287,Loss:1.739122986793518\n",
      "训练次数：1288,Loss:1.7167174816131592\n",
      "训练次数：1289,Loss:1.7515596151351929\n",
      "训练次数：1290,Loss:1.8120986223220825\n",
      "训练次数：1291,Loss:1.9848254919052124\n",
      "训练次数：1292,Loss:1.8768099546432495\n",
      "训练次数：1293,Loss:1.8899805545806885\n",
      "训练次数：1294,Loss:1.68446946144104\n",
      "训练次数：1295,Loss:1.9012974500656128\n",
      "训练次数：1296,Loss:1.8365317583084106\n",
      "训练次数：1297,Loss:2.000859022140503\n",
      "训练次数：1298,Loss:1.7301348447799683\n",
      "训练次数：1299,Loss:1.9568421840667725\n",
      "训练次数：1300,Loss:1.6701925992965698\n",
      "训练次数：1301,Loss:1.6941947937011719\n",
      "训练次数：1302,Loss:1.7898788452148438\n",
      "训练次数：1303,Loss:1.8689650297164917\n",
      "训练次数：1304,Loss:1.8576124906539917\n",
      "训练次数：1305,Loss:1.840996503829956\n",
      "训练次数：1306,Loss:1.869321346282959\n",
      "训练次数：1307,Loss:1.8738868236541748\n",
      "训练次数：1308,Loss:1.9305591583251953\n",
      "训练次数：1309,Loss:1.825850486755371\n",
      "训练次数：1310,Loss:1.827349066734314\n",
      "训练次数：1311,Loss:1.85834538936615\n",
      "训练次数：1312,Loss:2.1346518993377686\n",
      "训练次数：1313,Loss:2.1327028274536133\n",
      "训练次数：1314,Loss:1.954805850982666\n",
      "训练次数：1315,Loss:1.8609007596969604\n",
      "训练次数：1316,Loss:1.5982215404510498\n",
      "训练次数：1317,Loss:1.7173265218734741\n",
      "训练次数：1318,Loss:1.7575749158859253\n",
      "训练次数：1319,Loss:1.8219325542449951\n",
      "训练次数：1320,Loss:1.6604124307632446\n",
      "训练次数：1321,Loss:1.8102911710739136\n",
      "训练次数：1322,Loss:1.7842861413955688\n",
      "训练次数：1323,Loss:1.8104248046875\n",
      "训练次数：1324,Loss:1.868768334388733\n",
      "训练次数：1325,Loss:1.71636164188385\n",
      "训练次数：1326,Loss:1.6891430616378784\n",
      "训练次数：1327,Loss:1.7952296733856201\n",
      "训练次数：1328,Loss:1.6578314304351807\n",
      "训练次数：1329,Loss:1.8881105184555054\n",
      "训练次数：1330,Loss:1.8347867727279663\n",
      "训练次数：1331,Loss:1.7906948328018188\n",
      "训练次数：1332,Loss:1.7234574556350708\n",
      "训练次数：1333,Loss:1.7402881383895874\n",
      "训练次数：1334,Loss:1.9814338684082031\n",
      "训练次数：1335,Loss:1.706872820854187\n",
      "训练次数：1336,Loss:1.8342528343200684\n",
      "训练次数：1337,Loss:1.7015985250473022\n",
      "训练次数：1338,Loss:1.6813961267471313\n",
      "训练次数：1339,Loss:1.9615055322647095\n",
      "训练次数：1340,Loss:1.8354490995407104\n",
      "训练次数：1341,Loss:1.814916968345642\n",
      "训练次数：1342,Loss:1.7122673988342285\n",
      "训练次数：1343,Loss:1.622819423675537\n",
      "训练次数：1344,Loss:1.768445372581482\n",
      "训练次数：1345,Loss:1.9350227117538452\n",
      "训练次数：1346,Loss:1.884250521659851\n",
      "训练次数：1347,Loss:1.8837254047393799\n",
      "训练次数：1348,Loss:1.8143274784088135\n",
      "训练次数：1349,Loss:1.684984564781189\n",
      "训练次数：1350,Loss:1.8131904602050781\n",
      "训练次数：1351,Loss:1.7256832122802734\n",
      "训练次数：1352,Loss:1.638674259185791\n",
      "训练次数：1353,Loss:1.7184265851974487\n",
      "训练次数：1354,Loss:1.6328771114349365\n",
      "训练次数：1355,Loss:1.6862736940383911\n",
      "训练次数：1356,Loss:1.7923084497451782\n",
      "训练次数：1357,Loss:1.7875864505767822\n",
      "训练次数：1358,Loss:1.811401128768921\n",
      "训练次数：1359,Loss:1.844801664352417\n",
      "训练次数：1360,Loss:1.8922276496887207\n",
      "训练次数：1361,Loss:1.673980474472046\n",
      "训练次数：1362,Loss:1.7638248205184937\n",
      "训练次数：1363,Loss:1.8138279914855957\n",
      "训练次数：1364,Loss:1.8032623529434204\n",
      "训练次数：1365,Loss:1.6401164531707764\n",
      "训练次数：1366,Loss:1.879523754119873\n",
      "训练次数：1367,Loss:1.8183943033218384\n",
      "训练次数：1368,Loss:1.6720682382583618\n",
      "训练次数：1369,Loss:1.8946938514709473\n",
      "训练次数：1370,Loss:1.765354037284851\n",
      "训练次数：1371,Loss:1.816359281539917\n",
      "训练次数：1372,Loss:1.781442403793335\n",
      "训练次数：1373,Loss:1.839852213859558\n",
      "训练次数：1374,Loss:1.651339054107666\n",
      "训练次数：1375,Loss:1.8128410577774048\n",
      "训练次数：1376,Loss:2.003772020339966\n",
      "训练次数：1377,Loss:1.8270494937896729\n",
      "训练次数：1378,Loss:2.0119946002960205\n",
      "训练次数：1379,Loss:1.748103141784668\n",
      "训练次数：1380,Loss:1.8287242650985718\n",
      "训练次数：1381,Loss:1.7730982303619385\n",
      "训练次数：1382,Loss:1.8209432363510132\n",
      "训练次数：1383,Loss:1.9314229488372803\n",
      "训练次数：1384,Loss:1.6650289297103882\n",
      "训练次数：1385,Loss:1.5558274984359741\n",
      "训练次数：1386,Loss:1.7840487957000732\n",
      "训练次数：1387,Loss:1.8759191036224365\n",
      "训练次数：1388,Loss:1.6210267543792725\n",
      "训练次数：1389,Loss:1.5947251319885254\n",
      "训练次数：1390,Loss:1.7439101934432983\n",
      "训练次数：1391,Loss:1.7436347007751465\n",
      "训练次数：1392,Loss:1.7849422693252563\n",
      "训练次数：1393,Loss:1.7187985181808472\n",
      "训练次数：1394,Loss:1.9830695390701294\n",
      "训练次数：1395,Loss:1.8809866905212402\n",
      "训练次数：1396,Loss:1.7314223051071167\n",
      "训练次数：1397,Loss:1.8526382446289062\n",
      "训练次数：1398,Loss:1.7393296957015991\n",
      "训练次数：1399,Loss:1.761221170425415\n",
      "训练次数：1400,Loss:1.7323882579803467\n",
      "训练次数：1401,Loss:1.747953176498413\n",
      "训练次数：1402,Loss:1.8606735467910767\n",
      "训练次数：1403,Loss:1.8168565034866333\n",
      "训练次数：1404,Loss:1.9477814435958862\n",
      "训练次数：1405,Loss:1.6567721366882324\n",
      "训练次数：1406,Loss:1.7764965295791626\n",
      "训练次数：1407,Loss:1.8494268655776978\n",
      "训练次数：1408,Loss:1.5963627099990845\n",
      "训练次数：1409,Loss:1.6661033630371094\n",
      "训练次数：1410,Loss:1.9305517673492432\n",
      "训练次数：1411,Loss:1.8539295196533203\n",
      "训练次数：1412,Loss:1.725022792816162\n",
      "训练次数：1413,Loss:1.8258963823318481\n",
      "训练次数：1414,Loss:1.8898340463638306\n",
      "训练次数：1415,Loss:1.7302049398422241\n",
      "训练次数：1416,Loss:1.5939358472824097\n",
      "训练次数：1417,Loss:1.7905762195587158\n",
      "训练次数：1418,Loss:1.6672649383544922\n",
      "训练次数：1419,Loss:2.1380865573883057\n",
      "训练次数：1420,Loss:1.8685022592544556\n",
      "训练次数：1421,Loss:1.79501473903656\n",
      "训练次数：1422,Loss:1.9737350940704346\n",
      "训练次数：1423,Loss:1.8287513256072998\n",
      "训练次数：1424,Loss:1.9316492080688477\n",
      "训练次数：1425,Loss:1.8281103372573853\n",
      "训练次数：1426,Loss:1.7156331539154053\n",
      "训练次数：1427,Loss:1.687421202659607\n",
      "训练次数：1428,Loss:2.127673387527466\n",
      "训练次数：1429,Loss:1.6360424757003784\n",
      "训练次数：1430,Loss:1.7292674779891968\n",
      "训练次数：1431,Loss:1.8354681730270386\n",
      "训练次数：1432,Loss:1.8285948038101196\n",
      "训练次数：1433,Loss:1.7116385698318481\n",
      "训练次数：1434,Loss:1.8759971857070923\n",
      "训练次数：1435,Loss:1.565433382987976\n",
      "训练次数：1436,Loss:1.6245555877685547\n",
      "训练次数：1437,Loss:1.756874680519104\n",
      "训练次数：1438,Loss:1.900187611579895\n",
      "训练次数：1439,Loss:1.9209001064300537\n",
      "训练次数：1440,Loss:1.7279038429260254\n",
      "训练次数：1441,Loss:1.8861438035964966\n",
      "训练次数：1442,Loss:1.8098328113555908\n",
      "训练次数：1443,Loss:1.973122239112854\n",
      "训练次数：1444,Loss:1.7330602407455444\n",
      "训练次数：1445,Loss:1.6072570085525513\n",
      "训练次数：1446,Loss:1.6569550037384033\n",
      "训练次数：1447,Loss:1.9123142957687378\n",
      "训练次数：1448,Loss:1.8366366624832153\n",
      "训练次数：1449,Loss:1.6774264574050903\n",
      "训练次数：1450,Loss:1.807242512702942\n",
      "训练次数：1451,Loss:1.8467962741851807\n",
      "训练次数：1452,Loss:1.7532305717468262\n",
      "训练次数：1453,Loss:1.8365085124969482\n",
      "训练次数：1454,Loss:1.7262120246887207\n",
      "训练次数：1455,Loss:1.6401922702789307\n",
      "训练次数：1456,Loss:1.7070024013519287\n",
      "训练次数：1457,Loss:1.7902485132217407\n",
      "训练次数：1458,Loss:1.7336558103561401\n",
      "训练次数：1459,Loss:1.6349838972091675\n",
      "训练次数：1460,Loss:1.579182744026184\n",
      "训练次数：1461,Loss:1.7810628414154053\n",
      "训练次数：1462,Loss:1.871415615081787\n",
      "训练次数：1463,Loss:1.8201831579208374\n",
      "训练次数：1464,Loss:1.5264874696731567\n",
      "训练次数：1465,Loss:1.737157940864563\n",
      "训练次数：1466,Loss:1.6764695644378662\n",
      "训练次数：1467,Loss:1.821076512336731\n",
      "训练次数：1468,Loss:2.0648791790008545\n",
      "训练次数：1469,Loss:1.8385612964630127\n",
      "训练次数：1470,Loss:1.648463249206543\n",
      "训练次数：1471,Loss:1.7513285875320435\n",
      "训练次数：1472,Loss:1.8378762006759644\n",
      "训练次数：1473,Loss:1.8867188692092896\n",
      "训练次数：1474,Loss:1.9066282510757446\n",
      "训练次数：1475,Loss:1.9582387208938599\n",
      "训练次数：1476,Loss:1.9419153928756714\n",
      "训练次数：1477,Loss:1.8028528690338135\n",
      "训练次数：1478,Loss:1.7443809509277344\n",
      "训练次数：1479,Loss:1.730495572090149\n",
      "训练次数：1480,Loss:1.7961013317108154\n",
      "训练次数：1481,Loss:1.686646580696106\n",
      "训练次数：1482,Loss:1.815438985824585\n",
      "训练次数：1483,Loss:1.7930856943130493\n",
      "训练次数：1484,Loss:1.854638934135437\n",
      "训练次数：1485,Loss:1.737526297569275\n",
      "训练次数：1486,Loss:1.549425721168518\n",
      "训练次数：1487,Loss:1.6861189603805542\n",
      "训练次数：1488,Loss:1.7938458919525146\n",
      "训练次数：1489,Loss:1.8622314929962158\n",
      "训练次数：1490,Loss:1.7377439737319946\n",
      "训练次数：1491,Loss:1.7349165678024292\n",
      "训练次数：1492,Loss:1.688977837562561\n",
      "训练次数：1493,Loss:1.8255404233932495\n",
      "训练次数：1494,Loss:1.797184705734253\n",
      "训练次数：1495,Loss:1.6777822971343994\n",
      "训练次数：1496,Loss:1.6760591268539429\n",
      "训练次数：1497,Loss:1.6727683544158936\n",
      "训练次数：1498,Loss:1.873049259185791\n",
      "训练次数：1499,Loss:1.7366124391555786\n",
      "训练次数：1500,Loss:1.8034180402755737\n",
      "训练次数：1501,Loss:1.7010579109191895\n",
      "训练次数：1502,Loss:1.7733485698699951\n",
      "训练次数：1503,Loss:1.5468229055404663\n",
      "训练次数：1504,Loss:1.7687466144561768\n",
      "训练次数：1505,Loss:1.6116156578063965\n",
      "训练次数：1506,Loss:1.8798226118087769\n",
      "训练次数：1507,Loss:1.743625521659851\n",
      "训练次数：1508,Loss:1.67625093460083\n",
      "训练次数：1509,Loss:1.842118501663208\n",
      "训练次数：1510,Loss:1.9704267978668213\n",
      "训练次数：1511,Loss:1.641696810722351\n",
      "训练次数：1512,Loss:1.8941718339920044\n",
      "训练次数：1513,Loss:1.5885049104690552\n",
      "训练次数：1514,Loss:1.7169123888015747\n",
      "训练次数：1515,Loss:1.8280357122421265\n",
      "训练次数：1516,Loss:1.7186131477355957\n",
      "训练次数：1517,Loss:1.6830929517745972\n",
      "训练次数：1518,Loss:1.9340816736221313\n",
      "训练次数：1519,Loss:1.616600751876831\n",
      "训练次数：1520,Loss:1.8540374040603638\n",
      "训练次数：1521,Loss:1.5857487916946411\n",
      "训练次数：1522,Loss:1.5663148164749146\n",
      "训练次数：1523,Loss:1.7732806205749512\n",
      "训练次数：1524,Loss:1.6991080045700073\n",
      "训练次数：1525,Loss:1.8307006359100342\n",
      "训练次数：1526,Loss:1.7374030351638794\n",
      "训练次数：1527,Loss:1.6716241836547852\n",
      "训练次数：1528,Loss:1.6841049194335938\n",
      "训练次数：1529,Loss:1.494964838027954\n",
      "训练次数：1530,Loss:1.9718574285507202\n",
      "训练次数：1531,Loss:1.872747540473938\n",
      "训练次数：1532,Loss:1.6597366333007812\n",
      "训练次数：1533,Loss:1.9502500295639038\n",
      "训练次数：1534,Loss:1.903885006904602\n",
      "训练次数：1535,Loss:1.7181087732315063\n",
      "训练次数：1536,Loss:1.7922492027282715\n",
      "训练次数：1537,Loss:1.7622575759887695\n",
      "训练次数：1538,Loss:1.789821743965149\n",
      "训练次数：1539,Loss:2.017996311187744\n",
      "训练次数：1540,Loss:1.6703879833221436\n",
      "训练次数：1541,Loss:1.7941944599151611\n",
      "训练次数：1542,Loss:1.832700490951538\n",
      "训练次数：1543,Loss:1.7411131858825684\n",
      "训练次数：1544,Loss:1.784794569015503\n",
      "训练次数：1545,Loss:1.8770387172698975\n",
      "训练次数：1546,Loss:1.7277895212173462\n",
      "训练次数：1547,Loss:1.6574690341949463\n",
      "训练次数：1548,Loss:1.818131446838379\n",
      "训练次数：1549,Loss:1.804824709892273\n",
      "训练次数：1550,Loss:1.767135739326477\n",
      "训练次数：1551,Loss:1.589854121208191\n",
      "训练次数：1552,Loss:1.5539170503616333\n",
      "训练次数：1553,Loss:1.753182053565979\n",
      "训练次数：1554,Loss:1.649113655090332\n",
      "训练次数：1555,Loss:1.6842296123504639\n",
      "训练次数：1556,Loss:1.7546182870864868\n",
      "训练次数：1557,Loss:1.7087600231170654\n",
      "训练次数：1558,Loss:1.5954512357711792\n",
      "训练次数：1559,Loss:1.9105582237243652\n",
      "训练次数：1560,Loss:1.8200148344039917\n",
      "训练次数：1561,Loss:1.7776215076446533\n",
      "训练次数：1562,Loss:1.55397629737854\n",
      "训练次数：1563,Loss:1.8087983131408691\n",
      "训练次数：1564,Loss:2.2793188095092773\n",
      "-----第 3 轮训练开始-----\n",
      "训练次数：1565,Loss:1.9356534481048584\n",
      "训练次数：1566,Loss:1.83677339553833\n",
      "训练次数：1567,Loss:1.8948180675506592\n",
      "训练次数：1568,Loss:1.5588184595108032\n",
      "训练次数：1569,Loss:1.5617775917053223\n",
      "训练次数：1570,Loss:1.6150164604187012\n",
      "训练次数：1571,Loss:1.6544219255447388\n",
      "训练次数：1572,Loss:1.7346065044403076\n",
      "训练次数：1573,Loss:1.665360450744629\n",
      "训练次数：1574,Loss:1.771325707435608\n",
      "训练次数：1575,Loss:1.7644520998001099\n",
      "训练次数：1576,Loss:1.7371554374694824\n",
      "训练次数：1577,Loss:1.8158414363861084\n",
      "训练次数：1578,Loss:1.867449164390564\n",
      "训练次数：1579,Loss:1.700918197631836\n",
      "训练次数：1580,Loss:1.5509519577026367\n",
      "训练次数：1581,Loss:1.608866572380066\n",
      "训练次数：1582,Loss:1.6608330011367798\n",
      "训练次数：1583,Loss:1.7544267177581787\n",
      "训练次数：1584,Loss:1.5818395614624023\n",
      "训练次数：1585,Loss:1.767782211303711\n",
      "训练次数：1586,Loss:1.698954463005066\n",
      "训练次数：1587,Loss:1.6822199821472168\n",
      "训练次数：1588,Loss:1.7289559841156006\n",
      "训练次数：1589,Loss:1.6642382144927979\n",
      "训练次数：1590,Loss:1.6479183435440063\n",
      "训练次数：1591,Loss:1.7452465295791626\n",
      "训练次数：1592,Loss:1.7463698387145996\n",
      "训练次数：1593,Loss:1.6209462881088257\n",
      "训练次数：1594,Loss:1.7437350749969482\n",
      "训练次数：1595,Loss:1.89543879032135\n",
      "训练次数：1596,Loss:1.9530366659164429\n",
      "训练次数：1597,Loss:1.9096403121948242\n",
      "训练次数：1598,Loss:1.592484474182129\n",
      "训练次数：1599,Loss:1.664807915687561\n",
      "训练次数：1600,Loss:1.789846420288086\n",
      "训练次数：1601,Loss:1.7209997177124023\n",
      "训练次数：1602,Loss:1.596459150314331\n",
      "训练次数：1603,Loss:1.633360743522644\n",
      "训练次数：1604,Loss:1.8303022384643555\n",
      "训练次数：1605,Loss:1.7504847049713135\n",
      "训练次数：1606,Loss:1.7624014616012573\n",
      "训练次数：1607,Loss:1.715679407119751\n",
      "训练次数：1608,Loss:1.8042975664138794\n",
      "训练次数：1609,Loss:1.5763205289840698\n",
      "训练次数：1610,Loss:1.6573247909545898\n",
      "训练次数：1611,Loss:1.783427357673645\n",
      "训练次数：1612,Loss:1.651843547821045\n",
      "训练次数：1613,Loss:1.8169013261795044\n",
      "训练次数：1614,Loss:1.7804882526397705\n",
      "训练次数：1615,Loss:1.7329847812652588\n",
      "训练次数：1616,Loss:1.7165874242782593\n",
      "训练次数：1617,Loss:1.7571028470993042\n",
      "训练次数：1618,Loss:1.8789691925048828\n",
      "训练次数：1619,Loss:1.5939103364944458\n",
      "训练次数：1620,Loss:1.5800185203552246\n",
      "训练次数：1621,Loss:1.7923084497451782\n",
      "训练次数：1622,Loss:1.7534120082855225\n",
      "训练次数：1623,Loss:1.5671898126602173\n",
      "训练次数：1624,Loss:1.6283055543899536\n",
      "训练次数：1625,Loss:1.8100687265396118\n",
      "训练次数：1626,Loss:1.951805591583252\n",
      "训练次数：1627,Loss:1.7603553533554077\n",
      "训练次数：1628,Loss:1.6426639556884766\n",
      "训练次数：1629,Loss:1.7514673471450806\n",
      "训练次数：1630,Loss:1.6241005659103394\n",
      "训练次数：1631,Loss:1.716713309288025\n",
      "训练次数：1632,Loss:1.7451825141906738\n",
      "训练次数：1633,Loss:1.659537672996521\n",
      "训练次数：1634,Loss:1.5970075130462646\n",
      "训练次数：1635,Loss:1.885298490524292\n",
      "训练次数：1636,Loss:1.5667868852615356\n",
      "训练次数：1637,Loss:1.6620707511901855\n",
      "训练次数：1638,Loss:1.6184684038162231\n",
      "训练次数：1639,Loss:1.6390764713287354\n",
      "训练次数：1640,Loss:1.6962958574295044\n",
      "训练次数：1641,Loss:1.6995211839675903\n",
      "训练次数：1642,Loss:1.7666099071502686\n",
      "训练次数：1643,Loss:1.5673195123672485\n",
      "训练次数：1644,Loss:1.940739393234253\n",
      "训练次数：1645,Loss:1.8128644227981567\n",
      "训练次数：1646,Loss:1.7935415506362915\n",
      "训练次数：1647,Loss:1.8789496421813965\n",
      "训练次数：1648,Loss:1.512894630432129\n",
      "训练次数：1649,Loss:1.6914265155792236\n",
      "训练次数：1650,Loss:1.7235904932022095\n",
      "训练次数：1651,Loss:1.6937189102172852\n",
      "训练次数：1652,Loss:1.4841570854187012\n",
      "训练次数：1653,Loss:1.6122894287109375\n",
      "训练次数：1654,Loss:1.677573561668396\n",
      "训练次数：1655,Loss:1.7359344959259033\n",
      "训练次数：1656,Loss:1.6176494359970093\n",
      "训练次数：1657,Loss:1.659261703491211\n",
      "训练次数：1658,Loss:1.5825759172439575\n",
      "训练次数：1659,Loss:1.7676100730895996\n",
      "训练次数：1660,Loss:1.777230143547058\n",
      "训练次数：1661,Loss:1.5096814632415771\n",
      "训练次数：1662,Loss:1.7094849348068237\n",
      "训练次数：1663,Loss:1.5887869596481323\n",
      "训练次数：1664,Loss:1.67181396484375\n",
      "训练次数：1665,Loss:1.6881738901138306\n",
      "训练次数：1666,Loss:1.5273948907852173\n",
      "训练次数：1667,Loss:1.8267652988433838\n",
      "训练次数：1668,Loss:2.0213098526000977\n",
      "训练次数：1669,Loss:2.431464672088623\n",
      "训练次数：1670,Loss:1.7847282886505127\n",
      "训练次数：1671,Loss:1.6951757669448853\n",
      "训练次数：1672,Loss:1.8718392848968506\n",
      "训练次数：1673,Loss:1.750069499015808\n",
      "训练次数：1674,Loss:1.452639102935791\n",
      "训练次数：1675,Loss:1.6312029361724854\n",
      "训练次数：1676,Loss:1.659903883934021\n",
      "训练次数：1677,Loss:1.701542854309082\n",
      "训练次数：1678,Loss:1.4776906967163086\n",
      "训练次数：1679,Loss:1.749969244003296\n",
      "训练次数：1680,Loss:1.7272636890411377\n",
      "训练次数：1681,Loss:1.6621578931808472\n",
      "训练次数：1682,Loss:1.6130023002624512\n",
      "训练次数：1683,Loss:1.8595020771026611\n",
      "训练次数：1684,Loss:1.8556030988693237\n",
      "训练次数：1685,Loss:1.7377151250839233\n",
      "训练次数：1686,Loss:1.7692639827728271\n",
      "训练次数：1687,Loss:1.7622207403182983\n",
      "训练次数：1688,Loss:1.6792795658111572\n",
      "训练次数：1689,Loss:1.6326056718826294\n",
      "训练次数：1690,Loss:1.6151193380355835\n",
      "训练次数：1691,Loss:1.6263957023620605\n",
      "训练次数：1692,Loss:1.5889408588409424\n",
      "训练次数：1693,Loss:1.6608680486679077\n",
      "训练次数：1694,Loss:1.5533959865570068\n",
      "训练次数：1695,Loss:1.8290390968322754\n",
      "训练次数：1696,Loss:1.7618094682693481\n",
      "训练次数：1697,Loss:1.9324469566345215\n",
      "训练次数：1698,Loss:1.4338546991348267\n",
      "训练次数：1699,Loss:1.6502372026443481\n",
      "训练次数：1700,Loss:1.6670594215393066\n",
      "训练次数：1701,Loss:1.7656745910644531\n",
      "训练次数：1702,Loss:1.8246533870697021\n",
      "训练次数：1703,Loss:1.6964141130447388\n",
      "训练次数：1704,Loss:1.8596001863479614\n",
      "训练次数：1705,Loss:1.994633674621582\n",
      "训练次数：1706,Loss:1.7170387506484985\n",
      "训练次数：1707,Loss:1.6972330808639526\n",
      "训练次数：1708,Loss:1.5918278694152832\n",
      "训练次数：1709,Loss:1.640247106552124\n",
      "训练次数：1710,Loss:1.8767331838607788\n",
      "训练次数：1711,Loss:1.6980499029159546\n",
      "训练次数：1712,Loss:1.8610255718231201\n",
      "训练次数：1713,Loss:1.5410494804382324\n",
      "训练次数：1714,Loss:1.6478488445281982\n",
      "训练次数：1715,Loss:1.5972360372543335\n",
      "训练次数：1716,Loss:1.737928867340088\n",
      "训练次数：1717,Loss:1.7631564140319824\n",
      "训练次数：1718,Loss:1.5805919170379639\n",
      "训练次数：1719,Loss:1.6727346181869507\n",
      "训练次数：1720,Loss:1.683424472808838\n",
      "训练次数：1721,Loss:1.761900544166565\n",
      "训练次数：1722,Loss:1.6306467056274414\n",
      "训练次数：1723,Loss:1.8694195747375488\n",
      "训练次数：1724,Loss:2.129265785217285\n",
      "训练次数：1725,Loss:1.7373653650283813\n",
      "训练次数：1726,Loss:1.6315277814865112\n",
      "训练次数：1727,Loss:1.7553002834320068\n",
      "训练次数：1728,Loss:1.833164095878601\n",
      "训练次数：1729,Loss:1.6682147979736328\n",
      "训练次数：1730,Loss:1.944288730621338\n",
      "训练次数：1731,Loss:1.6886454820632935\n",
      "训练次数：1732,Loss:1.640844464302063\n",
      "训练次数：1733,Loss:1.6001784801483154\n",
      "训练次数：1734,Loss:1.5945651531219482\n",
      "训练次数：1735,Loss:1.7705820798873901\n",
      "训练次数：1736,Loss:1.668893814086914\n",
      "训练次数：1737,Loss:1.6474295854568481\n",
      "训练次数：1738,Loss:1.8350311517715454\n",
      "训练次数：1739,Loss:1.5649889707565308\n",
      "训练次数：1740,Loss:1.6820183992385864\n",
      "训练次数：1741,Loss:1.599007248878479\n",
      "训练次数：1742,Loss:1.78865647315979\n",
      "训练次数：1743,Loss:1.766631007194519\n",
      "训练次数：1744,Loss:1.7468409538269043\n",
      "训练次数：1745,Loss:1.6521728038787842\n",
      "训练次数：1746,Loss:1.5439777374267578\n",
      "训练次数：1747,Loss:1.5480858087539673\n",
      "训练次数：1748,Loss:1.478291392326355\n",
      "训练次数：1749,Loss:1.792314052581787\n",
      "训练次数：1750,Loss:1.9417917728424072\n",
      "训练次数：1751,Loss:1.6256556510925293\n",
      "训练次数：1752,Loss:1.6783788204193115\n",
      "训练次数：1753,Loss:1.9140145778656006\n",
      "训练次数：1754,Loss:1.6563022136688232\n",
      "训练次数：1755,Loss:1.4991415739059448\n",
      "训练次数：1756,Loss:1.751565933227539\n",
      "训练次数：1757,Loss:1.8596932888031006\n",
      "训练次数：1758,Loss:1.5855892896652222\n",
      "训练次数：1759,Loss:1.6787331104278564\n",
      "训练次数：1760,Loss:1.8026355504989624\n",
      "训练次数：1761,Loss:1.678131103515625\n",
      "训练次数：1762,Loss:1.6185253858566284\n",
      "训练次数：1763,Loss:1.6653540134429932\n",
      "训练次数：1764,Loss:1.701863408088684\n",
      "训练次数：1765,Loss:1.4923661947250366\n",
      "训练次数：1766,Loss:1.9931963682174683\n",
      "训练次数：1767,Loss:1.73641836643219\n",
      "训练次数：1768,Loss:1.6865887641906738\n",
      "训练次数：1769,Loss:1.623181939125061\n",
      "训练次数：1770,Loss:1.8931065797805786\n",
      "训练次数：1771,Loss:1.776296854019165\n",
      "训练次数：1772,Loss:1.7645622491836548\n",
      "训练次数：1773,Loss:1.7568501234054565\n",
      "训练次数：1774,Loss:1.8887944221496582\n",
      "训练次数：1775,Loss:1.5255286693572998\n",
      "训练次数：1776,Loss:1.7275686264038086\n",
      "训练次数：1777,Loss:1.6326171159744263\n",
      "训练次数：1778,Loss:1.6538108587265015\n",
      "训练次数：1779,Loss:1.7652446031570435\n",
      "训练次数：1780,Loss:1.749444603919983\n",
      "训练次数：1781,Loss:1.669250726699829\n",
      "训练次数：1782,Loss:1.6841808557510376\n",
      "训练次数：1783,Loss:1.6874279975891113\n",
      "训练次数：1784,Loss:1.6626570224761963\n",
      "训练次数：1785,Loss:1.696049451828003\n",
      "训练次数：1786,Loss:1.7014738321304321\n",
      "训练次数：1787,Loss:1.655288815498352\n",
      "训练次数：1788,Loss:1.8718645572662354\n",
      "训练次数：1789,Loss:1.9143949747085571\n",
      "训练次数：1790,Loss:1.7262916564941406\n",
      "训练次数：1791,Loss:1.6708661317825317\n",
      "训练次数：1792,Loss:1.6159300804138184\n",
      "训练次数：1793,Loss:1.6358706951141357\n",
      "训练次数：1794,Loss:1.9186488389968872\n",
      "训练次数：1795,Loss:1.7483514547348022\n",
      "训练次数：1796,Loss:1.565144658088684\n",
      "训练次数：1797,Loss:1.669718861579895\n",
      "训练次数：1798,Loss:1.730337381362915\n",
      "训练次数：1799,Loss:1.7857365608215332\n",
      "训练次数：1800,Loss:1.9364749193191528\n",
      "训练次数：1801,Loss:1.9900431632995605\n",
      "训练次数：1802,Loss:1.7085471153259277\n",
      "训练次数：1803,Loss:1.5382699966430664\n",
      "训练次数：1804,Loss:1.6423522233963013\n",
      "训练次数：1805,Loss:1.7609626054763794\n",
      "训练次数：1806,Loss:1.621693730354309\n",
      "训练次数：1807,Loss:1.6661741733551025\n",
      "训练次数：1808,Loss:1.9799753427505493\n",
      "训练次数：1809,Loss:1.5308068990707397\n",
      "训练次数：1810,Loss:1.7012121677398682\n",
      "训练次数：1811,Loss:1.7056761980056763\n",
      "训练次数：1812,Loss:1.8120124340057373\n",
      "训练次数：1813,Loss:1.8030203580856323\n",
      "训练次数：1814,Loss:1.8114887475967407\n",
      "训练次数：1815,Loss:1.6296873092651367\n",
      "训练次数：1816,Loss:1.6320551633834839\n",
      "训练次数：1817,Loss:1.6718711853027344\n",
      "训练次数：1818,Loss:1.6749839782714844\n",
      "训练次数：1819,Loss:1.7353748083114624\n",
      "训练次数：1820,Loss:1.52352774143219\n",
      "训练次数：1821,Loss:1.646406888961792\n",
      "训练次数：1822,Loss:1.5114752054214478\n",
      "训练次数：1823,Loss:1.8552592992782593\n",
      "训练次数：1824,Loss:1.8105353116989136\n",
      "训练次数：1825,Loss:1.6736137866973877\n",
      "训练次数：1826,Loss:1.830007791519165\n",
      "训练次数：1827,Loss:1.6638028621673584\n",
      "训练次数：1828,Loss:1.7104336023330688\n",
      "训练次数：1829,Loss:1.4476275444030762\n",
      "训练次数：1830,Loss:1.4654921293258667\n",
      "训练次数：1831,Loss:1.7345870733261108\n",
      "训练次数：1832,Loss:1.6417254209518433\n",
      "训练次数：1833,Loss:1.69974946975708\n",
      "训练次数：1834,Loss:1.7000477313995361\n",
      "训练次数：1835,Loss:1.8194081783294678\n",
      "训练次数：1836,Loss:1.7628687620162964\n",
      "训练次数：1837,Loss:1.5854482650756836\n",
      "训练次数：1838,Loss:1.5708945989608765\n",
      "训练次数：1839,Loss:1.6219501495361328\n",
      "训练次数：1840,Loss:1.6416411399841309\n",
      "训练次数：1841,Loss:2.0092978477478027\n",
      "训练次数：1842,Loss:1.7847639322280884\n",
      "训练次数：1843,Loss:1.7209465503692627\n",
      "训练次数：1844,Loss:1.5133700370788574\n",
      "训练次数：1845,Loss:1.6183037757873535\n",
      "训练次数：1846,Loss:1.6587724685668945\n",
      "训练次数：1847,Loss:1.6384913921356201\n",
      "训练次数：1848,Loss:1.6822351217269897\n",
      "训练次数：1849,Loss:1.6096376180648804\n",
      "训练次数：1850,Loss:1.6555768251419067\n",
      "训练次数：1851,Loss:1.73972749710083\n",
      "训练次数：1852,Loss:1.6145384311676025\n",
      "训练次数：1853,Loss:1.7696294784545898\n",
      "训练次数：1854,Loss:1.5991220474243164\n",
      "训练次数：1855,Loss:1.5969840288162231\n",
      "训练次数：1856,Loss:1.682404637336731\n",
      "训练次数：1857,Loss:1.6745209693908691\n",
      "训练次数：1858,Loss:1.552731990814209\n",
      "训练次数：1859,Loss:1.678635597229004\n",
      "训练次数：1860,Loss:1.6053119897842407\n",
      "训练次数：1861,Loss:1.934914231300354\n",
      "训练次数：1862,Loss:1.6777520179748535\n",
      "训练次数：1863,Loss:1.4916445016860962\n",
      "训练次数：1864,Loss:1.671634316444397\n",
      "训练次数：1865,Loss:1.671437382698059\n",
      "训练次数：1866,Loss:1.7982679605484009\n",
      "训练次数：1867,Loss:1.5978615283966064\n",
      "训练次数：1868,Loss:1.464901089668274\n",
      "训练次数：1869,Loss:1.6144537925720215\n",
      "训练次数：1870,Loss:1.8401588201522827\n",
      "训练次数：1871,Loss:1.5674833059310913\n",
      "训练次数：1872,Loss:1.7330961227416992\n",
      "训练次数：1873,Loss:1.6255632638931274\n",
      "训练次数：1874,Loss:1.552445650100708\n",
      "训练次数：1875,Loss:1.5669506788253784\n",
      "训练次数：1876,Loss:1.5721684694290161\n",
      "训练次数：1877,Loss:1.817223072052002\n",
      "训练次数：1878,Loss:1.701047420501709\n",
      "训练次数：1879,Loss:1.5395995378494263\n",
      "训练次数：1880,Loss:1.8615446090698242\n",
      "训练次数：1881,Loss:1.8021290302276611\n",
      "训练次数：1882,Loss:1.8826314210891724\n",
      "训练次数：1883,Loss:1.4757872819900513\n",
      "训练次数：1884,Loss:1.5887789726257324\n",
      "训练次数：1885,Loss:1.654687523841858\n",
      "训练次数：1886,Loss:1.8181904554367065\n",
      "训练次数：1887,Loss:1.5891294479370117\n",
      "训练次数：1888,Loss:1.8066432476043701\n",
      "训练次数：1889,Loss:1.5632702112197876\n",
      "训练次数：1890,Loss:1.5072879791259766\n",
      "训练次数：1891,Loss:1.5998084545135498\n",
      "训练次数：1892,Loss:1.6572492122650146\n",
      "训练次数：1893,Loss:1.5808722972869873\n",
      "训练次数：1894,Loss:1.6545004844665527\n",
      "训练次数：1895,Loss:1.6037771701812744\n",
      "训练次数：1896,Loss:1.6525965929031372\n",
      "训练次数：1897,Loss:1.5342154502868652\n",
      "训练次数：1898,Loss:1.7504339218139648\n",
      "训练次数：1899,Loss:1.7427467107772827\n",
      "训练次数：1900,Loss:1.7213099002838135\n",
      "训练次数：1901,Loss:1.6840674877166748\n",
      "训练次数：1902,Loss:1.5450819730758667\n",
      "训练次数：1903,Loss:1.6753743886947632\n",
      "训练次数：1904,Loss:1.5106189250946045\n",
      "训练次数：1905,Loss:1.7432557344436646\n",
      "训练次数：1906,Loss:1.747301697731018\n",
      "训练次数：1907,Loss:1.5415583848953247\n",
      "训练次数：1908,Loss:1.7099858522415161\n",
      "训练次数：1909,Loss:1.6608599424362183\n",
      "训练次数：1910,Loss:1.483405590057373\n",
      "训练次数：1911,Loss:1.596704363822937\n",
      "训练次数：1912,Loss:1.7202976942062378\n",
      "训练次数：1913,Loss:1.6803175210952759\n",
      "训练次数：1914,Loss:1.528668761253357\n",
      "训练次数：1915,Loss:1.620733380317688\n",
      "训练次数：1916,Loss:1.6823627948760986\n",
      "训练次数：1917,Loss:1.5625544786453247\n",
      "训练次数：1918,Loss:1.8141170740127563\n",
      "训练次数：1919,Loss:1.57731032371521\n",
      "训练次数：1920,Loss:1.6356009244918823\n",
      "训练次数：1921,Loss:1.6295969486236572\n",
      "训练次数：1922,Loss:1.7123808860778809\n",
      "训练次数：1923,Loss:1.5556292533874512\n",
      "训练次数：1924,Loss:1.805647611618042\n",
      "训练次数：1925,Loss:1.6534377336502075\n",
      "训练次数：1926,Loss:1.491947054862976\n",
      "训练次数：1927,Loss:1.8662927150726318\n",
      "训练次数：1928,Loss:1.5933245420455933\n",
      "训练次数：1929,Loss:1.2736262083053589\n",
      "训练次数：1930,Loss:1.4555820226669312\n",
      "训练次数：1931,Loss:1.6314946413040161\n",
      "训练次数：1932,Loss:1.7655431032180786\n",
      "训练次数：1933,Loss:1.4723693132400513\n",
      "训练次数：1934,Loss:1.5651564598083496\n",
      "训练次数：1935,Loss:1.3950902223587036\n",
      "训练次数：1936,Loss:1.719967246055603\n",
      "训练次数：1937,Loss:1.5557535886764526\n",
      "训练次数：1938,Loss:1.5429210662841797\n",
      "训练次数：1939,Loss:1.8056095838546753\n",
      "训练次数：1940,Loss:1.587165117263794\n",
      "训练次数：1941,Loss:1.5760767459869385\n",
      "训练次数：1942,Loss:1.5401127338409424\n",
      "训练次数：1943,Loss:1.6053121089935303\n",
      "训练次数：1944,Loss:1.6283073425292969\n",
      "训练次数：1945,Loss:1.7467840909957886\n",
      "训练次数：1946,Loss:1.5488789081573486\n",
      "训练次数：1947,Loss:1.5781426429748535\n",
      "训练次数：1948,Loss:1.5193735361099243\n",
      "训练次数：1949,Loss:1.558044195175171\n",
      "训练次数：1950,Loss:1.5804213285446167\n",
      "训练次数：1951,Loss:1.5227329730987549\n",
      "训练次数：1952,Loss:1.8060898780822754\n",
      "训练次数：1953,Loss:1.7194479703903198\n",
      "训练次数：1954,Loss:1.4301730394363403\n",
      "训练次数：1955,Loss:1.6249104738235474\n",
      "训练次数：1956,Loss:1.4158867597579956\n",
      "训练次数：1957,Loss:1.5481103658676147\n",
      "训练次数：1958,Loss:1.724886417388916\n",
      "训练次数：1959,Loss:1.6677937507629395\n",
      "训练次数：1960,Loss:1.6641480922698975\n",
      "训练次数：1961,Loss:1.6159892082214355\n",
      "训练次数：1962,Loss:1.718822717666626\n",
      "训练次数：1963,Loss:1.4708237648010254\n",
      "训练次数：1964,Loss:1.6255366802215576\n",
      "训练次数：1965,Loss:1.7356573343276978\n",
      "训练次数：1966,Loss:1.7539825439453125\n",
      "训练次数：1967,Loss:1.6755844354629517\n",
      "训练次数：1968,Loss:1.5275472402572632\n",
      "训练次数：1969,Loss:1.7774147987365723\n",
      "训练次数：1970,Loss:1.5420092344284058\n",
      "训练次数：1971,Loss:1.6093543767929077\n",
      "训练次数：1972,Loss:1.3529694080352783\n",
      "训练次数：1973,Loss:1.527160882949829\n",
      "训练次数：1974,Loss:1.474394679069519\n",
      "训练次数：1975,Loss:1.7709060907363892\n",
      "训练次数：1976,Loss:1.6377816200256348\n",
      "训练次数：1977,Loss:1.752823829650879\n",
      "训练次数：1978,Loss:1.684913158416748\n",
      "训练次数：1979,Loss:1.7214893102645874\n",
      "训练次数：1980,Loss:1.568885087966919\n",
      "训练次数：1981,Loss:1.8202474117279053\n",
      "训练次数：1982,Loss:1.5461114645004272\n",
      "训练次数：1983,Loss:1.5831102132797241\n",
      "训练次数：1984,Loss:1.6715763807296753\n",
      "训练次数：1985,Loss:1.6388918161392212\n",
      "训练次数：1986,Loss:1.5767931938171387\n",
      "训练次数：1987,Loss:1.7796729803085327\n",
      "训练次数：1988,Loss:1.5991491079330444\n",
      "训练次数：1989,Loss:1.5266095399856567\n",
      "训练次数：1990,Loss:1.742123007774353\n",
      "训练次数：1991,Loss:1.7865095138549805\n",
      "训练次数：1992,Loss:1.5490636825561523\n",
      "训练次数：1993,Loss:1.7170066833496094\n",
      "训练次数：1994,Loss:1.7376407384872437\n",
      "训练次数：1995,Loss:1.6292483806610107\n",
      "训练次数：1996,Loss:1.5887168645858765\n",
      "训练次数：1997,Loss:1.7996567487716675\n",
      "训练次数：1998,Loss:1.5786807537078857\n",
      "训练次数：1999,Loss:1.599722981452942\n",
      "训练次数：2000,Loss:1.8820120096206665\n",
      "训练次数：2001,Loss:1.9108655452728271\n",
      "训练次数：2002,Loss:1.7429478168487549\n",
      "训练次数：2003,Loss:1.6995956897735596\n",
      "训练次数：2004,Loss:1.6731642484664917\n",
      "训练次数：2005,Loss:1.7848103046417236\n",
      "训练次数：2006,Loss:1.6219854354858398\n",
      "训练次数：2007,Loss:1.6927263736724854\n",
      "训练次数：2008,Loss:1.543434739112854\n",
      "训练次数：2009,Loss:1.6594568490982056\n",
      "训练次数：2010,Loss:1.6950479745864868\n",
      "训练次数：2011,Loss:1.6443049907684326\n",
      "训练次数：2012,Loss:1.6514346599578857\n",
      "训练次数：2013,Loss:1.4534008502960205\n",
      "训练次数：2014,Loss:1.740121841430664\n",
      "训练次数：2015,Loss:1.45128333568573\n",
      "训练次数：2016,Loss:1.3516515493392944\n",
      "训练次数：2017,Loss:1.5437132120132446\n",
      "训练次数：2018,Loss:1.5712804794311523\n",
      "训练次数：2019,Loss:1.7461830377578735\n",
      "训练次数：2020,Loss:1.6584464311599731\n",
      "训练次数：2021,Loss:1.7367308139801025\n",
      "训练次数：2022,Loss:1.4761089086532593\n",
      "训练次数：2023,Loss:1.4877784252166748\n",
      "训练次数：2024,Loss:1.5534709692001343\n",
      "训练次数：2025,Loss:1.7362725734710693\n",
      "训练次数：2026,Loss:1.6337374448776245\n",
      "训练次数：2027,Loss:1.588340401649475\n",
      "训练次数：2028,Loss:1.6305302381515503\n",
      "训练次数：2029,Loss:1.668928623199463\n",
      "训练次数：2030,Loss:1.5221977233886719\n",
      "训练次数：2031,Loss:1.7031941413879395\n",
      "训练次数：2032,Loss:1.4990280866622925\n",
      "训练次数：2033,Loss:1.5769219398498535\n",
      "训练次数：2034,Loss:1.6956714391708374\n",
      "训练次数：2035,Loss:1.5846924781799316\n",
      "训练次数：2036,Loss:1.7188814878463745\n",
      "训练次数：2037,Loss:1.4767205715179443\n",
      "训练次数：2038,Loss:1.7468384504318237\n",
      "训练次数：2039,Loss:1.3792983293533325\n",
      "训练次数：2040,Loss:1.7720885276794434\n",
      "训练次数：2041,Loss:1.639885425567627\n",
      "训练次数：2042,Loss:1.9434700012207031\n",
      "训练次数：2043,Loss:1.9082093238830566\n",
      "训练次数：2044,Loss:1.7046594619750977\n",
      "训练次数：2045,Loss:1.7113163471221924\n",
      "训练次数：2046,Loss:1.7601345777511597\n",
      "训练次数：2047,Loss:1.6799105405807495\n",
      "训练次数：2048,Loss:1.4723484516143799\n",
      "训练次数：2049,Loss:1.4995231628417969\n",
      "训练次数：2050,Loss:1.4142955541610718\n",
      "训练次数：2051,Loss:1.761176586151123\n",
      "训练次数：2052,Loss:1.7532585859298706\n",
      "训练次数：2053,Loss:1.6407314538955688\n",
      "训练次数：2054,Loss:1.4591344594955444\n",
      "训练次数：2055,Loss:1.871991753578186\n",
      "训练次数：2056,Loss:1.6831903457641602\n",
      "训练次数：2057,Loss:1.5946365594863892\n",
      "训练次数：2058,Loss:1.4820104837417603\n",
      "训练次数：2059,Loss:1.6118863821029663\n",
      "训练次数：2060,Loss:1.632278561592102\n",
      "训练次数：2061,Loss:1.6318001747131348\n",
      "训练次数：2062,Loss:1.7137473821640015\n",
      "训练次数：2063,Loss:1.8928643465042114\n",
      "训练次数：2064,Loss:1.672420620918274\n",
      "训练次数：2065,Loss:1.6674628257751465\n",
      "训练次数：2066,Loss:1.6592665910720825\n",
      "训练次数：2067,Loss:1.5597929954528809\n",
      "训练次数：2068,Loss:1.7908589839935303\n",
      "训练次数：2069,Loss:1.6808301210403442\n",
      "训练次数：2070,Loss:1.664817214012146\n",
      "训练次数：2071,Loss:1.5492132902145386\n",
      "训练次数：2072,Loss:1.5927717685699463\n",
      "训练次数：2073,Loss:1.7018362283706665\n",
      "训练次数：2074,Loss:1.5515565872192383\n",
      "训练次数：2075,Loss:1.7019678354263306\n",
      "训练次数：2076,Loss:1.5129153728485107\n",
      "训练次数：2077,Loss:1.7183177471160889\n",
      "训练次数：2078,Loss:1.6536306142807007\n",
      "训练次数：2079,Loss:1.9230278730392456\n",
      "训练次数：2080,Loss:1.5849723815917969\n",
      "训练次数：2081,Loss:1.848503828048706\n",
      "训练次数：2082,Loss:1.4102258682250977\n",
      "训练次数：2083,Loss:1.5987043380737305\n",
      "训练次数：2084,Loss:1.5883240699768066\n",
      "训练次数：2085,Loss:1.7388805150985718\n",
      "训练次数：2086,Loss:1.7146488428115845\n",
      "训练次数：2087,Loss:1.6689246892929077\n",
      "训练次数：2088,Loss:1.7274960279464722\n",
      "训练次数：2089,Loss:1.80381441116333\n",
      "训练次数：2090,Loss:1.8169357776641846\n",
      "训练次数：2091,Loss:1.6543630361557007\n",
      "训练次数：2092,Loss:1.5768415927886963\n",
      "训练次数：2093,Loss:1.6617522239685059\n",
      "训练次数：2094,Loss:1.8949624300003052\n",
      "训练次数：2095,Loss:1.8354995250701904\n",
      "训练次数：2096,Loss:1.733262300491333\n",
      "训练次数：2097,Loss:1.64358389377594\n",
      "训练次数：2098,Loss:1.4248934984207153\n",
      "训练次数：2099,Loss:1.57102370262146\n",
      "训练次数：2100,Loss:1.5312718152999878\n",
      "训练次数：2101,Loss:1.6846133470535278\n",
      "训练次数：2102,Loss:1.5959556102752686\n",
      "训练次数：2103,Loss:1.6230534315109253\n",
      "训练次数：2104,Loss:1.6831952333450317\n",
      "训练次数：2105,Loss:1.6728184223175049\n",
      "训练次数：2106,Loss:1.71144700050354\n",
      "训练次数：2107,Loss:1.5933960676193237\n",
      "训练次数：2108,Loss:1.5795387029647827\n",
      "训练次数：2109,Loss:1.6078252792358398\n",
      "训练次数：2110,Loss:1.5401766300201416\n",
      "训练次数：2111,Loss:1.7085981369018555\n",
      "训练次数：2112,Loss:1.655726671218872\n",
      "训练次数：2113,Loss:1.6671643257141113\n",
      "训练次数：2114,Loss:1.6041754484176636\n",
      "训练次数：2115,Loss:1.507758378982544\n",
      "训练次数：2116,Loss:1.8027567863464355\n",
      "训练次数：2117,Loss:1.5054783821105957\n",
      "训练次数：2118,Loss:1.6691025495529175\n",
      "训练次数：2119,Loss:1.4971152544021606\n",
      "训练次数：2120,Loss:1.4973620176315308\n",
      "训练次数：2121,Loss:1.7968416213989258\n",
      "训练次数：2122,Loss:1.6901463270187378\n",
      "训练次数：2123,Loss:1.6188937425613403\n",
      "训练次数：2124,Loss:1.5503214597702026\n",
      "训练次数：2125,Loss:1.4687604904174805\n",
      "训练次数：2126,Loss:1.5964534282684326\n",
      "训练次数：2127,Loss:1.7624391317367554\n",
      "训练次数：2128,Loss:1.6961857080459595\n",
      "训练次数：2129,Loss:1.6502981185913086\n",
      "训练次数：2130,Loss:1.6537516117095947\n",
      "训练次数：2131,Loss:1.510553240776062\n",
      "训练次数：2132,Loss:1.6983493566513062\n",
      "训练次数：2133,Loss:1.6205782890319824\n",
      "训练次数：2134,Loss:1.4637751579284668\n",
      "训练次数：2135,Loss:1.6132323741912842\n",
      "训练次数：2136,Loss:1.481083631515503\n",
      "训练次数：2137,Loss:1.4697517156600952\n",
      "训练次数：2138,Loss:1.6397868394851685\n",
      "训练次数：2139,Loss:1.4992821216583252\n",
      "训练次数：2140,Loss:1.5861972570419312\n",
      "训练次数：2141,Loss:1.809314250946045\n",
      "训练次数：2142,Loss:1.8152992725372314\n",
      "训练次数：2143,Loss:1.4567121267318726\n",
      "训练次数：2144,Loss:1.6119744777679443\n",
      "训练次数：2145,Loss:1.6512287855148315\n",
      "训练次数：2146,Loss:1.6717772483825684\n",
      "训练次数：2147,Loss:1.4329670667648315\n",
      "训练次数：2148,Loss:1.6799730062484741\n",
      "训练次数：2149,Loss:1.6386795043945312\n",
      "训练次数：2150,Loss:1.4569965600967407\n",
      "训练次数：2151,Loss:1.7573699951171875\n",
      "训练次数：2152,Loss:1.5917102098464966\n",
      "训练次数：2153,Loss:1.6539900302886963\n",
      "训练次数：2154,Loss:1.5909372568130493\n",
      "训练次数：2155,Loss:1.7044106721878052\n",
      "训练次数：2156,Loss:1.4705078601837158\n",
      "训练次数：2157,Loss:1.6880342960357666\n",
      "训练次数：2158,Loss:1.8808350563049316\n",
      "训练次数：2159,Loss:1.6383122205734253\n",
      "训练次数：2160,Loss:1.9160144329071045\n",
      "训练次数：2161,Loss:1.6051263809204102\n",
      "训练次数：2162,Loss:1.6668225526809692\n",
      "训练次数：2163,Loss:1.5974634885787964\n",
      "训练次数：2164,Loss:1.6858068704605103\n",
      "训练次数：2165,Loss:1.7328546047210693\n",
      "训练次数：2166,Loss:1.511138677597046\n",
      "训练次数：2167,Loss:1.371250867843628\n",
      "训练次数：2168,Loss:1.6457979679107666\n",
      "训练次数：2169,Loss:1.6354687213897705\n",
      "训练次数：2170,Loss:1.4572913646697998\n",
      "训练次数：2171,Loss:1.4197412729263306\n",
      "训练次数：2172,Loss:1.6245241165161133\n",
      "训练次数：2173,Loss:1.614157795906067\n",
      "训练次数：2174,Loss:1.682347297668457\n",
      "训练次数：2175,Loss:1.5873485803604126\n",
      "训练次数：2176,Loss:1.792341709136963\n",
      "训练次数：2177,Loss:1.7083909511566162\n",
      "训练次数：2178,Loss:1.539235234260559\n",
      "训练次数：2179,Loss:1.6738216876983643\n",
      "训练次数：2180,Loss:1.5648081302642822\n",
      "训练次数：2181,Loss:1.577089548110962\n",
      "训练次数：2182,Loss:1.5444728136062622\n",
      "训练次数：2183,Loss:1.6541179418563843\n",
      "训练次数：2184,Loss:1.734462022781372\n",
      "训练次数：2185,Loss:1.652719259262085\n",
      "训练次数：2186,Loss:1.7932406663894653\n",
      "训练次数：2187,Loss:1.5583927631378174\n",
      "训练次数：2188,Loss:1.5504547357559204\n",
      "训练次数：2189,Loss:1.6547483205795288\n",
      "训练次数：2190,Loss:1.4131395816802979\n",
      "训练次数：2191,Loss:1.5219043493270874\n",
      "训练次数：2192,Loss:1.837403416633606\n",
      "训练次数：2193,Loss:1.781075358390808\n",
      "训练次数：2194,Loss:1.538131594657898\n",
      "训练次数：2195,Loss:1.6108187437057495\n",
      "训练次数：2196,Loss:1.7389006614685059\n",
      "训练次数：2197,Loss:1.5904502868652344\n",
      "训练次数：2198,Loss:1.418647289276123\n",
      "训练次数：2199,Loss:1.6865270137786865\n",
      "训练次数：2200,Loss:1.479793667793274\n",
      "训练次数：2201,Loss:1.8465884923934937\n",
      "训练次数：2202,Loss:1.6720761060714722\n",
      "训练次数：2203,Loss:1.6926692724227905\n",
      "训练次数：2204,Loss:1.8440810441970825\n",
      "训练次数：2205,Loss:1.6954638957977295\n",
      "训练次数：2206,Loss:1.790417194366455\n",
      "训练次数：2207,Loss:1.6820539236068726\n",
      "训练次数：2208,Loss:1.496414065361023\n",
      "训练次数：2209,Loss:1.5027424097061157\n",
      "训练次数：2210,Loss:1.930501937866211\n",
      "训练次数：2211,Loss:1.507962703704834\n",
      "训练次数：2212,Loss:1.50259530544281\n",
      "训练次数：2213,Loss:1.6828136444091797\n",
      "训练次数：2214,Loss:1.7623144388198853\n",
      "训练次数：2215,Loss:1.5170602798461914\n",
      "训练次数：2216,Loss:1.738425374031067\n",
      "训练次数：2217,Loss:1.4099950790405273\n",
      "训练次数：2218,Loss:1.4578189849853516\n",
      "训练次数：2219,Loss:1.6115195751190186\n",
      "训练次数：2220,Loss:1.6205790042877197\n",
      "训练次数：2221,Loss:1.7216689586639404\n",
      "训练次数：2222,Loss:1.5351800918579102\n",
      "训练次数：2223,Loss:1.7423264980316162\n",
      "训练次数：2224,Loss:1.6154699325561523\n",
      "训练次数：2225,Loss:1.7066699266433716\n",
      "训练次数：2226,Loss:1.4541399478912354\n",
      "训练次数：2227,Loss:1.4056059122085571\n",
      "训练次数：2228,Loss:1.5157387256622314\n",
      "训练次数：2229,Loss:1.8957830667495728\n",
      "训练次数：2230,Loss:1.79446542263031\n",
      "训练次数：2231,Loss:1.4884657859802246\n",
      "训练次数：2232,Loss:1.6687464714050293\n",
      "训练次数：2233,Loss:1.697670578956604\n",
      "训练次数：2234,Loss:1.6092065572738647\n",
      "训练次数：2235,Loss:1.709887146949768\n",
      "训练次数：2236,Loss:1.5108883380889893\n",
      "训练次数：2237,Loss:1.5191471576690674\n",
      "训练次数：2238,Loss:1.5666133165359497\n",
      "训练次数：2239,Loss:1.6923877000808716\n",
      "训练次数：2240,Loss:1.6429065465927124\n",
      "训练次数：2241,Loss:1.4945911169052124\n",
      "训练次数：2242,Loss:1.4677072763442993\n",
      "训练次数：2243,Loss:1.689650058746338\n",
      "训练次数：2244,Loss:1.7353980541229248\n",
      "训练次数：2245,Loss:1.727389931678772\n",
      "训练次数：2246,Loss:1.3479528427124023\n",
      "训练次数：2247,Loss:1.5074208974838257\n",
      "训练次数：2248,Loss:1.5638898611068726\n",
      "训练次数：2249,Loss:1.7548537254333496\n",
      "训练次数：2250,Loss:1.921433925628662\n",
      "训练次数：2251,Loss:1.6612699031829834\n",
      "训练次数：2252,Loss:1.5496007204055786\n",
      "训练次数：2253,Loss:1.6277672052383423\n",
      "训练次数：2254,Loss:1.6832623481750488\n",
      "训练次数：2255,Loss:1.6883463859558105\n",
      "训练次数：2256,Loss:1.7237006425857544\n",
      "训练次数：2257,Loss:1.7275283336639404\n",
      "训练次数：2258,Loss:1.695376992225647\n",
      "训练次数：2259,Loss:1.6602783203125\n",
      "训练次数：2260,Loss:1.609997034072876\n",
      "训练次数：2261,Loss:1.6065016984939575\n",
      "训练次数：2262,Loss:1.6878929138183594\n",
      "训练次数：2263,Loss:1.481797218322754\n",
      "训练次数：2264,Loss:1.7467570304870605\n",
      "训练次数：2265,Loss:1.6560955047607422\n",
      "训练次数：2266,Loss:1.7003151178359985\n",
      "训练次数：2267,Loss:1.5365720987319946\n",
      "训练次数：2268,Loss:1.3765332698822021\n",
      "训练次数：2269,Loss:1.5874786376953125\n",
      "训练次数：2270,Loss:1.6254076957702637\n",
      "训练次数：2271,Loss:1.7696741819381714\n",
      "训练次数：2272,Loss:1.6271411180496216\n",
      "训练次数：2273,Loss:1.4653788805007935\n",
      "训练次数：2274,Loss:1.4674692153930664\n",
      "训练次数：2275,Loss:1.7880030870437622\n",
      "训练次数：2276,Loss:1.5450890064239502\n",
      "训练次数：2277,Loss:1.5470279455184937\n",
      "训练次数：2278,Loss:1.5525215864181519\n",
      "训练次数：2279,Loss:1.549904227256775\n",
      "训练次数：2280,Loss:1.6986210346221924\n",
      "训练次数：2281,Loss:1.5300111770629883\n",
      "训练次数：2282,Loss:1.6108150482177734\n",
      "训练次数：2283,Loss:1.5891544818878174\n",
      "训练次数：2284,Loss:1.5274019241333008\n",
      "训练次数：2285,Loss:1.4445427656173706\n",
      "训练次数：2286,Loss:1.5913523435592651\n",
      "训练次数：2287,Loss:1.4636093378067017\n",
      "训练次数：2288,Loss:1.7300058603286743\n",
      "训练次数：2289,Loss:1.5560718774795532\n",
      "训练次数：2290,Loss:1.517371416091919\n",
      "训练次数：2291,Loss:1.750569462776184\n",
      "训练次数：2292,Loss:1.898571491241455\n",
      "训练次数：2293,Loss:1.5181797742843628\n",
      "训练次数：2294,Loss:1.7167621850967407\n",
      "训练次数：2295,Loss:1.46440589427948\n",
      "训练次数：2296,Loss:1.5471843481063843\n",
      "训练次数：2297,Loss:1.5900461673736572\n",
      "训练次数：2298,Loss:1.5698143243789673\n",
      "训练次数：2299,Loss:1.5552666187286377\n",
      "训练次数：2300,Loss:1.7969727516174316\n",
      "训练次数：2301,Loss:1.417209506034851\n",
      "训练次数：2302,Loss:1.7478933334350586\n",
      "训练次数：2303,Loss:1.415763020515442\n",
      "训练次数：2304,Loss:1.3943758010864258\n",
      "训练次数：2305,Loss:1.7115775346755981\n",
      "训练次数：2306,Loss:1.574678897857666\n",
      "训练次数：2307,Loss:1.7211910486221313\n",
      "训练次数：2308,Loss:1.5133787393569946\n",
      "训练次数：2309,Loss:1.4847089052200317\n",
      "训练次数：2310,Loss:1.5352931022644043\n",
      "训练次数：2311,Loss:1.359390139579773\n",
      "训练次数：2312,Loss:1.7601467370986938\n",
      "训练次数：2313,Loss:1.7544512748718262\n",
      "训练次数：2314,Loss:1.4594558477401733\n",
      "训练次数：2315,Loss:1.8984895944595337\n",
      "训练次数：2316,Loss:1.7043166160583496\n",
      "训练次数：2317,Loss:1.587791085243225\n",
      "训练次数：2318,Loss:1.6173598766326904\n",
      "训练次数：2319,Loss:1.5497673749923706\n",
      "训练次数：2320,Loss:1.6473591327667236\n",
      "训练次数：2321,Loss:1.7488340139389038\n",
      "训练次数：2322,Loss:1.6419473886489868\n",
      "训练次数：2323,Loss:1.718186855316162\n",
      "训练次数：2324,Loss:1.62763512134552\n",
      "训练次数：2325,Loss:1.5942972898483276\n",
      "训练次数：2326,Loss:1.7388983964920044\n",
      "训练次数：2327,Loss:1.6449025869369507\n",
      "训练次数：2328,Loss:1.5903019905090332\n",
      "训练次数：2329,Loss:1.4523591995239258\n",
      "训练次数：2330,Loss:1.7276811599731445\n",
      "训练次数：2331,Loss:1.5898977518081665\n",
      "训练次数：2332,Loss:1.6341078281402588\n",
      "训练次数：2333,Loss:1.4262311458587646\n",
      "训练次数：2334,Loss:1.4833675622940063\n",
      "训练次数：2335,Loss:1.6277750730514526\n",
      "训练次数：2336,Loss:1.5295217037200928\n",
      "训练次数：2337,Loss:1.5226576328277588\n",
      "训练次数：2338,Loss:1.608447790145874\n",
      "训练次数：2339,Loss:1.5799578428268433\n",
      "训练次数：2340,Loss:1.3782786130905151\n",
      "训练次数：2341,Loss:1.7731794118881226\n",
      "训练次数：2342,Loss:1.5912052392959595\n",
      "训练次数：2343,Loss:1.5751550197601318\n",
      "训练次数：2344,Loss:1.4116520881652832\n",
      "训练次数：2345,Loss:1.6155279874801636\n",
      "训练次数：2346,Loss:2.2861337661743164\n",
      "-----第 4 轮训练开始-----\n",
      "训练次数：2347,Loss:1.7306722402572632\n",
      "训练次数：2348,Loss:1.5578396320343018\n",
      "训练次数：2349,Loss:1.7035713195800781\n",
      "训练次数：2350,Loss:1.2803620100021362\n",
      "训练次数：2351,Loss:1.4180142879486084\n",
      "训练次数：2352,Loss:1.5266401767730713\n",
      "训练次数：2353,Loss:1.5970534086227417\n",
      "训练次数：2354,Loss:1.5590218305587769\n",
      "训练次数：2355,Loss:1.5422463417053223\n",
      "训练次数：2356,Loss:1.6917202472686768\n",
      "训练次数：2357,Loss:1.6828725337982178\n",
      "训练次数：2358,Loss:1.692922830581665\n",
      "训练次数：2359,Loss:1.6322970390319824\n",
      "训练次数：2360,Loss:1.7444684505462646\n",
      "训练次数：2361,Loss:1.6275882720947266\n",
      "训练次数：2362,Loss:1.3729684352874756\n",
      "训练次数：2363,Loss:1.5120028257369995\n",
      "训练次数：2364,Loss:1.4897552728652954\n",
      "训练次数：2365,Loss:1.6316179037094116\n",
      "训练次数：2366,Loss:1.4401428699493408\n",
      "训练次数：2367,Loss:1.6694049835205078\n",
      "训练次数：2368,Loss:1.534603476524353\n",
      "训练次数：2369,Loss:1.5170762538909912\n",
      "训练次数：2370,Loss:1.5610194206237793\n",
      "训练次数：2371,Loss:1.4735713005065918\n",
      "训练次数：2372,Loss:1.4842020273208618\n",
      "训练次数：2373,Loss:1.5570776462554932\n",
      "训练次数：2374,Loss:1.5418931245803833\n",
      "训练次数：2375,Loss:1.3992289304733276\n",
      "训练次数：2376,Loss:1.5365628004074097\n",
      "训练次数：2377,Loss:1.762062907218933\n",
      "训练次数：2378,Loss:1.7730457782745361\n",
      "训练次数：2379,Loss:1.676511287689209\n",
      "训练次数：2380,Loss:1.4173812866210938\n",
      "训练次数：2381,Loss:1.4917255640029907\n",
      "训练次数：2382,Loss:1.561882734298706\n",
      "训练次数：2383,Loss:1.5781311988830566\n",
      "训练次数：2384,Loss:1.5045276880264282\n",
      "训练次数：2385,Loss:1.531980037689209\n",
      "训练次数：2386,Loss:1.7959098815917969\n",
      "训练次数：2387,Loss:1.609127163887024\n",
      "训练次数：2388,Loss:1.5694925785064697\n",
      "训练次数：2389,Loss:1.5955883264541626\n",
      "训练次数：2390,Loss:1.7159860134124756\n",
      "训练次数：2391,Loss:1.5036813020706177\n",
      "训练次数：2392,Loss:1.4940071105957031\n",
      "训练次数：2393,Loss:1.5634620189666748\n",
      "训练次数：2394,Loss:1.5898782014846802\n",
      "训练次数：2395,Loss:1.6650598049163818\n",
      "训练次数：2396,Loss:1.6689234972000122\n",
      "训练次数：2397,Loss:1.5817514657974243\n",
      "训练次数：2398,Loss:1.6235687732696533\n",
      "训练次数：2399,Loss:1.6186610460281372\n",
      "训练次数：2400,Loss:1.7471097707748413\n",
      "训练次数：2401,Loss:1.4538296461105347\n",
      "训练次数：2402,Loss:1.4207695722579956\n",
      "训练次数：2403,Loss:1.693131923675537\n",
      "训练次数：2404,Loss:1.5051162242889404\n",
      "训练次数：2405,Loss:1.4223583936691284\n",
      "训练次数：2406,Loss:1.5065966844558716\n",
      "训练次数：2407,Loss:1.6172022819519043\n",
      "训练次数：2408,Loss:1.7151379585266113\n",
      "训练次数：2409,Loss:1.6502612829208374\n",
      "训练次数：2410,Loss:1.534273624420166\n",
      "训练次数：2411,Loss:1.6855140924453735\n",
      "训练次数：2412,Loss:1.53410804271698\n",
      "训练次数：2413,Loss:1.6524176597595215\n",
      "训练次数：2414,Loss:1.621410846710205\n",
      "训练次数：2415,Loss:1.4138274192810059\n",
      "训练次数：2416,Loss:1.4362773895263672\n",
      "训练次数：2417,Loss:1.7046339511871338\n",
      "训练次数：2418,Loss:1.4110273122787476\n",
      "训练次数：2419,Loss:1.5686993598937988\n",
      "训练次数：2420,Loss:1.487876296043396\n",
      "训练次数：2421,Loss:1.566224217414856\n",
      "训练次数：2422,Loss:1.5715301036834717\n",
      "训练次数：2423,Loss:1.597988486289978\n",
      "训练次数：2424,Loss:1.6702016592025757\n",
      "训练次数：2425,Loss:1.4648096561431885\n",
      "训练次数：2426,Loss:1.8364906311035156\n",
      "训练次数：2427,Loss:1.7178908586502075\n",
      "训练次数：2428,Loss:1.5520524978637695\n",
      "训练次数：2429,Loss:1.7327979803085327\n",
      "训练次数：2430,Loss:1.3888007402420044\n",
      "训练次数：2431,Loss:1.603898286819458\n",
      "训练次数：2432,Loss:1.568118691444397\n",
      "训练次数：2433,Loss:1.5218079090118408\n",
      "训练次数：2434,Loss:1.2866605520248413\n",
      "训练次数：2435,Loss:1.519081950187683\n",
      "训练次数：2436,Loss:1.5086041688919067\n",
      "训练次数：2437,Loss:1.6201571226119995\n",
      "训练次数：2438,Loss:1.538839340209961\n",
      "训练次数：2439,Loss:1.4451075792312622\n",
      "训练次数：2440,Loss:1.497159481048584\n",
      "训练次数：2441,Loss:1.6015667915344238\n",
      "训练次数：2442,Loss:1.7333250045776367\n",
      "训练次数：2443,Loss:1.3506242036819458\n",
      "训练次数：2444,Loss:1.6466021537780762\n",
      "训练次数：2445,Loss:1.3886642456054688\n",
      "训练次数：2446,Loss:1.4501110315322876\n",
      "训练次数：2447,Loss:1.5448665618896484\n",
      "训练次数：2448,Loss:1.3356488943099976\n",
      "训练次数：2449,Loss:1.6831821203231812\n",
      "训练次数：2450,Loss:1.8586400747299194\n",
      "训练次数：2451,Loss:2.0876355171203613\n",
      "训练次数：2452,Loss:1.5800225734710693\n",
      "训练次数：2453,Loss:1.5496028661727905\n",
      "训练次数：2454,Loss:1.727561354637146\n",
      "训练次数：2455,Loss:1.5929181575775146\n",
      "训练次数：2456,Loss:1.3765513896942139\n",
      "训练次数：2457,Loss:1.528140664100647\n",
      "训练次数：2458,Loss:1.5138996839523315\n",
      "训练次数：2459,Loss:1.5190333127975464\n",
      "训练次数：2460,Loss:1.3445208072662354\n",
      "训练次数：2461,Loss:1.6177688837051392\n",
      "训练次数：2462,Loss:1.6127368211746216\n",
      "训练次数：2463,Loss:1.5143554210662842\n",
      "训练次数：2464,Loss:1.4686212539672852\n",
      "训练次数：2465,Loss:1.78123939037323\n",
      "训练次数：2466,Loss:1.6679775714874268\n",
      "训练次数：2467,Loss:1.5949589014053345\n",
      "训练次数：2468,Loss:1.593115210533142\n",
      "训练次数：2469,Loss:1.5900564193725586\n",
      "训练次数：2470,Loss:1.512587547302246\n",
      "训练次数：2471,Loss:1.585986852645874\n",
      "训练次数：2472,Loss:1.5026386976242065\n",
      "训练次数：2473,Loss:1.4881411790847778\n",
      "训练次数：2474,Loss:1.4376657009124756\n",
      "训练次数：2475,Loss:1.5509403944015503\n",
      "训练次数：2476,Loss:1.3859997987747192\n",
      "训练次数：2477,Loss:1.6189565658569336\n",
      "训练次数：2478,Loss:1.6020816564559937\n",
      "训练次数：2479,Loss:1.7633544206619263\n",
      "训练次数：2480,Loss:1.2578256130218506\n",
      "训练次数：2481,Loss:1.4575350284576416\n",
      "训练次数：2482,Loss:1.554580807685852\n",
      "训练次数：2483,Loss:1.6935302019119263\n",
      "训练次数：2484,Loss:1.6941678524017334\n",
      "训练次数：2485,Loss:1.4913681745529175\n",
      "训练次数：2486,Loss:1.7903121709823608\n",
      "训练次数：2487,Loss:1.8739502429962158\n",
      "训练次数：2488,Loss:1.663893222808838\n",
      "训练次数：2489,Loss:1.49122154712677\n",
      "训练次数：2490,Loss:1.4719619750976562\n",
      "训练次数：2491,Loss:1.4824161529541016\n",
      "训练次数：2492,Loss:1.6694512367248535\n",
      "训练次数：2493,Loss:1.446488857269287\n",
      "训练次数：2494,Loss:1.7055050134658813\n",
      "训练次数：2495,Loss:1.3928462266921997\n",
      "训练次数：2496,Loss:1.4115787744522095\n",
      "训练次数：2497,Loss:1.455536127090454\n",
      "训练次数：2498,Loss:1.6492116451263428\n",
      "训练次数：2499,Loss:1.5670064687728882\n",
      "训练次数：2500,Loss:1.3545804023742676\n",
      "训练次数：2501,Loss:1.6014869213104248\n",
      "训练次数：2502,Loss:1.563222050666809\n",
      "训练次数：2503,Loss:1.5854690074920654\n",
      "训练次数：2504,Loss:1.420990228652954\n",
      "训练次数：2505,Loss:1.7251248359680176\n",
      "训练次数：2506,Loss:1.9183343648910522\n",
      "训练次数：2507,Loss:1.571216344833374\n",
      "训练次数：2508,Loss:1.5132979154586792\n",
      "训练次数：2509,Loss:1.6784639358520508\n",
      "训练次数：2510,Loss:1.7305160760879517\n",
      "训练次数：2511,Loss:1.5310677289962769\n",
      "训练次数：2512,Loss:1.8845210075378418\n",
      "训练次数：2513,Loss:1.5540878772735596\n",
      "训练次数：2514,Loss:1.5236742496490479\n",
      "训练次数：2515,Loss:1.444373607635498\n",
      "训练次数：2516,Loss:1.4292407035827637\n",
      "训练次数：2517,Loss:1.6848276853561401\n",
      "训练次数：2518,Loss:1.556388020515442\n",
      "训练次数：2519,Loss:1.521365761756897\n",
      "训练次数：2520,Loss:1.7686606645584106\n",
      "训练次数：2521,Loss:1.3528084754943848\n",
      "训练次数：2522,Loss:1.6201915740966797\n",
      "训练次数：2523,Loss:1.459714412689209\n",
      "训练次数：2524,Loss:1.667064905166626\n",
      "训练次数：2525,Loss:1.683974027633667\n",
      "训练次数：2526,Loss:1.636926531791687\n",
      "训练次数：2527,Loss:1.4544873237609863\n",
      "训练次数：2528,Loss:1.418696403503418\n",
      "训练次数：2529,Loss:1.4599943161010742\n",
      "训练次数：2530,Loss:1.3836326599121094\n",
      "训练次数：2531,Loss:1.7032434940338135\n",
      "训练次数：2532,Loss:1.7221813201904297\n",
      "训练次数：2533,Loss:1.4766818284988403\n",
      "训练次数：2534,Loss:1.543986439704895\n",
      "训练次数：2535,Loss:1.7534763813018799\n",
      "训练次数：2536,Loss:1.5409927368164062\n",
      "训练次数：2537,Loss:1.3791385889053345\n",
      "训练次数：2538,Loss:1.6316922903060913\n",
      "训练次数：2539,Loss:1.825659990310669\n",
      "训练次数：2540,Loss:1.4670718908309937\n",
      "训练次数：2541,Loss:1.6040605306625366\n",
      "训练次数：2542,Loss:1.7014015913009644\n",
      "训练次数：2543,Loss:1.5531278848648071\n",
      "训练次数：2544,Loss:1.5640559196472168\n",
      "训练次数：2545,Loss:1.5964516401290894\n",
      "训练次数：2546,Loss:1.5708332061767578\n",
      "训练次数：2547,Loss:1.37453293800354\n",
      "训练次数：2548,Loss:1.863021731376648\n",
      "训练次数：2549,Loss:1.6086163520812988\n",
      "训练次数：2550,Loss:1.5826600790023804\n",
      "训练次数：2551,Loss:1.525020956993103\n",
      "训练次数：2552,Loss:1.7842588424682617\n",
      "训练次数：2553,Loss:1.555248498916626\n",
      "训练次数：2554,Loss:1.6598823070526123\n",
      "训练次数：2555,Loss:1.6278661489486694\n",
      "训练次数：2556,Loss:1.7998318672180176\n",
      "训练次数：2557,Loss:1.3735687732696533\n",
      "训练次数：2558,Loss:1.6055145263671875\n",
      "训练次数：2559,Loss:1.448749303817749\n",
      "训练次数：2560,Loss:1.5299369096755981\n",
      "训练次数：2561,Loss:1.68052339553833\n",
      "训练次数：2562,Loss:1.6506898403167725\n",
      "训练次数：2563,Loss:1.567403793334961\n",
      "训练次数：2564,Loss:1.4714782238006592\n",
      "训练次数：2565,Loss:1.574345350265503\n",
      "训练次数：2566,Loss:1.5459007024765015\n",
      "训练次数：2567,Loss:1.5434446334838867\n",
      "训练次数：2568,Loss:1.5918937921524048\n",
      "训练次数：2569,Loss:1.5431702136993408\n",
      "训练次数：2570,Loss:1.7757607698440552\n",
      "训练次数：2571,Loss:1.7456120252609253\n",
      "训练次数：2572,Loss:1.582379698753357\n",
      "训练次数：2573,Loss:1.5622072219848633\n",
      "训练次数：2574,Loss:1.5032392740249634\n",
      "训练次数：2575,Loss:1.4999785423278809\n",
      "训练次数：2576,Loss:1.7790403366088867\n",
      "训练次数：2577,Loss:1.5104851722717285\n",
      "训练次数：2578,Loss:1.5269842147827148\n",
      "训练次数：2579,Loss:1.5809168815612793\n",
      "训练次数：2580,Loss:1.6489684581756592\n",
      "训练次数：2581,Loss:1.683053970336914\n",
      "训练次数：2582,Loss:1.8332481384277344\n",
      "训练次数：2583,Loss:1.8937065601348877\n",
      "训练次数：2584,Loss:1.595352053642273\n",
      "训练次数：2585,Loss:1.4158998727798462\n",
      "训练次数：2586,Loss:1.4739266633987427\n",
      "训练次数：2587,Loss:1.6208609342575073\n",
      "训练次数：2588,Loss:1.530518889427185\n",
      "训练次数：2589,Loss:1.5549888610839844\n",
      "训练次数：2590,Loss:1.8490238189697266\n",
      "训练次数：2591,Loss:1.4009679555892944\n",
      "训练次数：2592,Loss:1.6549286842346191\n",
      "训练次数：2593,Loss:1.5625531673431396\n",
      "训练次数：2594,Loss:1.6772857904434204\n",
      "训练次数：2595,Loss:1.7139768600463867\n",
      "训练次数：2596,Loss:1.7011245489120483\n",
      "训练次数：2597,Loss:1.5328425168991089\n",
      "训练次数：2598,Loss:1.5219998359680176\n",
      "训练次数：2599,Loss:1.5120598077774048\n",
      "训练次数：2600,Loss:1.525702953338623\n",
      "训练次数：2601,Loss:1.5519046783447266\n",
      "训练次数：2602,Loss:1.3868111371994019\n",
      "训练次数：2603,Loss:1.5375862121582031\n",
      "训练次数：2604,Loss:1.3589133024215698\n",
      "训练次数：2605,Loss:1.7159688472747803\n",
      "训练次数：2606,Loss:1.635609745979309\n",
      "训练次数：2607,Loss:1.45787513256073\n",
      "训练次数：2608,Loss:1.6140055656433105\n",
      "训练次数：2609,Loss:1.5178682804107666\n",
      "训练次数：2610,Loss:1.6007322072982788\n",
      "训练次数：2611,Loss:1.2469338178634644\n",
      "训练次数：2612,Loss:1.3525283336639404\n",
      "训练次数：2613,Loss:1.629640817642212\n",
      "训练次数：2614,Loss:1.5026311874389648\n",
      "训练次数：2615,Loss:1.5116435289382935\n",
      "训练次数：2616,Loss:1.6114360094070435\n",
      "训练次数：2617,Loss:1.61580491065979\n",
      "训练次数：2618,Loss:1.6146955490112305\n",
      "训练次数：2619,Loss:1.5177218914031982\n",
      "训练次数：2620,Loss:1.4247348308563232\n",
      "训练次数：2621,Loss:1.463313102722168\n",
      "训练次数：2622,Loss:1.5196641683578491\n",
      "训练次数：2623,Loss:1.8950355052947998\n",
      "训练次数：2624,Loss:1.6482685804367065\n",
      "训练次数：2625,Loss:1.6133302450180054\n",
      "训练次数：2626,Loss:1.3572570085525513\n",
      "训练次数：2627,Loss:1.4978317022323608\n",
      "训练次数：2628,Loss:1.580902338027954\n",
      "训练次数：2629,Loss:1.4568074941635132\n",
      "训练次数：2630,Loss:1.570656418800354\n",
      "训练次数：2631,Loss:1.4830127954483032\n",
      "训练次数：2632,Loss:1.5288047790527344\n",
      "训练次数：2633,Loss:1.6177892684936523\n",
      "训练次数：2634,Loss:1.4850201606750488\n",
      "训练次数：2635,Loss:1.7082821130752563\n",
      "训练次数：2636,Loss:1.5118812322616577\n",
      "训练次数：2637,Loss:1.5093624591827393\n",
      "训练次数：2638,Loss:1.5233882665634155\n",
      "训练次数：2639,Loss:1.49821138381958\n",
      "训练次数：2640,Loss:1.4387396574020386\n",
      "训练次数：2641,Loss:1.4998003244400024\n",
      "训练次数：2642,Loss:1.5128955841064453\n",
      "训练次数：2643,Loss:1.7696157693862915\n",
      "训练次数：2644,Loss:1.5283217430114746\n",
      "训练次数：2645,Loss:1.3865777254104614\n",
      "训练次数：2646,Loss:1.568540096282959\n",
      "训练次数：2647,Loss:1.5406197309494019\n",
      "训练次数：2648,Loss:1.6080152988433838\n",
      "训练次数：2649,Loss:1.5040717124938965\n",
      "训练次数：2650,Loss:1.3970690965652466\n",
      "训练次数：2651,Loss:1.4865784645080566\n",
      "训练次数：2652,Loss:1.8067361116409302\n",
      "训练次数：2653,Loss:1.4776140451431274\n",
      "训练次数：2654,Loss:1.649821162223816\n",
      "训练次数：2655,Loss:1.5650668144226074\n",
      "训练次数：2656,Loss:1.444491982460022\n",
      "训练次数：2657,Loss:1.4521781206130981\n",
      "训练次数：2658,Loss:1.502798318862915\n",
      "训练次数：2659,Loss:1.707218885421753\n",
      "训练次数：2660,Loss:1.5754153728485107\n",
      "训练次数：2661,Loss:1.466664433479309\n",
      "训练次数：2662,Loss:1.6972180604934692\n",
      "训练次数：2663,Loss:1.6614493131637573\n",
      "训练次数：2664,Loss:1.7252893447875977\n",
      "训练次数：2665,Loss:1.401158094406128\n",
      "训练次数：2666,Loss:1.4734688997268677\n",
      "训练次数：2667,Loss:1.5729340314865112\n",
      "训练次数：2668,Loss:1.7395228147506714\n",
      "训练次数：2669,Loss:1.5087041854858398\n",
      "训练次数：2670,Loss:1.7075588703155518\n",
      "训练次数：2671,Loss:1.4893156290054321\n",
      "训练次数：2672,Loss:1.4949883222579956\n",
      "训练次数：2673,Loss:1.466338872909546\n",
      "训练次数：2674,Loss:1.5418137311935425\n",
      "训练次数：2675,Loss:1.4826772212982178\n",
      "训练次数：2676,Loss:1.5279065370559692\n",
      "训练次数：2677,Loss:1.466968297958374\n",
      "训练次数：2678,Loss:1.560569405555725\n",
      "训练次数：2679,Loss:1.4689472913742065\n",
      "训练次数：2680,Loss:1.609323263168335\n",
      "训练次数：2681,Loss:1.707417368888855\n",
      "训练次数：2682,Loss:1.6179646253585815\n",
      "训练次数：2683,Loss:1.5538698434829712\n",
      "训练次数：2684,Loss:1.3782212734222412\n",
      "训练次数：2685,Loss:1.553743839263916\n",
      "训练次数：2686,Loss:1.4760537147521973\n",
      "训练次数：2687,Loss:1.760058045387268\n",
      "训练次数：2688,Loss:1.6210581064224243\n",
      "训练次数：2689,Loss:1.4015300273895264\n",
      "训练次数：2690,Loss:1.5366501808166504\n",
      "训练次数：2691,Loss:1.5689091682434082\n",
      "训练次数：2692,Loss:1.3266321420669556\n",
      "训练次数：2693,Loss:1.4504824876785278\n",
      "训练次数：2694,Loss:1.6267402172088623\n",
      "训练次数：2695,Loss:1.5643908977508545\n",
      "训练次数：2696,Loss:1.3769720792770386\n",
      "训练次数：2697,Loss:1.5511975288391113\n",
      "训练次数：2698,Loss:1.572015643119812\n",
      "训练次数：2699,Loss:1.4709601402282715\n",
      "训练次数：2700,Loss:1.714347243309021\n",
      "训练次数：2701,Loss:1.455119252204895\n",
      "训练次数：2702,Loss:1.4864810705184937\n",
      "训练次数：2703,Loss:1.5303372144699097\n",
      "训练次数：2704,Loss:1.5710830688476562\n",
      "训练次数：2705,Loss:1.397276520729065\n",
      "训练次数：2706,Loss:1.725130558013916\n",
      "训练次数：2707,Loss:1.5600621700286865\n",
      "训练次数：2708,Loss:1.3340449333190918\n",
      "训练次数：2709,Loss:1.706109642982483\n",
      "训练次数：2710,Loss:1.4674174785614014\n",
      "训练次数：2711,Loss:1.1600247621536255\n",
      "训练次数：2712,Loss:1.3652676343917847\n",
      "训练次数：2713,Loss:1.554947853088379\n",
      "训练次数：2714,Loss:1.6957284212112427\n",
      "训练次数：2715,Loss:1.4391303062438965\n",
      "训练次数：2716,Loss:1.4813621044158936\n",
      "训练次数：2717,Loss:1.2465900182724\n",
      "训练次数：2718,Loss:1.6425849199295044\n",
      "训练次数：2719,Loss:1.3862440586090088\n",
      "训练次数：2720,Loss:1.4674708843231201\n",
      "训练次数：2721,Loss:1.6724785566329956\n",
      "训练次数：2722,Loss:1.4396827220916748\n",
      "训练次数：2723,Loss:1.3924281597137451\n",
      "训练次数：2724,Loss:1.423343300819397\n",
      "训练次数：2725,Loss:1.4720126390457153\n",
      "训练次数：2726,Loss:1.5478732585906982\n",
      "训练次数：2727,Loss:1.6844873428344727\n",
      "训练次数：2728,Loss:1.4727659225463867\n",
      "训练次数：2729,Loss:1.4407148361206055\n",
      "训练次数：2730,Loss:1.4274243116378784\n",
      "训练次数：2731,Loss:1.4110839366912842\n",
      "训练次数：2732,Loss:1.5047438144683838\n",
      "训练次数：2733,Loss:1.4204633235931396\n",
      "训练次数：2734,Loss:1.7244595289230347\n",
      "训练次数：2735,Loss:1.57658851146698\n",
      "训练次数：2736,Loss:1.316224455833435\n",
      "训练次数：2737,Loss:1.5171862840652466\n",
      "训练次数：2738,Loss:1.288773536682129\n",
      "训练次数：2739,Loss:1.4789985418319702\n",
      "训练次数：2740,Loss:1.6966687440872192\n",
      "训练次数：2741,Loss:1.5688388347625732\n",
      "训练次数：2742,Loss:1.588546872138977\n",
      "训练次数：2743,Loss:1.5370852947235107\n",
      "训练次数：2744,Loss:1.6635257005691528\n",
      "训练次数：2745,Loss:1.3532238006591797\n",
      "训练次数：2746,Loss:1.4995423555374146\n",
      "训练次数：2747,Loss:1.5537045001983643\n",
      "训练次数：2748,Loss:1.6988379955291748\n",
      "训练次数：2749,Loss:1.5595084428787231\n",
      "训练次数：2750,Loss:1.3841055631637573\n",
      "训练次数：2751,Loss:1.6609337329864502\n",
      "训练次数：2752,Loss:1.4325932264328003\n",
      "训练次数：2753,Loss:1.4614675045013428\n",
      "训练次数：2754,Loss:1.2162444591522217\n",
      "训练次数：2755,Loss:1.4826055765151978\n",
      "训练次数：2756,Loss:1.333801031112671\n",
      "训练次数：2757,Loss:1.732454776763916\n",
      "训练次数：2758,Loss:1.51652991771698\n",
      "训练次数：2759,Loss:1.6450927257537842\n",
      "训练次数：2760,Loss:1.5652798414230347\n",
      "训练次数：2761,Loss:1.580576777458191\n",
      "训练次数：2762,Loss:1.4816417694091797\n",
      "训练次数：2763,Loss:1.8087092638015747\n",
      "训练次数：2764,Loss:1.391084909439087\n",
      "训练次数：2765,Loss:1.4879165887832642\n",
      "训练次数：2766,Loss:1.5869299173355103\n",
      "训练次数：2767,Loss:1.5341182947158813\n",
      "训练次数：2768,Loss:1.4620938301086426\n",
      "训练次数：2769,Loss:1.6828007698059082\n",
      "训练次数：2770,Loss:1.4795669317245483\n",
      "训练次数：2771,Loss:1.4759305715560913\n",
      "训练次数：2772,Loss:1.6565759181976318\n",
      "训练次数：2773,Loss:1.6100590229034424\n",
      "训练次数：2774,Loss:1.4378060102462769\n",
      "训练次数：2775,Loss:1.5648808479309082\n",
      "训练次数：2776,Loss:1.595196008682251\n",
      "训练次数：2777,Loss:1.4999113082885742\n",
      "训练次数：2778,Loss:1.4125736951828003\n",
      "训练次数：2779,Loss:1.6345895528793335\n",
      "训练次数：2780,Loss:1.4614174365997314\n",
      "训练次数：2781,Loss:1.566078543663025\n",
      "训练次数：2782,Loss:1.8393808603286743\n",
      "训练次数：2783,Loss:1.7345387935638428\n",
      "训练次数：2784,Loss:1.6230361461639404\n",
      "训练次数：2785,Loss:1.543756365776062\n",
      "训练次数：2786,Loss:1.5730375051498413\n",
      "训练次数：2787,Loss:1.648681879043579\n",
      "训练次数：2788,Loss:1.4830570220947266\n",
      "训练次数：2789,Loss:1.6145542860031128\n",
      "训练次数：2790,Loss:1.4597758054733276\n",
      "训练次数：2791,Loss:1.5692942142486572\n",
      "训练次数：2792,Loss:1.5968387126922607\n",
      "训练次数：2793,Loss:1.5338504314422607\n",
      "训练次数：2794,Loss:1.4995899200439453\n",
      "训练次数：2795,Loss:1.3905285596847534\n",
      "训练次数：2796,Loss:1.6310933828353882\n",
      "训练次数：2797,Loss:1.3161734342575073\n",
      "训练次数：2798,Loss:1.2424362897872925\n",
      "训练次数：2799,Loss:1.4000110626220703\n",
      "训练次数：2800,Loss:1.4772688150405884\n",
      "训练次数：2801,Loss:1.6461756229400635\n",
      "训练次数：2802,Loss:1.5425810813903809\n",
      "训练次数：2803,Loss:1.61192786693573\n",
      "训练次数：2804,Loss:1.343536138534546\n",
      "训练次数：2805,Loss:1.3561009168624878\n",
      "训练次数：2806,Loss:1.42253839969635\n",
      "训练次数：2807,Loss:1.627286434173584\n",
      "训练次数：2808,Loss:1.5183641910552979\n",
      "训练次数：2809,Loss:1.4401499032974243\n",
      "训练次数：2810,Loss:1.5503296852111816\n",
      "训练次数：2811,Loss:1.5564351081848145\n",
      "训练次数：2812,Loss:1.3535618782043457\n",
      "训练次数：2813,Loss:1.5933051109313965\n",
      "训练次数：2814,Loss:1.4519091844558716\n",
      "训练次数：2815,Loss:1.419641137123108\n",
      "训练次数：2816,Loss:1.5443912744522095\n",
      "训练次数：2817,Loss:1.5067323446273804\n",
      "训练次数：2818,Loss:1.649783730506897\n",
      "训练次数：2819,Loss:1.379480004310608\n",
      "训练次数：2820,Loss:1.6680338382720947\n",
      "训练次数：2821,Loss:1.2315179109573364\n",
      "训练次数：2822,Loss:1.6746915578842163\n",
      "训练次数：2823,Loss:1.474090337753296\n",
      "训练次数：2824,Loss:1.7045819759368896\n",
      "训练次数：2825,Loss:1.6815097332000732\n",
      "训练次数：2826,Loss:1.5422816276550293\n",
      "训练次数：2827,Loss:1.5935462713241577\n",
      "训练次数：2828,Loss:1.676314115524292\n",
      "训练次数：2829,Loss:1.5227822065353394\n",
      "训练次数：2830,Loss:1.3415796756744385\n",
      "训练次数：2831,Loss:1.418149709701538\n",
      "训练次数：2832,Loss:1.2650847434997559\n",
      "训练次数：2833,Loss:1.6312215328216553\n",
      "训练次数：2834,Loss:1.6588034629821777\n",
      "训练次数：2835,Loss:1.4526820182800293\n",
      "训练次数：2836,Loss:1.285573124885559\n",
      "训练次数：2837,Loss:1.7784477472305298\n",
      "训练次数：2838,Loss:1.5531628131866455\n",
      "训练次数：2839,Loss:1.5293140411376953\n",
      "训练次数：2840,Loss:1.3708181381225586\n",
      "训练次数：2841,Loss:1.5145342350006104\n",
      "训练次数：2842,Loss:1.6490241289138794\n",
      "训练次数：2843,Loss:1.4885743856430054\n",
      "训练次数：2844,Loss:1.5838521718978882\n",
      "训练次数：2845,Loss:1.7751165628433228\n",
      "训练次数：2846,Loss:1.5161575078964233\n",
      "训练次数：2847,Loss:1.6172080039978027\n",
      "训练次数：2848,Loss:1.5526129007339478\n",
      "训练次数：2849,Loss:1.4368531703948975\n",
      "训练次数：2850,Loss:1.6822365522384644\n",
      "训练次数：2851,Loss:1.6345628499984741\n",
      "训练次数：2852,Loss:1.6199965476989746\n",
      "训练次数：2853,Loss:1.4385383129119873\n",
      "训练次数：2854,Loss:1.4793182611465454\n",
      "训练次数：2855,Loss:1.5777219533920288\n",
      "训练次数：2856,Loss:1.4618796110153198\n",
      "训练次数：2857,Loss:1.6084129810333252\n",
      "训练次数：2858,Loss:1.4589474201202393\n",
      "训练次数：2859,Loss:1.6437896490097046\n",
      "训练次数：2860,Loss:1.6017318964004517\n",
      "训练次数：2861,Loss:1.7895419597625732\n",
      "训练次数：2862,Loss:1.4810676574707031\n",
      "训练次数：2863,Loss:1.7672606706619263\n",
      "训练次数：2864,Loss:1.3054224252700806\n",
      "训练次数：2865,Loss:1.5079461336135864\n",
      "训练次数：2866,Loss:1.4746659994125366\n",
      "训练次数：2867,Loss:1.646176815032959\n",
      "训练次数：2868,Loss:1.6276291608810425\n",
      "训练次数：2869,Loss:1.5259093046188354\n",
      "训练次数：2870,Loss:1.6471635103225708\n",
      "训练次数：2871,Loss:1.675187349319458\n",
      "训练次数：2872,Loss:1.7501096725463867\n",
      "训练次数：2873,Loss:1.5552208423614502\n",
      "训练次数：2874,Loss:1.4302691221237183\n",
      "训练次数：2875,Loss:1.564599871635437\n",
      "训练次数：2876,Loss:1.7830740213394165\n",
      "训练次数：2877,Loss:1.6428401470184326\n",
      "训练次数：2878,Loss:1.6690343618392944\n",
      "训练次数：2879,Loss:1.500739574432373\n",
      "训练次数：2880,Loss:1.351058006286621\n",
      "训练次数：2881,Loss:1.4756957292556763\n",
      "训练次数：2882,Loss:1.4022984504699707\n",
      "训练次数：2883,Loss:1.6128414869308472\n",
      "训练次数：2884,Loss:1.5566444396972656\n",
      "训练次数：2885,Loss:1.5075491666793823\n",
      "训练次数：2886,Loss:1.5658950805664062\n",
      "训练次数：2887,Loss:1.6088794469833374\n",
      "训练次数：2888,Loss:1.5727063417434692\n",
      "训练次数：2889,Loss:1.507138967514038\n",
      "训练次数：2890,Loss:1.4712313413619995\n",
      "训练次数：2891,Loss:1.4660720825195312\n",
      "训练次数：2892,Loss:1.4654823541641235\n",
      "训练次数：2893,Loss:1.5878163576126099\n",
      "训练次数：2894,Loss:1.5253971815109253\n",
      "训练次数：2895,Loss:1.5431113243103027\n",
      "训练次数：2896,Loss:1.5356311798095703\n",
      "训练次数：2897,Loss:1.4274792671203613\n",
      "训练次数：2898,Loss:1.7382879257202148\n",
      "训练次数：2899,Loss:1.4063904285430908\n",
      "训练次数：2900,Loss:1.6046618223190308\n",
      "训练次数：2901,Loss:1.4015482664108276\n",
      "训练次数：2902,Loss:1.3830310106277466\n",
      "训练次数：2903,Loss:1.6702368259429932\n",
      "训练次数：2904,Loss:1.5712467432022095\n",
      "训练次数：2905,Loss:1.4845341444015503\n",
      "训练次数：2906,Loss:1.5142943859100342\n",
      "训练次数：2907,Loss:1.4085397720336914\n",
      "训练次数：2908,Loss:1.5011554956436157\n",
      "训练次数：2909,Loss:1.6317174434661865\n",
      "训练次数：2910,Loss:1.5442479848861694\n",
      "训练次数：2911,Loss:1.486047625541687\n",
      "训练次数：2912,Loss:1.5930991172790527\n",
      "训练次数：2913,Loss:1.4094704389572144\n",
      "训练次数：2914,Loss:1.6386902332305908\n",
      "训练次数：2915,Loss:1.5470411777496338\n",
      "训练次数：2916,Loss:1.3444653749465942\n",
      "训练次数：2917,Loss:1.523032307624817\n",
      "训练次数：2918,Loss:1.3933430910110474\n",
      "训练次数：2919,Loss:1.3666439056396484\n",
      "训练次数：2920,Loss:1.5754541158676147\n",
      "训练次数：2921,Loss:1.3993281126022339\n",
      "训练次数：2922,Loss:1.4739105701446533\n",
      "训练次数：2923,Loss:1.7188475131988525\n",
      "训练次数：2924,Loss:1.8145592212677002\n",
      "训练次数：2925,Loss:1.3545316457748413\n",
      "训练次数：2926,Loss:1.5654603242874146\n",
      "训练次数：2927,Loss:1.5326751470565796\n",
      "训练次数：2928,Loss:1.5610519647598267\n",
      "训练次数：2929,Loss:1.35017991065979\n",
      "训练次数：2930,Loss:1.5481504201889038\n",
      "训练次数：2931,Loss:1.5447646379470825\n",
      "训练次数：2932,Loss:1.4073439836502075\n",
      "训练次数：2933,Loss:1.6516015529632568\n",
      "训练次数：2934,Loss:1.4448044300079346\n",
      "训练次数：2935,Loss:1.5918937921524048\n",
      "训练次数：2936,Loss:1.4756269454956055\n",
      "训练次数：2937,Loss:1.6050419807434082\n",
      "训练次数：2938,Loss:1.3751213550567627\n",
      "训练次数：2939,Loss:1.5867990255355835\n",
      "训练次数：2940,Loss:1.7162538766860962\n",
      "训练次数：2941,Loss:1.5109118223190308\n",
      "训练次数：2942,Loss:1.8563891649246216\n",
      "训练次数：2943,Loss:1.5552910566329956\n",
      "训练次数：2944,Loss:1.5601264238357544\n",
      "训练次数：2945,Loss:1.5876023769378662\n",
      "训练次数：2946,Loss:1.6207000017166138\n",
      "训练次数：2947,Loss:1.6206283569335938\n",
      "训练次数：2948,Loss:1.4085332155227661\n",
      "训练次数：2949,Loss:1.2915966510772705\n",
      "训练次数：2950,Loss:1.5910656452178955\n",
      "训练次数：2951,Loss:1.5404367446899414\n",
      "训练次数：2952,Loss:1.340052843093872\n",
      "训练次数：2953,Loss:1.2810149192810059\n",
      "训练次数：2954,Loss:1.5159739255905151\n",
      "训练次数：2955,Loss:1.4680808782577515\n",
      "训练次数：2956,Loss:1.6141232252120972\n",
      "训练次数：2957,Loss:1.4928929805755615\n",
      "训练次数：2958,Loss:1.6267104148864746\n",
      "训练次数：2959,Loss:1.59042489528656\n",
      "训练次数：2960,Loss:1.384023666381836\n",
      "训练次数：2961,Loss:1.527185320854187\n",
      "训练次数：2962,Loss:1.487095832824707\n",
      "训练次数：2963,Loss:1.4963021278381348\n",
      "训练次数：2964,Loss:1.4618014097213745\n",
      "训练次数：2965,Loss:1.5864968299865723\n",
      "训练次数：2966,Loss:1.6933653354644775\n",
      "训练次数：2967,Loss:1.5435774326324463\n",
      "训练次数：2968,Loss:1.6416200399398804\n",
      "训练次数：2969,Loss:1.4841665029525757\n",
      "训练次数：2970,Loss:1.4303839206695557\n",
      "训练次数：2971,Loss:1.510089635848999\n",
      "训练次数：2972,Loss:1.299986720085144\n",
      "训练次数：2973,Loss:1.4115662574768066\n",
      "训练次数：2974,Loss:1.749383807182312\n",
      "训练次数：2975,Loss:1.6838215589523315\n",
      "训练次数：2976,Loss:1.3827362060546875\n",
      "训练次数：2977,Loss:1.4575401544570923\n",
      "训练次数：2978,Loss:1.6684576272964478\n",
      "训练次数：2979,Loss:1.5118889808654785\n",
      "训练次数：2980,Loss:1.2624828815460205\n",
      "训练次数：2981,Loss:1.5605968236923218\n",
      "训练次数：2982,Loss:1.3694870471954346\n",
      "训练次数：2983,Loss:1.6600048542022705\n",
      "训练次数：2984,Loss:1.5827394723892212\n",
      "训练次数：2985,Loss:1.599782943725586\n",
      "训练次数：2986,Loss:1.7254940271377563\n",
      "训练次数：2987,Loss:1.5990335941314697\n",
      "训练次数：2988,Loss:1.618897557258606\n",
      "训练次数：2989,Loss:1.5892893075942993\n",
      "训练次数：2990,Loss:1.3972735404968262\n",
      "训练次数：2991,Loss:1.413053035736084\n",
      "训练次数：2992,Loss:1.7538940906524658\n",
      "训练次数：2993,Loss:1.457721471786499\n",
      "训练次数：2994,Loss:1.4116863012313843\n",
      "训练次数：2995,Loss:1.5395290851593018\n",
      "训练次数：2996,Loss:1.6952767372131348\n",
      "训练次数：2997,Loss:1.4082038402557373\n",
      "训练次数：2998,Loss:1.608339548110962\n",
      "训练次数：2999,Loss:1.3023630380630493\n",
      "训练次数：3000,Loss:1.3670810461044312\n",
      "训练次数：3001,Loss:1.5403367280960083\n",
      "训练次数：3002,Loss:1.492480754852295\n",
      "训练次数：3003,Loss:1.5676885843276978\n",
      "训练次数：3004,Loss:1.4423367977142334\n",
      "训练次数：3005,Loss:1.7105371952056885\n",
      "训练次数：3006,Loss:1.4952430725097656\n",
      "训练次数：3007,Loss:1.575205683708191\n",
      "训练次数：3008,Loss:1.3410760164260864\n",
      "训练次数：3009,Loss:1.2765451669692993\n",
      "训练次数：3010,Loss:1.418215036392212\n",
      "训练次数：3011,Loss:1.7807247638702393\n",
      "训练次数：3012,Loss:1.7291314601898193\n",
      "训练次数：3013,Loss:1.346842646598816\n",
      "训练次数：3014,Loss:1.6176995038986206\n",
      "训练次数：3015,Loss:1.57054603099823\n",
      "训练次数：3016,Loss:1.498598575592041\n",
      "训练次数：3017,Loss:1.5895426273345947\n",
      "训练次数：3018,Loss:1.3680545091629028\n",
      "训练次数：3019,Loss:1.43788743019104\n",
      "训练次数：3020,Loss:1.496340036392212\n",
      "训练次数：3021,Loss:1.608948826789856\n",
      "训练次数：3022,Loss:1.6442530155181885\n",
      "训练次数：3023,Loss:1.4583544731140137\n",
      "训练次数：3024,Loss:1.3809248208999634\n",
      "训练次数：3025,Loss:1.6268980503082275\n",
      "训练次数：3026,Loss:1.6436752080917358\n",
      "训练次数：3027,Loss:1.6615653038024902\n",
      "训练次数：3028,Loss:1.243992805480957\n",
      "训练次数：3029,Loss:1.3640745878219604\n",
      "训练次数：3030,Loss:1.4533910751342773\n",
      "训练次数：3031,Loss:1.6619774103164673\n",
      "训练次数：3032,Loss:1.8013917207717896\n",
      "训练次数：3033,Loss:1.5334128141403198\n",
      "训练次数：3034,Loss:1.4144556522369385\n",
      "训练次数：3035,Loss:1.5227030515670776\n",
      "训练次数：3036,Loss:1.557866096496582\n",
      "训练次数：3037,Loss:1.5459880828857422\n",
      "训练次数：3038,Loss:1.6487942934036255\n",
      "训练次数：3039,Loss:1.6700506210327148\n",
      "训练次数：3040,Loss:1.6120027303695679\n",
      "训练次数：3041,Loss:1.5555726289749146\n",
      "训练次数：3042,Loss:1.492392659187317\n",
      "训练次数：3043,Loss:1.532926321029663\n",
      "训练次数：3044,Loss:1.5690197944641113\n",
      "训练次数：3045,Loss:1.353878378868103\n",
      "训练次数：3046,Loss:1.6809239387512207\n",
      "训练次数：3047,Loss:1.5649957656860352\n",
      "训练次数：3048,Loss:1.6101953983306885\n",
      "训练次数：3049,Loss:1.43552827835083\n",
      "训练次数：3050,Loss:1.2650902271270752\n",
      "训练次数：3051,Loss:1.4935426712036133\n",
      "训练次数：3052,Loss:1.519954800605774\n",
      "训练次数：3053,Loss:1.6877983808517456\n",
      "训练次数：3054,Loss:1.5442758798599243\n",
      "训练次数：3055,Loss:1.3320531845092773\n",
      "训练次数：3056,Loss:1.3357839584350586\n",
      "训练次数：3057,Loss:1.6635693311691284\n",
      "训练次数：3058,Loss:1.3976337909698486\n",
      "训练次数：3059,Loss:1.4950942993164062\n",
      "训练次数：3060,Loss:1.46169114112854\n",
      "训练次数：3061,Loss:1.4496461153030396\n",
      "训练次数：3062,Loss:1.5494493246078491\n",
      "训练次数：3063,Loss:1.396661639213562\n",
      "训练次数：3064,Loss:1.4922878742218018\n",
      "训练次数：3065,Loss:1.493603229522705\n",
      "训练次数：3066,Loss:1.4172284603118896\n",
      "训练次数：3067,Loss:1.3921116590499878\n",
      "训练次数：3068,Loss:1.474695086479187\n",
      "训练次数：3069,Loss:1.332812786102295\n",
      "训练次数：3070,Loss:1.6780600547790527\n",
      "训练次数：3071,Loss:1.479392647743225\n",
      "训练次数：3072,Loss:1.4027880430221558\n",
      "训练次数：3073,Loss:1.6824473142623901\n",
      "训练次数：3074,Loss:1.8197258710861206\n",
      "训练次数：3075,Loss:1.4469540119171143\n",
      "训练次数：3076,Loss:1.5637083053588867\n",
      "训练次数：3077,Loss:1.3577910661697388\n",
      "训练次数：3078,Loss:1.4559993743896484\n",
      "训练次数：3079,Loss:1.4633809328079224\n",
      "训练次数：3080,Loss:1.4686022996902466\n",
      "训练次数：3081,Loss:1.4698492288589478\n",
      "训练次数：3082,Loss:1.6746268272399902\n",
      "训练次数：3083,Loss:1.3094531297683716\n",
      "训练次数：3084,Loss:1.5997488498687744\n",
      "训练次数：3085,Loss:1.3080224990844727\n",
      "训练次数：3086,Loss:1.2863349914550781\n",
      "训练次数：3087,Loss:1.6015009880065918\n",
      "训练次数：3088,Loss:1.540853500366211\n",
      "训练次数：3089,Loss:1.682450771331787\n",
      "训练次数：3090,Loss:1.4094090461730957\n",
      "训练次数：3091,Loss:1.3764622211456299\n",
      "训练次数：3092,Loss:1.4320064783096313\n",
      "训练次数：3093,Loss:1.3250893354415894\n",
      "训练次数：3094,Loss:1.6723145246505737\n",
      "训练次数：3095,Loss:1.6828869581222534\n",
      "训练次数：3096,Loss:1.3159048557281494\n",
      "训练次数：3097,Loss:1.8884270191192627\n",
      "训练次数：3098,Loss:1.5458810329437256\n",
      "训练次数：3099,Loss:1.4550966024398804\n",
      "训练次数：3100,Loss:1.494515299797058\n",
      "训练次数：3101,Loss:1.4681189060211182\n",
      "训练次数：3102,Loss:1.5400069952011108\n",
      "训练次数：3103,Loss:1.552375078201294\n",
      "训练次数：3104,Loss:1.5772851705551147\n",
      "训练次数：3105,Loss:1.667551040649414\n",
      "训练次数：3106,Loss:1.4850144386291504\n",
      "训练次数：3107,Loss:1.4562339782714844\n",
      "训练次数：3108,Loss:1.66515052318573\n",
      "训练次数：3109,Loss:1.4747909307479858\n",
      "训练次数：3110,Loss:1.5070439577102661\n",
      "训练次数：3111,Loss:1.3549444675445557\n",
      "训练次数：3112,Loss:1.6498334407806396\n",
      "训练次数：3113,Loss:1.4757205247879028\n",
      "训练次数：3114,Loss:1.552191972732544\n",
      "训练次数：3115,Loss:1.2783936262130737\n",
      "训练次数：3116,Loss:1.3847267627716064\n",
      "训练次数：3117,Loss:1.5191084146499634\n",
      "训练次数：3118,Loss:1.479891300201416\n",
      "训练次数：3119,Loss:1.4418948888778687\n",
      "训练次数：3120,Loss:1.4633275270462036\n",
      "训练次数：3121,Loss:1.4587769508361816\n",
      "训练次数：3122,Loss:1.2976975440979004\n",
      "训练次数：3123,Loss:1.6541682481765747\n",
      "训练次数：3124,Loss:1.5001170635223389\n",
      "训练次数：3125,Loss:1.4710301160812378\n",
      "训练次数：3126,Loss:1.3798854351043701\n",
      "训练次数：3127,Loss:1.54007089138031\n",
      "训练次数：3128,Loss:2.2693662643432617\n",
      "-----第 5 轮训练开始-----\n",
      "训练次数：3129,Loss:1.6932563781738281\n",
      "训练次数：3130,Loss:1.5000795125961304\n",
      "训练次数：3131,Loss:1.632685899734497\n",
      "训练次数：3132,Loss:1.16341233253479\n",
      "训练次数：3133,Loss:1.3640488386154175\n",
      "训练次数：3134,Loss:1.4686918258666992\n",
      "训练次数：3135,Loss:1.5389223098754883\n",
      "训练次数：3136,Loss:1.4280776977539062\n",
      "训练次数：3137,Loss:1.4367923736572266\n",
      "训练次数：3138,Loss:1.6572574377059937\n",
      "训练次数：3139,Loss:1.6162911653518677\n",
      "训练次数：3140,Loss:1.6087278127670288\n",
      "训练次数：3141,Loss:1.5577629804611206\n",
      "训练次数：3142,Loss:1.694451093673706\n",
      "训练次数：3143,Loss:1.5879902839660645\n",
      "训练次数：3144,Loss:1.272950291633606\n",
      "训练次数：3145,Loss:1.4462307691574097\n",
      "训练次数：3146,Loss:1.348357915878296\n",
      "训练次数：3147,Loss:1.5045180320739746\n",
      "训练次数：3148,Loss:1.3500983715057373\n",
      "训练次数：3149,Loss:1.5856190919876099\n",
      "训练次数：3150,Loss:1.400261402130127\n",
      "训练次数：3151,Loss:1.4503653049468994\n",
      "训练次数：3152,Loss:1.4869980812072754\n",
      "训练次数：3153,Loss:1.4108375310897827\n",
      "训练次数：3154,Loss:1.3743840456008911\n",
      "训练次数：3155,Loss:1.4433327913284302\n",
      "训练次数：3156,Loss:1.4139697551727295\n",
      "训练次数：3157,Loss:1.2990236282348633\n",
      "训练次数：3158,Loss:1.4049087762832642\n",
      "训练次数：3159,Loss:1.6580322980880737\n",
      "训练次数：3160,Loss:1.681040644645691\n",
      "训练次数：3161,Loss:1.5624136924743652\n",
      "训练次数：3162,Loss:1.3269716501235962\n",
      "训练次数：3163,Loss:1.425212025642395\n",
      "训练次数：3164,Loss:1.4089237451553345\n",
      "训练次数：3165,Loss:1.5029226541519165\n",
      "训练次数：3166,Loss:1.4643598794937134\n",
      "训练次数：3167,Loss:1.4404877424240112\n",
      "训练次数：3168,Loss:1.8379578590393066\n",
      "训练次数：3169,Loss:1.613546371459961\n",
      "训练次数：3170,Loss:1.3930717706680298\n",
      "训练次数：3171,Loss:1.5603934526443481\n",
      "训练次数：3172,Loss:1.6824404001235962\n",
      "训练次数：3173,Loss:1.4007238149642944\n",
      "训练次数：3174,Loss:1.3617254495620728\n",
      "训练次数：3175,Loss:1.5011357069015503\n",
      "训练次数：3176,Loss:1.5951876640319824\n",
      "训练次数：3177,Loss:1.5461689233779907\n",
      "训练次数：3178,Loss:1.603192687034607\n",
      "训练次数：3179,Loss:1.462539792060852\n",
      "训练次数：3180,Loss:1.598548412322998\n",
      "训练次数：3181,Loss:1.521751880645752\n",
      "训练次数：3182,Loss:1.6323952674865723\n",
      "训练次数：3183,Loss:1.3812273740768433\n",
      "训练次数：3184,Loss:1.3476600646972656\n",
      "训练次数：3185,Loss:1.604759693145752\n",
      "训练次数：3186,Loss:1.3995509147644043\n",
      "训练次数：3187,Loss:1.3480308055877686\n",
      "训练次数：3188,Loss:1.403390884399414\n",
      "训练次数：3189,Loss:1.521328330039978\n",
      "训练次数：3190,Loss:1.6062452793121338\n",
      "训练次数：3191,Loss:1.5749517679214478\n",
      "训练次数：3192,Loss:1.4602904319763184\n",
      "训练次数：3193,Loss:1.595432162284851\n",
      "训练次数：3194,Loss:1.4759691953659058\n",
      "训练次数：3195,Loss:1.5550211668014526\n",
      "训练次数：3196,Loss:1.5505887269973755\n",
      "训练次数：3197,Loss:1.2874908447265625\n",
      "训练次数：3198,Loss:1.3487673997879028\n",
      "训练次数：3199,Loss:1.6083884239196777\n",
      "训练次数：3200,Loss:1.3147993087768555\n",
      "训练次数：3201,Loss:1.5131429433822632\n",
      "训练次数：3202,Loss:1.3795576095581055\n",
      "训练次数：3203,Loss:1.4995098114013672\n",
      "训练次数：3204,Loss:1.53759765625\n",
      "训练次数：3205,Loss:1.543109655380249\n",
      "训练次数：3206,Loss:1.614230990409851\n",
      "训练次数：3207,Loss:1.394788146018982\n",
      "训练次数：3208,Loss:1.754127860069275\n",
      "训练次数：3209,Loss:1.6323504447937012\n",
      "训练次数：3210,Loss:1.3892945051193237\n",
      "训练次数：3211,Loss:1.576342225074768\n",
      "训练次数：3212,Loss:1.3439390659332275\n",
      "训练次数：3213,Loss:1.5444875955581665\n",
      "训练次数：3214,Loss:1.4862104654312134\n",
      "训练次数：3215,Loss:1.4145790338516235\n",
      "训练次数：3216,Loss:1.1601675748825073\n",
      "训练次数：3217,Loss:1.4760884046554565\n",
      "训练次数：3218,Loss:1.4448840618133545\n",
      "训练次数：3219,Loss:1.5503318309783936\n",
      "训练次数：3220,Loss:1.4987094402313232\n",
      "训练次数：3221,Loss:1.32284677028656\n",
      "训练次数：3222,Loss:1.4441279172897339\n",
      "训练次数：3223,Loss:1.456174612045288\n",
      "训练次数：3224,Loss:1.7196873426437378\n",
      "训练次数：3225,Loss:1.2897149324417114\n",
      "训练次数：3226,Loss:1.5818012952804565\n",
      "训练次数：3227,Loss:1.2701996564865112\n",
      "训练次数：3228,Loss:1.3228152990341187\n",
      "训练次数：3229,Loss:1.4536819458007812\n",
      "训练次数：3230,Loss:1.275105595588684\n",
      "训练次数：3231,Loss:1.5864604711532593\n",
      "训练次数：3232,Loss:1.6728999614715576\n",
      "训练次数：3233,Loss:1.8146365880966187\n",
      "训练次数：3234,Loss:1.4450328350067139\n",
      "训练次数：3235,Loss:1.4402494430541992\n",
      "训练次数：3236,Loss:1.6488045454025269\n",
      "训练次数：3237,Loss:1.4695959091186523\n",
      "训练次数：3238,Loss:1.307631492614746\n",
      "训练次数：3239,Loss:1.4409937858581543\n",
      "训练次数：3240,Loss:1.4268471002578735\n",
      "训练次数：3241,Loss:1.4086296558380127\n",
      "训练次数：3242,Loss:1.2723205089569092\n",
      "训练次数：3243,Loss:1.4833166599273682\n",
      "训练次数：3244,Loss:1.5686299800872803\n",
      "训练次数：3245,Loss:1.4291657209396362\n",
      "训练次数：3246,Loss:1.4090487957000732\n",
      "训练次数：3247,Loss:1.7235238552093506\n",
      "训练次数：3248,Loss:1.556592345237732\n",
      "训练次数：3249,Loss:1.5067437887191772\n",
      "训练次数：3250,Loss:1.482474684715271\n",
      "训练次数：3251,Loss:1.458828091621399\n",
      "训练次数：3252,Loss:1.4469887018203735\n",
      "训练次数：3253,Loss:1.5605554580688477\n",
      "训练次数：3254,Loss:1.4163895845413208\n",
      "训练次数：3255,Loss:1.4429491758346558\n",
      "训练次数：3256,Loss:1.3441753387451172\n",
      "训练次数：3257,Loss:1.4593627452850342\n",
      "训练次数：3258,Loss:1.3312931060791016\n",
      "训练次数：3259,Loss:1.4756141901016235\n",
      "训练次数：3260,Loss:1.4783337116241455\n",
      "训练次数：3261,Loss:1.627475380897522\n",
      "训练次数：3262,Loss:1.1482527256011963\n",
      "训练次数：3263,Loss:1.3894504308700562\n",
      "训练次数：3264,Loss:1.4472283124923706\n",
      "训练次数：3265,Loss:1.6033368110656738\n",
      "训练次数：3266,Loss:1.6084047555923462\n",
      "训练次数：3267,Loss:1.362254023551941\n",
      "训练次数：3268,Loss:1.6888601779937744\n",
      "训练次数：3269,Loss:1.7939332723617554\n",
      "训练次数：3270,Loss:1.6359933614730835\n",
      "训练次数：3271,Loss:1.3679566383361816\n",
      "训练次数：3272,Loss:1.3794411420822144\n",
      "训练次数：3273,Loss:1.364764928817749\n",
      "训练次数：3274,Loss:1.5280081033706665\n",
      "训练次数：3275,Loss:1.3010224103927612\n",
      "训练次数：3276,Loss:1.6063885688781738\n",
      "训练次数：3277,Loss:1.2962859869003296\n",
      "训练次数：3278,Loss:1.2993566989898682\n",
      "训练次数：3279,Loss:1.3992176055908203\n",
      "训练次数：3280,Loss:1.5618631839752197\n",
      "训练次数：3281,Loss:1.4529145956039429\n",
      "训练次数：3282,Loss:1.2515720129013062\n",
      "训练次数：3283,Loss:1.5340542793273926\n",
      "训练次数：3284,Loss:1.5135204792022705\n",
      "训练次数：3285,Loss:1.4564120769500732\n",
      "训练次数：3286,Loss:1.3254814147949219\n",
      "训练次数：3287,Loss:1.6428196430206299\n",
      "训练次数：3288,Loss:1.780276894569397\n",
      "训练次数：3289,Loss:1.4933152198791504\n",
      "训练次数：3290,Loss:1.4170012474060059\n",
      "训练次数：3291,Loss:1.619222640991211\n",
      "训练次数：3292,Loss:1.62919282913208\n",
      "训练次数：3293,Loss:1.479981541633606\n",
      "训练次数：3294,Loss:1.8381528854370117\n",
      "训练次数：3295,Loss:1.4323939085006714\n",
      "训练次数：3296,Loss:1.452210545539856\n",
      "训练次数：3297,Loss:1.3981415033340454\n",
      "训练次数：3298,Loss:1.3577029705047607\n",
      "训练次数：3299,Loss:1.6081337928771973\n",
      "训练次数：3300,Loss:1.4901583194732666\n",
      "训练次数：3301,Loss:1.469653844833374\n",
      "训练次数：3302,Loss:1.7598578929901123\n",
      "训练次数：3303,Loss:1.2916207313537598\n",
      "训练次数：3304,Loss:1.574939250946045\n",
      "训练次数：3305,Loss:1.410526156425476\n",
      "训练次数：3306,Loss:1.5530223846435547\n",
      "训练次数：3307,Loss:1.5986849069595337\n",
      "训练次数：3308,Loss:1.5565733909606934\n",
      "训练次数：3309,Loss:1.3411388397216797\n",
      "训练次数：3310,Loss:1.3351221084594727\n",
      "训练次数：3311,Loss:1.369775652885437\n",
      "训练次数：3312,Loss:1.3731595277786255\n",
      "训练次数：3313,Loss:1.6683701276779175\n",
      "训练次数：3314,Loss:1.5718944072723389\n",
      "训练次数：3315,Loss:1.4213464260101318\n",
      "训练次数：3316,Loss:1.4463725090026855\n",
      "训练次数：3317,Loss:1.595582127571106\n",
      "训练次数：3318,Loss:1.4936527013778687\n",
      "训练次数：3319,Loss:1.2874890565872192\n",
      "训练次数：3320,Loss:1.5433485507965088\n",
      "训练次数：3321,Loss:1.7731664180755615\n",
      "训练次数：3322,Loss:1.3906378746032715\n",
      "训练次数：3323,Loss:1.488281011581421\n",
      "训练次数：3324,Loss:1.5753371715545654\n",
      "训练次数：3325,Loss:1.453193187713623\n",
      "训练次数：3326,Loss:1.5199075937271118\n",
      "训练次数：3327,Loss:1.5447297096252441\n",
      "训练次数：3328,Loss:1.475391149520874\n",
      "训练次数：3329,Loss:1.256409764289856\n",
      "训练次数：3330,Loss:1.72919762134552\n",
      "训练次数：3331,Loss:1.504859447479248\n",
      "训练次数：3332,Loss:1.514116644859314\n",
      "训练次数：3333,Loss:1.480407476425171\n",
      "训练次数：3334,Loss:1.695669412612915\n",
      "训练次数：3335,Loss:1.4310904741287231\n",
      "训练次数：3336,Loss:1.5833297967910767\n",
      "训练次数：3337,Loss:1.497660756111145\n",
      "训练次数：3338,Loss:1.767280101776123\n",
      "训练次数：3339,Loss:1.2892223596572876\n",
      "训练次数：3340,Loss:1.4951720237731934\n",
      "训练次数：3341,Loss:1.3641879558563232\n",
      "训练次数：3342,Loss:1.4807108640670776\n",
      "训练次数：3343,Loss:1.6118924617767334\n",
      "训练次数：3344,Loss:1.5626893043518066\n",
      "训练次数：3345,Loss:1.465720772743225\n",
      "训练次数：3346,Loss:1.3752378225326538\n",
      "训练次数：3347,Loss:1.4969134330749512\n",
      "训练次数：3348,Loss:1.4433250427246094\n",
      "训练次数：3349,Loss:1.450295329093933\n",
      "训练次数：3350,Loss:1.5454391241073608\n",
      "训练次数：3351,Loss:1.48761785030365\n",
      "训练次数：3352,Loss:1.6443462371826172\n",
      "训练次数：3353,Loss:1.624128818511963\n",
      "训练次数：3354,Loss:1.4547046422958374\n",
      "训练次数：3355,Loss:1.4774430990219116\n",
      "训练次数：3356,Loss:1.3877993822097778\n",
      "训练次数：3357,Loss:1.4070544242858887\n",
      "训练次数：3358,Loss:1.6751881837844849\n",
      "训练次数：3359,Loss:1.3926210403442383\n",
      "训练次数：3360,Loss:1.509291648864746\n",
      "训练次数：3361,Loss:1.5117872953414917\n",
      "训练次数：3362,Loss:1.6101316213607788\n",
      "训练次数：3363,Loss:1.6177220344543457\n",
      "训练次数：3364,Loss:1.7101458311080933\n",
      "训练次数：3365,Loss:1.7726202011108398\n",
      "训练次数：3366,Loss:1.4897525310516357\n",
      "训练次数：3367,Loss:1.3398644924163818\n",
      "训练次数：3368,Loss:1.3834306001663208\n",
      "训练次数：3369,Loss:1.542492151260376\n",
      "训练次数：3370,Loss:1.4633677005767822\n",
      "训练次数：3371,Loss:1.4352933168411255\n",
      "训练次数：3372,Loss:1.7799135446548462\n",
      "训练次数：3373,Loss:1.3217207193374634\n",
      "训练次数：3374,Loss:1.6017340421676636\n",
      "训练次数：3375,Loss:1.4507631063461304\n",
      "训练次数：3376,Loss:1.5748666524887085\n",
      "训练次数：3377,Loss:1.6403926610946655\n",
      "训练次数：3378,Loss:1.6399238109588623\n",
      "训练次数：3379,Loss:1.4679192304611206\n",
      "训练次数：3380,Loss:1.4515351057052612\n",
      "训练次数：3381,Loss:1.433706521987915\n",
      "训练次数：3382,Loss:1.4458297491073608\n",
      "训练次数：3383,Loss:1.4607077836990356\n",
      "训练次数：3384,Loss:1.349877953529358\n",
      "训练次数：3385,Loss:1.4777872562408447\n",
      "训练次数：3386,Loss:1.2724510431289673\n",
      "训练次数：3387,Loss:1.6103452444076538\n",
      "训练次数：3388,Loss:1.5082405805587769\n",
      "训练次数：3389,Loss:1.354396939277649\n",
      "训练次数：3390,Loss:1.4990098476409912\n",
      "训练次数：3391,Loss:1.4301739931106567\n",
      "训练次数：3392,Loss:1.541172742843628\n",
      "训练次数：3393,Loss:1.1452078819274902\n",
      "训练次数：3394,Loss:1.2829272747039795\n",
      "训练次数：3395,Loss:1.5072306394577026\n",
      "训练次数：3396,Loss:1.461032748222351\n",
      "训练次数：3397,Loss:1.3906893730163574\n",
      "训练次数：3398,Loss:1.559755802154541\n",
      "训练次数：3399,Loss:1.5166397094726562\n",
      "训练次数：3400,Loss:1.4814646244049072\n",
      "训练次数：3401,Loss:1.4025565385818481\n",
      "训练次数：3402,Loss:1.3223552703857422\n",
      "训练次数：3403,Loss:1.360783576965332\n",
      "训练次数：3404,Loss:1.4130538702011108\n",
      "训练次数：3405,Loss:1.7832205295562744\n",
      "训练次数：3406,Loss:1.5366792678833008\n",
      "训练次数：3407,Loss:1.5680264234542847\n",
      "训练次数：3408,Loss:1.2680522203445435\n",
      "训练次数：3409,Loss:1.4004955291748047\n",
      "训练次数：3410,Loss:1.5096254348754883\n",
      "训练次数：3411,Loss:1.331933856010437\n",
      "训练次数：3412,Loss:1.537691593170166\n",
      "训练次数：3413,Loss:1.397023320198059\n",
      "训练次数：3414,Loss:1.4266350269317627\n",
      "训练次数：3415,Loss:1.5530744791030884\n",
      "训练次数：3416,Loss:1.4563651084899902\n",
      "训练次数：3417,Loss:1.651435136795044\n",
      "训练次数：3418,Loss:1.544012188911438\n",
      "训练次数：3419,Loss:1.4635218381881714\n",
      "训练次数：3420,Loss:1.4377371072769165\n",
      "训练次数：3421,Loss:1.4015451669692993\n",
      "训练次数：3422,Loss:1.3457638025283813\n",
      "训练次数：3423,Loss:1.3908112049102783\n",
      "训练次数：3424,Loss:1.4488047361373901\n",
      "训练次数：3425,Loss:1.6662920713424683\n",
      "训练次数：3426,Loss:1.4188084602355957\n",
      "训练次数：3427,Loss:1.3056937456130981\n",
      "训练次数：3428,Loss:1.5084353685379028\n",
      "训练次数：3429,Loss:1.4581760168075562\n",
      "训练次数：3430,Loss:1.4524829387664795\n",
      "训练次数：3431,Loss:1.419752836227417\n",
      "训练次数：3432,Loss:1.3467133045196533\n",
      "训练次数：3433,Loss:1.3749570846557617\n",
      "训练次数：3434,Loss:1.7323524951934814\n",
      "训练次数：3435,Loss:1.4186731576919556\n",
      "训练次数：3436,Loss:1.5758795738220215\n",
      "训练次数：3437,Loss:1.4948822259902954\n",
      "训练次数：3438,Loss:1.3779516220092773\n",
      "训练次数：3439,Loss:1.3869059085845947\n",
      "训练次数：3440,Loss:1.472608208656311\n",
      "训练次数：3441,Loss:1.587515950202942\n",
      "训练次数：3442,Loss:1.4901666641235352\n",
      "训练次数：3443,Loss:1.3972296714782715\n",
      "训练次数：3444,Loss:1.5688693523406982\n",
      "训练次数：3445,Loss:1.5908235311508179\n",
      "训练次数：3446,Loss:1.6021555662155151\n",
      "训练次数：3447,Loss:1.3453606367111206\n",
      "训练次数：3448,Loss:1.4057286977767944\n",
      "训练次数：3449,Loss:1.5151150226593018\n",
      "训练次数：3450,Loss:1.6248085498809814\n",
      "训练次数：3451,Loss:1.4458448886871338\n",
      "训练次数：3452,Loss:1.6314866542816162\n",
      "训练次数：3453,Loss:1.4315810203552246\n",
      "训练次数：3454,Loss:1.4888756275177002\n",
      "训练次数：3455,Loss:1.3484339714050293\n",
      "训练次数：3456,Loss:1.4413952827453613\n",
      "训练次数：3457,Loss:1.4258688688278198\n",
      "训练次数：3458,Loss:1.4484981298446655\n",
      "训练次数：3459,Loss:1.3812494277954102\n",
      "训练次数：3460,Loss:1.4684187173843384\n",
      "训练次数：3461,Loss:1.4012430906295776\n",
      "训练次数：3462,Loss:1.5077210664749146\n",
      "训练次数：3463,Loss:1.6461538076400757\n",
      "训练次数：3464,Loss:1.5352967977523804\n",
      "训练次数：3465,Loss:1.489111065864563\n",
      "训练次数：3466,Loss:1.2559007406234741\n",
      "训练次数：3467,Loss:1.4715672731399536\n",
      "训练次数：3468,Loss:1.4302618503570557\n",
      "训练次数：3469,Loss:1.7313345670700073\n",
      "训练次数：3470,Loss:1.520322322845459\n",
      "训练次数：3471,Loss:1.338748812675476\n",
      "训练次数：3472,Loss:1.4191468954086304\n",
      "训练次数：3473,Loss:1.4965541362762451\n",
      "训练次数：3474,Loss:1.2234153747558594\n",
      "训练次数：3475,Loss:1.3509266376495361\n",
      "训练次数：3476,Loss:1.5237302780151367\n",
      "训练次数：3477,Loss:1.4647897481918335\n",
      "训练次数：3478,Loss:1.301020860671997\n",
      "训练次数：3479,Loss:1.4739023447036743\n",
      "训练次数：3480,Loss:1.5179541110992432\n",
      "训练次数：3481,Loss:1.384007215499878\n",
      "训练次数：3482,Loss:1.613511085510254\n",
      "训练次数：3483,Loss:1.3956403732299805\n",
      "训练次数：3484,Loss:1.4144718647003174\n",
      "训练次数：3485,Loss:1.4531307220458984\n",
      "训练次数：3486,Loss:1.4977885484695435\n",
      "训练次数：3487,Loss:1.2938940525054932\n",
      "训练次数：3488,Loss:1.6328099966049194\n",
      "训练次数：3489,Loss:1.4924298524856567\n",
      "训练次数：3490,Loss:1.2352224588394165\n",
      "训练次数：3491,Loss:1.6057169437408447\n",
      "训练次数：3492,Loss:1.3692073822021484\n",
      "训练次数：3493,Loss:1.0938938856124878\n",
      "训练次数：3494,Loss:1.3051527738571167\n",
      "训练次数：3495,Loss:1.5047662258148193\n",
      "训练次数：3496,Loss:1.6118216514587402\n",
      "训练次数：3497,Loss:1.3592623472213745\n",
      "训练次数：3498,Loss:1.4146589040756226\n",
      "训练次数：3499,Loss:1.1451829671859741\n",
      "训练次数：3500,Loss:1.5874582529067993\n",
      "训练次数：3501,Loss:1.304274320602417\n",
      "训练次数：3502,Loss:1.3934056758880615\n",
      "训练次数：3503,Loss:1.5546036958694458\n",
      "训练次数：3504,Loss:1.3431313037872314\n",
      "训练次数：3505,Loss:1.3065814971923828\n",
      "训练次数：3506,Loss:1.37102210521698\n",
      "训练次数：3507,Loss:1.3697508573532104\n",
      "训练次数：3508,Loss:1.528973937034607\n",
      "训练次数：3509,Loss:1.6066442728042603\n",
      "训练次数：3510,Loss:1.447611689567566\n",
      "训练次数：3511,Loss:1.3982104063034058\n",
      "训练次数：3512,Loss:1.354602575302124\n",
      "训练次数：3513,Loss:1.3354511260986328\n",
      "训练次数：3514,Loss:1.4317595958709717\n",
      "训练次数：3515,Loss:1.3751990795135498\n",
      "训练次数：3516,Loss:1.6435858011245728\n",
      "训练次数：3517,Loss:1.4886208772659302\n",
      "训练次数：3518,Loss:1.2685937881469727\n",
      "训练次数：3519,Loss:1.4178944826126099\n",
      "训练次数：3520,Loss:1.2073249816894531\n",
      "训练次数：3521,Loss:1.4394400119781494\n",
      "训练次数：3522,Loss:1.6384356021881104\n",
      "训练次数：3523,Loss:1.5308680534362793\n",
      "训练次数：3524,Loss:1.4809017181396484\n",
      "训练次数：3525,Loss:1.4491091966629028\n",
      "训练次数：3526,Loss:1.6319996118545532\n",
      "训练次数：3527,Loss:1.2856764793395996\n",
      "训练次数：3528,Loss:1.4040418863296509\n",
      "训练次数：3529,Loss:1.4074572324752808\n",
      "训练次数：3530,Loss:1.6312668323516846\n",
      "训练次数：3531,Loss:1.4922517538070679\n",
      "训练次数：3532,Loss:1.2903861999511719\n",
      "训练次数：3533,Loss:1.5424240827560425\n",
      "训练次数：3534,Loss:1.337697148323059\n",
      "训练次数：3535,Loss:1.3816214799880981\n",
      "训练次数：3536,Loss:1.123809814453125\n",
      "训练次数：3537,Loss:1.4276738166809082\n",
      "训练次数：3538,Loss:1.2342798709869385\n",
      "训练次数：3539,Loss:1.6993412971496582\n",
      "训练次数：3540,Loss:1.3886226415634155\n",
      "训练次数：3541,Loss:1.545270562171936\n",
      "训练次数：3542,Loss:1.4642202854156494\n",
      "训练次数：3543,Loss:1.485960841178894\n",
      "训练次数：3544,Loss:1.4104349613189697\n",
      "训练次数：3545,Loss:1.713415265083313\n",
      "训练次数：3546,Loss:1.334124207496643\n",
      "训练次数：3547,Loss:1.4165540933609009\n",
      "训练次数：3548,Loss:1.5284931659698486\n",
      "训练次数：3549,Loss:1.4878408908843994\n",
      "训练次数：3550,Loss:1.3857487440109253\n",
      "训练次数：3551,Loss:1.6205072402954102\n",
      "训练次数：3552,Loss:1.3741289377212524\n",
      "训练次数：3553,Loss:1.4068825244903564\n",
      "训练次数：3554,Loss:1.577214241027832\n",
      "训练次数：3555,Loss:1.4495452642440796\n",
      "训练次数：3556,Loss:1.3370908498764038\n",
      "训练次数：3557,Loss:1.4613230228424072\n",
      "训练次数：3558,Loss:1.4571553468704224\n",
      "训练次数：3559,Loss:1.4527575969696045\n",
      "训练次数：3560,Loss:1.3274670839309692\n",
      "训练次数：3561,Loss:1.5415866374969482\n",
      "训练次数：3562,Loss:1.3801482915878296\n",
      "训练次数：3563,Loss:1.5025765895843506\n",
      "训练次数：3564,Loss:1.7811002731323242\n",
      "训练次数：3565,Loss:1.6234662532806396\n",
      "训练次数：3566,Loss:1.5176987648010254\n",
      "训练次数：3567,Loss:1.4207838773727417\n",
      "训练次数：3568,Loss:1.4833027124404907\n",
      "训练次数：3569,Loss:1.513810157775879\n",
      "训练次数：3570,Loss:1.3597853183746338\n",
      "训练次数：3571,Loss:1.5245835781097412\n",
      "训练次数：3572,Loss:1.3471404314041138\n",
      "训练次数：3573,Loss:1.5120530128479004\n",
      "训练次数：3574,Loss:1.4914164543151855\n",
      "训练次数：3575,Loss:1.463746190071106\n",
      "训练次数：3576,Loss:1.3987171649932861\n",
      "训练次数：3577,Loss:1.3920745849609375\n",
      "训练次数：3578,Loss:1.5212405920028687\n",
      "训练次数：3579,Loss:1.2502467632293701\n",
      "训练次数：3580,Loss:1.176528811454773\n",
      "训练次数：3581,Loss:1.2985411882400513\n",
      "训练次数：3582,Loss:1.392543911933899\n",
      "训练次数：3583,Loss:1.5408390760421753\n",
      "训练次数：3584,Loss:1.4921931028366089\n",
      "训练次数：3585,Loss:1.5734155178070068\n",
      "训练次数：3586,Loss:1.2598676681518555\n",
      "训练次数：3587,Loss:1.2524888515472412\n",
      "训练次数：3588,Loss:1.3304662704467773\n",
      "训练次数：3589,Loss:1.5115318298339844\n",
      "训练次数：3590,Loss:1.463840365409851\n",
      "训练次数：3591,Loss:1.354886531829834\n",
      "训练次数：3592,Loss:1.4941390752792358\n",
      "训练次数：3593,Loss:1.4458818435668945\n",
      "训练次数：3594,Loss:1.2424771785736084\n",
      "训练次数：3595,Loss:1.5523022413253784\n",
      "训练次数：3596,Loss:1.3950355052947998\n",
      "训练次数：3597,Loss:1.2969828844070435\n",
      "训练次数：3598,Loss:1.465211033821106\n",
      "训练次数：3599,Loss:1.4654481410980225\n",
      "训练次数：3600,Loss:1.5775865316390991\n",
      "训练次数：3601,Loss:1.3144882917404175\n",
      "训练次数：3602,Loss:1.5909115076065063\n",
      "训练次数：3603,Loss:1.160211205482483\n",
      "训练次数：3604,Loss:1.5766477584838867\n",
      "训练次数：3605,Loss:1.3821384906768799\n",
      "训练次数：3606,Loss:1.5395323038101196\n",
      "训练次数：3607,Loss:1.5499012470245361\n",
      "训练次数：3608,Loss:1.480057716369629\n",
      "训练次数：3609,Loss:1.5448346138000488\n",
      "训练次数：3610,Loss:1.6026796102523804\n",
      "训练次数：3611,Loss:1.4393525123596191\n",
      "训练次数：3612,Loss:1.2482802867889404\n",
      "训练次数：3613,Loss:1.3475710153579712\n",
      "训练次数：3614,Loss:1.1831333637237549\n",
      "训练次数：3615,Loss:1.5078829526901245\n",
      "训练次数：3616,Loss:1.5956382751464844\n",
      "训练次数：3617,Loss:1.355362892150879\n",
      "训练次数：3618,Loss:1.183409571647644\n",
      "训练次数：3619,Loss:1.7167158126831055\n",
      "训练次数：3620,Loss:1.485182762145996\n",
      "训练次数：3621,Loss:1.48909592628479\n",
      "训练次数：3622,Loss:1.2904738187789917\n",
      "训练次数：3623,Loss:1.4657371044158936\n",
      "训练次数：3624,Loss:1.608788013458252\n",
      "训练次数：3625,Loss:1.3982869386672974\n",
      "训练次数：3626,Loss:1.509251594543457\n",
      "训练次数：3627,Loss:1.684282660484314\n",
      "训练次数：3628,Loss:1.408434271812439\n",
      "训练次数：3629,Loss:1.5819497108459473\n",
      "训练次数：3630,Loss:1.4546902179718018\n",
      "训练次数：3631,Loss:1.3429057598114014\n",
      "训练次数：3632,Loss:1.5893067121505737\n",
      "训练次数：3633,Loss:1.59703528881073\n",
      "训练次数：3634,Loss:1.539473056793213\n",
      "训练次数：3635,Loss:1.3683431148529053\n",
      "训练次数：3636,Loss:1.4099116325378418\n",
      "训练次数：3637,Loss:1.491827368736267\n",
      "训练次数：3638,Loss:1.4405478239059448\n",
      "训练次数：3639,Loss:1.536133885383606\n",
      "训练次数：3640,Loss:1.3971004486083984\n",
      "训练次数：3641,Loss:1.5701911449432373\n",
      "训练次数：3642,Loss:1.5604761838912964\n",
      "训练次数：3643,Loss:1.6790879964828491\n",
      "训练次数：3644,Loss:1.407194972038269\n",
      "训练次数：3645,Loss:1.7075315713882446\n",
      "训练次数：3646,Loss:1.2718373537063599\n",
      "训练次数：3647,Loss:1.4243098497390747\n",
      "训练次数：3648,Loss:1.3988730907440186\n",
      "训练次数：3649,Loss:1.5954416990280151\n",
      "训练次数：3650,Loss:1.5614076852798462\n",
      "训练次数：3651,Loss:1.431012749671936\n",
      "训练次数：3652,Loss:1.5977592468261719\n",
      "训练次数：3653,Loss:1.5551174879074097\n",
      "训练次数：3654,Loss:1.7006111145019531\n",
      "训练次数：3655,Loss:1.4960230588912964\n",
      "训练次数：3656,Loss:1.3283809423446655\n",
      "训练次数：3657,Loss:1.504278540611267\n",
      "训练次数：3658,Loss:1.6780956983566284\n",
      "训练次数：3659,Loss:1.4923906326293945\n",
      "训练次数：3660,Loss:1.5821707248687744\n",
      "训练次数：3661,Loss:1.371314287185669\n",
      "训练次数：3662,Loss:1.2962394952774048\n",
      "训练次数：3663,Loss:1.3953337669372559\n",
      "训练次数：3664,Loss:1.3055548667907715\n",
      "训练次数：3665,Loss:1.519581913948059\n",
      "训练次数：3666,Loss:1.4880551099777222\n",
      "训练次数：3667,Loss:1.4170827865600586\n",
      "训练次数：3668,Loss:1.4686774015426636\n",
      "训练次数：3669,Loss:1.524103045463562\n",
      "训练次数：3670,Loss:1.5055277347564697\n",
      "训练次数：3671,Loss:1.4184781312942505\n",
      "训练次数：3672,Loss:1.3834812641143799\n",
      "训练次数：3673,Loss:1.3834257125854492\n",
      "训练次数：3674,Loss:1.3872779607772827\n",
      "训练次数：3675,Loss:1.5030298233032227\n",
      "训练次数：3676,Loss:1.4440666437149048\n",
      "训练次数：3677,Loss:1.4240896701812744\n",
      "训练次数：3678,Loss:1.4918158054351807\n",
      "训练次数：3679,Loss:1.381727695465088\n",
      "训练次数：3680,Loss:1.7165933847427368\n",
      "训练次数：3681,Loss:1.3741925954818726\n",
      "训练次数：3682,Loss:1.5536144971847534\n",
      "训练次数：3683,Loss:1.3119728565216064\n",
      "训练次数：3684,Loss:1.328407645225525\n",
      "训练次数：3685,Loss:1.5810741186141968\n",
      "训练次数：3686,Loss:1.475960373878479\n",
      "训练次数：3687,Loss:1.3700494766235352\n",
      "训练次数：3688,Loss:1.4975464344024658\n",
      "训练次数：3689,Loss:1.364055871963501\n",
      "训练次数：3690,Loss:1.42246413230896\n",
      "训练次数：3691,Loss:1.5432454347610474\n",
      "训练次数：3692,Loss:1.4250447750091553\n",
      "训练次数：3693,Loss:1.4167718887329102\n",
      "训练次数：3694,Loss:1.5442931652069092\n",
      "训练次数：3695,Loss:1.3454135656356812\n",
      "训练次数：3696,Loss:1.5446580648422241\n",
      "训练次数：3697,Loss:1.492661952972412\n",
      "训练次数：3698,Loss:1.2426592111587524\n",
      "训练次数：3699,Loss:1.4267244338989258\n",
      "训练次数：3700,Loss:1.3196446895599365\n",
      "训练次数：3701,Loss:1.294054388999939\n",
      "训练次数：3702,Loss:1.5267215967178345\n",
      "训练次数：3703,Loss:1.3373007774353027\n",
      "训练次数：3704,Loss:1.3905683755874634\n",
      "训练次数：3705,Loss:1.593788743019104\n",
      "训练次数：3706,Loss:1.8123290538787842\n",
      "训练次数：3707,Loss:1.2921870946884155\n",
      "训练次数：3708,Loss:1.4959895610809326\n",
      "训练次数：3709,Loss:1.4121350049972534\n",
      "训练次数：3710,Loss:1.489014983177185\n",
      "训练次数：3711,Loss:1.2925161123275757\n",
      "训练次数：3712,Loss:1.4711902141571045\n",
      "训练次数：3713,Loss:1.4534481763839722\n",
      "训练次数：3714,Loss:1.3560599088668823\n",
      "训练次数：3715,Loss:1.573447585105896\n",
      "训练次数：3716,Loss:1.3252935409545898\n",
      "训练次数：3717,Loss:1.5765478610992432\n",
      "训练次数：3718,Loss:1.40118408203125\n",
      "训练次数：3719,Loss:1.5118963718414307\n",
      "训练次数：3720,Loss:1.2722058296203613\n",
      "训练次数：3721,Loss:1.5207328796386719\n",
      "训练次数：3722,Loss:1.5798060894012451\n",
      "训练次数：3723,Loss:1.437543511390686\n",
      "训练次数：3724,Loss:1.7760143280029297\n",
      "训练次数：3725,Loss:1.5138767957687378\n",
      "训练次数：3726,Loss:1.4658193588256836\n",
      "训练次数：3727,Loss:1.5514181852340698\n",
      "训练次数：3728,Loss:1.5458564758300781\n",
      "训练次数：3729,Loss:1.5477830171585083\n",
      "训练次数：3730,Loss:1.306357502937317\n",
      "训练次数：3731,Loss:1.2178269624710083\n",
      "训练次数：3732,Loss:1.5832109451293945\n",
      "训练次数：3733,Loss:1.5095224380493164\n",
      "训练次数：3734,Loss:1.2213494777679443\n",
      "训练次数：3735,Loss:1.1518197059631348\n",
      "训练次数：3736,Loss:1.4119415283203125\n",
      "训练次数：3737,Loss:1.35556161403656\n",
      "训练次数：3738,Loss:1.5483036041259766\n",
      "训练次数：3739,Loss:1.3902997970581055\n",
      "训练次数：3740,Loss:1.5056055784225464\n",
      "训练次数：3741,Loss:1.506485939025879\n",
      "训练次数：3742,Loss:1.2755683660507202\n",
      "训练次数：3743,Loss:1.4039103984832764\n",
      "训练次数：3744,Loss:1.4137921333312988\n",
      "训练次数：3745,Loss:1.4227709770202637\n",
      "训练次数：3746,Loss:1.3756951093673706\n",
      "训练次数：3747,Loss:1.5217078924179077\n",
      "训练次数：3748,Loss:1.6227400302886963\n",
      "训练次数：3749,Loss:1.468316912651062\n",
      "训练次数：3750,Loss:1.5537148714065552\n",
      "训练次数：3751,Loss:1.3978214263916016\n",
      "训练次数：3752,Loss:1.329664707183838\n",
      "训练次数：3753,Loss:1.3954133987426758\n",
      "训练次数：3754,Loss:1.2240121364593506\n",
      "训练次数：3755,Loss:1.3219261169433594\n",
      "训练次数：3756,Loss:1.727262258529663\n",
      "训练次数：3757,Loss:1.5926445722579956\n",
      "训练次数：3758,Loss:1.2909021377563477\n",
      "训练次数：3759,Loss:1.370216727256775\n",
      "训练次数：3760,Loss:1.5927172899246216\n",
      "训练次数：3761,Loss:1.4544641971588135\n",
      "训练次数：3762,Loss:1.1765204668045044\n",
      "训练次数：3763,Loss:1.4713129997253418\n",
      "训练次数：3764,Loss:1.2756484746932983\n",
      "训练次数：3765,Loss:1.503362774848938\n",
      "训练次数：3766,Loss:1.491338849067688\n",
      "训练次数：3767,Loss:1.5071383714675903\n",
      "训练次数：3768,Loss:1.644235372543335\n",
      "训练次数：3769,Loss:1.5261363983154297\n",
      "训练次数：3770,Loss:1.4875847101211548\n",
      "训练次数：3771,Loss:1.5093505382537842\n",
      "训练次数：3772,Loss:1.3330278396606445\n",
      "训练次数：3773,Loss:1.3683675527572632\n",
      "训练次数：3774,Loss:1.6303863525390625\n",
      "训练次数：3775,Loss:1.4057561159133911\n",
      "训练次数：3776,Loss:1.3281409740447998\n",
      "训练次数：3777,Loss:1.4275065660476685\n",
      "训练次数：3778,Loss:1.6465702056884766\n",
      "训练次数：3779,Loss:1.3605848550796509\n",
      "训练次数：3780,Loss:1.490719199180603\n",
      "训练次数：3781,Loss:1.2253366708755493\n",
      "训练次数：3782,Loss:1.3092881441116333\n",
      "训练次数：3783,Loss:1.4903639554977417\n",
      "训练次数：3784,Loss:1.4087624549865723\n",
      "训练次数：3785,Loss:1.4572557210922241\n",
      "训练次数：3786,Loss:1.3785114288330078\n",
      "训练次数：3787,Loss:1.6752110719680786\n",
      "训练次数：3788,Loss:1.4127951860427856\n",
      "训练次数：3789,Loss:1.4424558877944946\n",
      "训练次数：3790,Loss:1.2455071210861206\n",
      "训练次数：3791,Loss:1.199506163597107\n",
      "训练次数：3792,Loss:1.3674582242965698\n",
      "训练次数：3793,Loss:1.6748325824737549\n",
      "训练次数：3794,Loss:1.6790217161178589\n",
      "训练次数：3795,Loss:1.2654677629470825\n",
      "训练次数：3796,Loss:1.5871866941452026\n",
      "训练次数：3797,Loss:1.466781497001648\n",
      "训练次数：3798,Loss:1.401229739189148\n",
      "训练次数：3799,Loss:1.4910627603530884\n",
      "训练次数：3800,Loss:1.288991093635559\n",
      "训练次数：3801,Loss:1.3713873624801636\n",
      "训练次数：3802,Loss:1.4257621765136719\n",
      "训练次数：3803,Loss:1.5038530826568604\n",
      "训练次数：3804,Loss:1.6398686170578003\n",
      "训练次数：3805,Loss:1.4608774185180664\n",
      "训练次数：3806,Loss:1.3195137977600098\n",
      "训练次数：3807,Loss:1.5227481126785278\n",
      "训练次数：3808,Loss:1.576088309288025\n",
      "训练次数：3809,Loss:1.5859744548797607\n",
      "训练次数：3810,Loss:1.1695293188095093\n",
      "训练次数：3811,Loss:1.2726655006408691\n",
      "训练次数：3812,Loss:1.3569142818450928\n",
      "训练次数：3813,Loss:1.5505675077438354\n",
      "训练次数：3814,Loss:1.7080012559890747\n",
      "训练次数：3815,Loss:1.436476469039917\n",
      "训练次数：3816,Loss:1.3167434930801392\n",
      "训练次数：3817,Loss:1.4228601455688477\n",
      "训练次数：3818,Loss:1.470853328704834\n",
      "训练次数：3819,Loss:1.4623898267745972\n",
      "训练次数：3820,Loss:1.5701539516448975\n",
      "训练次数：3821,Loss:1.632940649986267\n",
      "训练次数：3822,Loss:1.5793715715408325\n",
      "训练次数：3823,Loss:1.4823843240737915\n",
      "训练次数：3824,Loss:1.4180020093917847\n",
      "训练次数：3825,Loss:1.4598546028137207\n",
      "训练次数：3826,Loss:1.4766689538955688\n",
      "训练次数：3827,Loss:1.2678695917129517\n",
      "训练次数：3828,Loss:1.6335887908935547\n",
      "训练次数：3829,Loss:1.5048580169677734\n",
      "训练次数：3830,Loss:1.522768497467041\n",
      "训练次数：3831,Loss:1.3818049430847168\n",
      "训练次数：3832,Loss:1.1950446367263794\n",
      "训练次数：3833,Loss:1.4236074686050415\n",
      "训练次数：3834,Loss:1.4423037767410278\n",
      "训练次数：3835,Loss:1.6328829526901245\n",
      "训练次数：3836,Loss:1.4976998567581177\n",
      "训练次数：3837,Loss:1.2442529201507568\n",
      "训练次数：3838,Loss:1.2262111902236938\n",
      "训练次数：3839,Loss:1.5581696033477783\n",
      "训练次数：3840,Loss:1.254605770111084\n",
      "训练次数：3841,Loss:1.409676432609558\n",
      "训练次数：3842,Loss:1.396597146987915\n",
      "训练次数：3843,Loss:1.375732660293579\n",
      "训练次数：3844,Loss:1.417970895767212\n",
      "训练次数：3845,Loss:1.2969989776611328\n",
      "训练次数：3846,Loss:1.3811060190200806\n",
      "训练次数：3847,Loss:1.4174267053604126\n",
      "训练次数：3848,Loss:1.3628637790679932\n",
      "训练次数：3849,Loss:1.3498228788375854\n",
      "训练次数：3850,Loss:1.3918569087982178\n",
      "训练次数：3851,Loss:1.2533607482910156\n",
      "训练次数：3852,Loss:1.6328024864196777\n",
      "训练次数：3853,Loss:1.444026231765747\n",
      "训练次数：3854,Loss:1.3277719020843506\n",
      "训练次数：3855,Loss:1.5997223854064941\n",
      "训练次数：3856,Loss:1.710979700088501\n",
      "训练次数：3857,Loss:1.3734065294265747\n",
      "训练次数：3858,Loss:1.4582602977752686\n",
      "训练次数：3859,Loss:1.2560136318206787\n",
      "训练次数：3860,Loss:1.3599481582641602\n",
      "训练次数：3861,Loss:1.3637921810150146\n",
      "训练次数：3862,Loss:1.3696855306625366\n",
      "训练次数：3863,Loss:1.433565616607666\n",
      "训练次数：3864,Loss:1.5472838878631592\n",
      "训练次数：3865,Loss:1.2374671697616577\n",
      "训练次数：3866,Loss:1.4777716398239136\n",
      "训练次数：3867,Loss:1.2353535890579224\n",
      "训练次数：3868,Loss:1.2022778987884521\n",
      "训练次数：3869,Loss:1.5022063255310059\n",
      "训练次数：3870,Loss:1.499584674835205\n",
      "训练次数：3871,Loss:1.6738454103469849\n",
      "训练次数：3872,Loss:1.328538417816162\n",
      "训练次数：3873,Loss:1.2981886863708496\n",
      "训练次数：3874,Loss:1.3451882600784302\n",
      "训练次数：3875,Loss:1.294893741607666\n",
      "训练次数：3876,Loss:1.625671625137329\n",
      "训练次数：3877,Loss:1.5755894184112549\n",
      "训练次数：3878,Loss:1.235122561454773\n",
      "训练次数：3879,Loss:1.845134973526001\n",
      "训练次数：3880,Loss:1.4265168905258179\n",
      "训练次数：3881,Loss:1.3522670269012451\n",
      "训练次数：3882,Loss:1.4314491748809814\n",
      "训练次数：3883,Loss:1.4247121810913086\n",
      "训练次数：3884,Loss:1.4182270765304565\n",
      "训练次数：3885,Loss:1.3937424421310425\n",
      "训练次数：3886,Loss:1.4997193813323975\n",
      "训练次数：3887,Loss:1.6218396425247192\n",
      "训练次数：3888,Loss:1.35561203956604\n",
      "训练次数：3889,Loss:1.3440333604812622\n",
      "训练次数：3890,Loss:1.561471939086914\n",
      "训练次数：3891,Loss:1.3787815570831299\n",
      "训练次数：3892,Loss:1.4245712757110596\n",
      "训练次数：3893,Loss:1.2848529815673828\n",
      "训练次数：3894,Loss:1.553916096687317\n",
      "训练次数：3895,Loss:1.3682568073272705\n",
      "训练次数：3896,Loss:1.5118800401687622\n",
      "训练次数：3897,Loss:1.1634515523910522\n",
      "训练次数：3898,Loss:1.2927018404006958\n",
      "训练次数：3899,Loss:1.4434102773666382\n",
      "训练次数：3900,Loss:1.4443659782409668\n",
      "训练次数：3901,Loss:1.405774474143982\n",
      "训练次数：3902,Loss:1.342033863067627\n",
      "训练次数：3903,Loss:1.366469144821167\n",
      "训练次数：3904,Loss:1.2430716753005981\n",
      "训练次数：3905,Loss:1.5782825946807861\n",
      "训练次数：3906,Loss:1.422188639640808\n",
      "训练次数：3907,Loss:1.3820880651474\n",
      "训练次数：3908,Loss:1.3236380815505981\n",
      "训练次数：3909,Loss:1.4893450736999512\n",
      "训练次数：3910,Loss:2.1880416870117188\n",
      "-----第 6 轮训练开始-----\n",
      "训练次数：3911,Loss:1.6222665309906006\n",
      "训练次数：3912,Loss:1.4406694173812866\n",
      "训练次数：3913,Loss:1.5593206882476807\n",
      "训练次数：3914,Loss:1.090861439704895\n",
      "训练次数：3915,Loss:1.3384301662445068\n",
      "训练次数：3916,Loss:1.406507968902588\n",
      "训练次数：3917,Loss:1.4743138551712036\n",
      "训练次数：3918,Loss:1.343305230140686\n",
      "训练次数：3919,Loss:1.3419219255447388\n",
      "训练次数：3920,Loss:1.6083738803863525\n",
      "训练次数：3921,Loss:1.5551460981369019\n",
      "训练次数：3922,Loss:1.523415446281433\n",
      "训练次数：3923,Loss:1.5128370523452759\n",
      "训练次数：3924,Loss:1.6262952089309692\n",
      "训练次数：3925,Loss:1.5091489553451538\n",
      "训练次数：3926,Loss:1.1971763372421265\n",
      "训练次数：3927,Loss:1.384460210800171\n",
      "训练次数：3928,Loss:1.2564167976379395\n",
      "训练次数：3929,Loss:1.400804042816162\n",
      "训练次数：3930,Loss:1.2891855239868164\n",
      "训练次数：3931,Loss:1.5442585945129395\n",
      "训练次数：3932,Loss:1.3064039945602417\n",
      "训练次数：3933,Loss:1.4054405689239502\n",
      "训练次数：3934,Loss:1.4462509155273438\n",
      "训练次数：3935,Loss:1.3726294040679932\n",
      "训练次数：3936,Loss:1.2885961532592773\n",
      "训练次数：3937,Loss:1.3742125034332275\n",
      "训练次数：3938,Loss:1.3131376504898071\n",
      "训练次数：3939,Loss:1.2363346815109253\n",
      "训练次数：3940,Loss:1.298154592514038\n",
      "训练次数：3941,Loss:1.568390965461731\n",
      "训练次数：3942,Loss:1.590908408164978\n",
      "训练次数：3943,Loss:1.4020559787750244\n",
      "训练次数：3944,Loss:1.2439730167388916\n",
      "训练次数：3945,Loss:1.3736391067504883\n",
      "训练次数：3946,Loss:1.304932951927185\n",
      "训练次数：3947,Loss:1.4453976154327393\n",
      "训练次数：3948,Loss:1.4514061212539673\n",
      "训练次数：3949,Loss:1.3465782403945923\n",
      "训练次数：3950,Loss:1.7820266485214233\n",
      "训练次数：3951,Loss:1.5809032917022705\n",
      "训练次数：3952,Loss:1.223934531211853\n",
      "训练次数：3953,Loss:1.550172209739685\n",
      "训练次数：3954,Loss:1.625659465789795\n",
      "训练次数：3955,Loss:1.2836111783981323\n",
      "训练次数：3956,Loss:1.277840495109558\n",
      "训练次数：3957,Loss:1.4664855003356934\n",
      "训练次数：3958,Loss:1.576369047164917\n",
      "训练次数：3959,Loss:1.4386898279190063\n",
      "训练次数：3960,Loss:1.567212462425232\n",
      "训练次数：3961,Loss:1.3726884126663208\n",
      "训练次数：3962,Loss:1.5475130081176758\n",
      "训练次数：3963,Loss:1.4384572505950928\n",
      "训练次数：3964,Loss:1.5279172658920288\n",
      "训练次数：3965,Loss:1.3208675384521484\n",
      "训练次数：3966,Loss:1.2979848384857178\n",
      "训练次数：3967,Loss:1.506937861442566\n",
      "训练次数：3968,Loss:1.3068196773529053\n",
      "训练次数：3969,Loss:1.2650878429412842\n",
      "训练次数：3970,Loss:1.3135886192321777\n",
      "训练次数：3971,Loss:1.4326304197311401\n",
      "训练次数：3972,Loss:1.5381836891174316\n",
      "训练次数：3973,Loss:1.4869872331619263\n",
      "训练次数：3974,Loss:1.4180819988250732\n",
      "训练次数：3975,Loss:1.5292657613754272\n",
      "训练次数：3976,Loss:1.4372882843017578\n",
      "训练次数：3977,Loss:1.443030595779419\n",
      "训练次数：3978,Loss:1.4882222414016724\n",
      "训练次数：3979,Loss:1.1924329996109009\n",
      "训练次数：3980,Loss:1.296186089515686\n",
      "训练次数：3981,Loss:1.559939980506897\n",
      "训练次数：3982,Loss:1.2205290794372559\n",
      "训练次数：3983,Loss:1.4292610883712769\n",
      "训练次数：3984,Loss:1.3162719011306763\n",
      "训练次数：3985,Loss:1.4578876495361328\n",
      "训练次数：3986,Loss:1.4977686405181885\n",
      "训练次数：3987,Loss:1.492170810699463\n",
      "训练次数：3988,Loss:1.5565671920776367\n",
      "训练次数：3989,Loss:1.303783655166626\n",
      "训练次数：3990,Loss:1.6884652376174927\n",
      "训练次数：3991,Loss:1.5673936605453491\n",
      "训练次数：3992,Loss:1.2923847436904907\n",
      "训练次数：3993,Loss:1.4863494634628296\n",
      "训练次数：3994,Loss:1.3091126680374146\n",
      "训练次数：3995,Loss:1.4924829006195068\n",
      "训练次数：3996,Loss:1.3999590873718262\n",
      "训练次数：3997,Loss:1.357230305671692\n",
      "训练次数：3998,Loss:1.0796539783477783\n",
      "训练次数：3999,Loss:1.4228986501693726\n",
      "训练次数：4000,Loss:1.3864867687225342\n",
      "训练次数：4001,Loss:1.4674220085144043\n",
      "训练次数：4002,Loss:1.4611852169036865\n",
      "训练次数：4003,Loss:1.2187751531600952\n",
      "训练次数：4004,Loss:1.3738034963607788\n",
      "训练次数：4005,Loss:1.3621689081192017\n",
      "训练次数：4006,Loss:1.6384172439575195\n",
      "训练次数：4007,Loss:1.2442529201507568\n",
      "训练次数：4008,Loss:1.533311367034912\n",
      "训练次数：4009,Loss:1.2041845321655273\n",
      "训练次数：4010,Loss:1.2293341159820557\n",
      "训练次数：4011,Loss:1.396011233329773\n",
      "训练次数：4012,Loss:1.2380061149597168\n",
      "训练次数：4013,Loss:1.4812262058258057\n",
      "训练次数：4014,Loss:1.5427926778793335\n",
      "训练次数：4015,Loss:1.6457360982894897\n",
      "训练次数：4016,Loss:1.3379067182540894\n",
      "训练次数：4017,Loss:1.375261664390564\n",
      "训练次数：4018,Loss:1.5803978443145752\n",
      "训练次数：4019,Loss:1.3543599843978882\n",
      "训练次数：4020,Loss:1.2313354015350342\n",
      "训练次数：4021,Loss:1.38340425491333\n",
      "训练次数：4022,Loss:1.3543301820755005\n",
      "训练次数：4023,Loss:1.3444349765777588\n",
      "训练次数：4024,Loss:1.1964750289916992\n",
      "训练次数：4025,Loss:1.3678369522094727\n",
      "训练次数：4026,Loss:1.53567636013031\n",
      "训练次数：4027,Loss:1.3507047891616821\n",
      "训练次数：4028,Loss:1.352155327796936\n",
      "训练次数：4029,Loss:1.6516125202178955\n",
      "训练次数：4030,Loss:1.4706072807312012\n",
      "训练次数：4031,Loss:1.4537043571472168\n",
      "训练次数：4032,Loss:1.3998534679412842\n",
      "训练次数：4033,Loss:1.3419079780578613\n",
      "训练次数：4034,Loss:1.4140347242355347\n",
      "训练次数：4035,Loss:1.5273982286453247\n",
      "训练次数：4036,Loss:1.3492368459701538\n",
      "训练次数：4037,Loss:1.407351016998291\n",
      "训练次数：4038,Loss:1.2635778188705444\n",
      "训练次数：4039,Loss:1.4125410318374634\n",
      "训练次数：4040,Loss:1.2776418924331665\n",
      "训练次数：4041,Loss:1.3544225692749023\n",
      "训练次数：4042,Loss:1.3279346227645874\n",
      "训练次数：4043,Loss:1.5546176433563232\n",
      "训练次数：4044,Loss:1.0752698183059692\n",
      "训练次数：4045,Loss:1.3235291242599487\n",
      "训练次数：4046,Loss:1.3597899675369263\n",
      "训练次数：4047,Loss:1.5371789932250977\n",
      "训练次数：4048,Loss:1.5328104496002197\n",
      "训练次数：4049,Loss:1.2428711652755737\n",
      "训练次数：4050,Loss:1.577372670173645\n",
      "训练次数：4051,Loss:1.7304785251617432\n",
      "训练次数：4052,Loss:1.5603643655776978\n",
      "训练次数：4053,Loss:1.3104594945907593\n",
      "训练次数：4054,Loss:1.31155264377594\n",
      "训练次数：4055,Loss:1.2680768966674805\n",
      "训练次数：4056,Loss:1.4329386949539185\n",
      "训练次数：4057,Loss:1.2330714464187622\n",
      "训练次数：4058,Loss:1.5179505348205566\n",
      "训练次数：4059,Loss:1.2264504432678223\n",
      "训练次数：4060,Loss:1.2227287292480469\n",
      "训练次数：4061,Loss:1.3526030778884888\n",
      "训练次数：4062,Loss:1.4699369668960571\n",
      "训练次数：4063,Loss:1.3941359519958496\n",
      "训练次数：4064,Loss:1.1775366067886353\n",
      "训练次数：4065,Loss:1.4614619016647339\n",
      "训练次数：4066,Loss:1.4505552053451538\n",
      "训练次数：4067,Loss:1.3713552951812744\n",
      "训练次数：4068,Loss:1.23524010181427\n",
      "训练次数：4069,Loss:1.5489457845687866\n",
      "训练次数：4070,Loss:1.6516928672790527\n",
      "训练次数：4071,Loss:1.428841471672058\n",
      "训练次数：4072,Loss:1.320139765739441\n",
      "训练次数：4073,Loss:1.5899653434753418\n",
      "训练次数：4074,Loss:1.5626739263534546\n",
      "训练次数：4075,Loss:1.4509737491607666\n",
      "训练次数：4076,Loss:1.785614252090454\n",
      "训练次数：4077,Loss:1.3548022508621216\n",
      "训练次数：4078,Loss:1.351792573928833\n",
      "训练次数：4079,Loss:1.3308371305465698\n",
      "训练次数：4080,Loss:1.3193473815917969\n",
      "训练次数：4081,Loss:1.540864109992981\n",
      "训练次数：4082,Loss:1.4291481971740723\n",
      "训练次数：4083,Loss:1.414491057395935\n",
      "训练次数：4084,Loss:1.7371764183044434\n",
      "训练次数：4085,Loss:1.2397968769073486\n",
      "训练次数：4086,Loss:1.4980547428131104\n",
      "训练次数：4087,Loss:1.3218029737472534\n",
      "训练次数：4088,Loss:1.4872915744781494\n",
      "训练次数：4089,Loss:1.4786368608474731\n",
      "训练次数：4090,Loss:1.501534342765808\n",
      "训练次数：4091,Loss:1.2629693746566772\n",
      "训练次数：4092,Loss:1.2706029415130615\n",
      "训练次数：4093,Loss:1.3010717630386353\n",
      "训练次数：4094,Loss:1.3506150245666504\n",
      "训练次数：4095,Loss:1.6226608753204346\n",
      "训练次数：4096,Loss:1.4686049222946167\n",
      "训练次数：4097,Loss:1.3787568807601929\n",
      "训练次数：4098,Loss:1.3564907312393188\n",
      "训练次数：4099,Loss:1.4687280654907227\n",
      "训练次数：4100,Loss:1.4581890106201172\n",
      "训练次数：4101,Loss:1.2111262083053589\n",
      "训练次数：4102,Loss:1.4768825769424438\n",
      "训练次数：4103,Loss:1.7242000102996826\n",
      "训练次数：4104,Loss:1.3162405490875244\n",
      "训练次数：4105,Loss:1.3784269094467163\n",
      "训练次数：4106,Loss:1.48395836353302\n",
      "训练次数：4107,Loss:1.3933830261230469\n",
      "训练次数：4108,Loss:1.486457109451294\n",
      "训练次数：4109,Loss:1.5054748058319092\n",
      "训练次数：4110,Loss:1.3875075578689575\n",
      "训练次数：4111,Loss:1.1724475622177124\n",
      "训练次数：4112,Loss:1.6196714639663696\n",
      "训练次数：4113,Loss:1.4073786735534668\n",
      "训练次数：4114,Loss:1.4429436922073364\n",
      "训练次数：4115,Loss:1.4266563653945923\n",
      "训练次数：4116,Loss:1.589660406112671\n",
      "训练次数：4117,Loss:1.3492178916931152\n",
      "训练次数：4118,Loss:1.509653091430664\n",
      "训练次数：4119,Loss:1.3787578344345093\n",
      "训练次数：4120,Loss:1.7064006328582764\n",
      "训练次数：4121,Loss:1.2315806150436401\n",
      "训练次数：4122,Loss:1.3874987363815308\n",
      "训练次数：4123,Loss:1.32278573513031\n",
      "训练次数：4124,Loss:1.4507758617401123\n",
      "训练次数：4125,Loss:1.572016954421997\n",
      "训练次数：4126,Loss:1.470045566558838\n",
      "训练次数：4127,Loss:1.3695101737976074\n",
      "训练次数：4128,Loss:1.3005211353302002\n",
      "训练次数：4129,Loss:1.4100430011749268\n",
      "训练次数：4130,Loss:1.3662720918655396\n",
      "训练次数：4131,Loss:1.3836917877197266\n",
      "训练次数：4132,Loss:1.4823178052902222\n",
      "训练次数：4133,Loss:1.4345016479492188\n",
      "训练次数：4134,Loss:1.539928913116455\n",
      "训练次数：4135,Loss:1.5163488388061523\n",
      "训练次数：4136,Loss:1.3695567846298218\n",
      "训练次数：4137,Loss:1.4108742475509644\n",
      "训练次数：4138,Loss:1.288727045059204\n",
      "训练次数：4139,Loss:1.3286412954330444\n",
      "训练次数：4140,Loss:1.5783110857009888\n",
      "训练次数：4141,Loss:1.3149174451828003\n",
      "训练次数：4142,Loss:1.4592256546020508\n",
      "训练次数：4143,Loss:1.440704584121704\n",
      "训练次数：4144,Loss:1.5367687940597534\n",
      "训练次数：4145,Loss:1.532888412475586\n",
      "训练次数：4146,Loss:1.5842984914779663\n",
      "训练次数：4147,Loss:1.6506295204162598\n",
      "训练次数：4148,Loss:1.4166359901428223\n",
      "训练次数：4149,Loss:1.2640535831451416\n",
      "训练次数：4150,Loss:1.306808590888977\n",
      "训练次数：4151,Loss:1.470505714416504\n",
      "训练次数：4152,Loss:1.4093459844589233\n",
      "训练次数：4153,Loss:1.3410747051239014\n",
      "训练次数：4154,Loss:1.7320410013198853\n",
      "训练次数：4155,Loss:1.289028525352478\n",
      "训练次数：4156,Loss:1.565226435661316\n",
      "训练次数：4157,Loss:1.352492094039917\n",
      "训练次数：4158,Loss:1.4949913024902344\n",
      "训练次数：4159,Loss:1.5789365768432617\n",
      "训练次数：4160,Loss:1.5734882354736328\n",
      "训练次数：4161,Loss:1.3944796323776245\n",
      "训练次数：4162,Loss:1.397800087928772\n",
      "训练次数：4163,Loss:1.390450119972229\n",
      "训练次数：4164,Loss:1.3802138566970825\n",
      "训练次数：4165,Loss:1.396179437637329\n",
      "训练次数：4166,Loss:1.312822937965393\n",
      "训练次数：4167,Loss:1.4224598407745361\n",
      "训练次数：4168,Loss:1.1995128393173218\n",
      "训练次数：4169,Loss:1.5380301475524902\n",
      "训练次数：4170,Loss:1.4140267372131348\n",
      "训练次数：4171,Loss:1.3019596338272095\n",
      "训练次数：4172,Loss:1.406275987625122\n",
      "训练次数：4173,Loss:1.3649885654449463\n",
      "训练次数：4174,Loss:1.5120904445648193\n",
      "训练次数：4175,Loss:1.0872080326080322\n",
      "训练次数：4176,Loss:1.2235506772994995\n",
      "训练次数：4177,Loss:1.4045085906982422\n",
      "训练次数：4178,Loss:1.4167195558547974\n",
      "训练次数：4179,Loss:1.2833821773529053\n",
      "训练次数：4180,Loss:1.4773296117782593\n",
      "训练次数：4181,Loss:1.4060169458389282\n",
      "训练次数：4182,Loss:1.364366888999939\n",
      "训练次数：4183,Loss:1.3180019855499268\n",
      "训练次数：4184,Loss:1.253714919090271\n",
      "训练次数：4185,Loss:1.2977283000946045\n",
      "训练次数：4186,Loss:1.3160154819488525\n",
      "训练次数：4187,Loss:1.707502841949463\n",
      "训练次数：4188,Loss:1.4585288763046265\n",
      "训练次数：4189,Loss:1.5091527700424194\n",
      "训练次数：4190,Loss:1.1700712442398071\n",
      "训练次数：4191,Loss:1.331302523612976\n",
      "训练次数：4192,Loss:1.456641674041748\n",
      "训练次数：4193,Loss:1.2506667375564575\n",
      "训练次数：4194,Loss:1.5060842037200928\n",
      "训练次数：4195,Loss:1.3327832221984863\n",
      "训练次数：4196,Loss:1.3618810176849365\n",
      "训练次数：4197,Loss:1.4566303491592407\n",
      "训练次数：4198,Loss:1.4221158027648926\n",
      "训练次数：4199,Loss:1.584489107131958\n",
      "训练次数：4200,Loss:1.5447232723236084\n",
      "训练次数：4201,Loss:1.3918524980545044\n",
      "训练次数：4202,Loss:1.3819715976715088\n",
      "训练次数：4203,Loss:1.3266226053237915\n",
      "训练次数：4204,Loss:1.2700353860855103\n",
      "训练次数：4205,Loss:1.3059855699539185\n",
      "训练次数：4206,Loss:1.3714474439620972\n",
      "训练次数：4207,Loss:1.5938999652862549\n",
      "训练次数：4208,Loss:1.3327261209487915\n",
      "训练次数：4209,Loss:1.2581799030303955\n",
      "训练次数：4210,Loss:1.4658435583114624\n",
      "训练次数：4211,Loss:1.3503590822219849\n",
      "训练次数：4212,Loss:1.333052635192871\n",
      "训练次数：4213,Loss:1.358020544052124\n",
      "训练次数：4214,Loss:1.2914094924926758\n",
      "训练次数：4215,Loss:1.2812186479568481\n",
      "训练次数：4216,Loss:1.6382942199707031\n",
      "训练次数：4217,Loss:1.3504482507705688\n",
      "训练次数：4218,Loss:1.5060523748397827\n",
      "训练次数：4219,Loss:1.4638853073120117\n",
      "训练次数：4220,Loss:1.3310625553131104\n",
      "训练次数：4221,Loss:1.311683177947998\n",
      "训练次数：4222,Loss:1.4317827224731445\n",
      "训练次数：4223,Loss:1.479866623878479\n",
      "训练次数：4224,Loss:1.414698839187622\n",
      "训练次数：4225,Loss:1.3153835535049438\n",
      "训练次数：4226,Loss:1.482132911682129\n",
      "训练次数：4227,Loss:1.547189474105835\n",
      "训练次数：4228,Loss:1.525410771369934\n",
      "训练次数：4229,Loss:1.2931851148605347\n",
      "训练次数：4230,Loss:1.3619240522384644\n",
      "训练次数：4231,Loss:1.457932949066162\n",
      "训练次数：4232,Loss:1.5232982635498047\n",
      "训练次数：4233,Loss:1.380615472793579\n",
      "训练次数：4234,Loss:1.5372437238693237\n",
      "训练次数：4235,Loss:1.3756871223449707\n",
      "训练次数：4236,Loss:1.4328420162200928\n",
      "训练次数：4237,Loss:1.2436553239822388\n",
      "训练次数：4238,Loss:1.3687949180603027\n",
      "训练次数：4239,Loss:1.381575584411621\n",
      "训练次数：4240,Loss:1.3969391584396362\n",
      "训练次数：4241,Loss:1.319427251815796\n",
      "训练次数：4242,Loss:1.3714475631713867\n",
      "训练次数：4243,Loss:1.3554279804229736\n",
      "训练次数：4244,Loss:1.4111500978469849\n",
      "训练次数：4245,Loss:1.5524494647979736\n",
      "训练次数：4246,Loss:1.4713624715805054\n",
      "训练次数：4247,Loss:1.4297722578048706\n",
      "训练次数：4248,Loss:1.1557090282440186\n",
      "训练次数：4249,Loss:1.3840996026992798\n",
      "训练次数：4250,Loss:1.372318148612976\n",
      "训练次数：4251,Loss:1.6750502586364746\n",
      "训练次数：4252,Loss:1.4486602544784546\n",
      "训练次数：4253,Loss:1.2821153402328491\n",
      "训练次数：4254,Loss:1.326580286026001\n",
      "训练次数：4255,Loss:1.4155552387237549\n",
      "训练次数：4256,Loss:1.156935214996338\n",
      "训练次数：4257,Loss:1.2670267820358276\n",
      "训练次数：4258,Loss:1.4427162408828735\n",
      "训练次数：4259,Loss:1.372796893119812\n",
      "训练次数：4260,Loss:1.2658900022506714\n",
      "训练次数：4261,Loss:1.404341220855713\n",
      "训练次数：4262,Loss:1.4798483848571777\n",
      "训练次数：4263,Loss:1.2917981147766113\n",
      "训练次数：4264,Loss:1.5142083168029785\n",
      "训练次数：4265,Loss:1.3687145709991455\n",
      "训练次数：4266,Loss:1.3662192821502686\n",
      "训练次数：4267,Loss:1.3878108263015747\n",
      "训练次数：4268,Loss:1.4425790309906006\n",
      "训练次数：4269,Loss:1.2306431531906128\n",
      "训练次数：4270,Loss:1.5586941242218018\n",
      "训练次数：4271,Loss:1.4270142316818237\n",
      "训练次数：4272,Loss:1.1202293634414673\n",
      "训练次数：4273,Loss:1.4956096410751343\n",
      "训练次数：4274,Loss:1.3026539087295532\n",
      "训练次数：4275,Loss:1.0475187301635742\n",
      "训练次数：4276,Loss:1.23481285572052\n",
      "训练次数：4277,Loss:1.4640079736709595\n",
      "训练次数：4278,Loss:1.542077898979187\n",
      "训练次数：4279,Loss:1.2929902076721191\n",
      "训练次数：4280,Loss:1.354392409324646\n",
      "训练次数：4281,Loss:1.0533652305603027\n",
      "训练次数：4282,Loss:1.505997657775879\n",
      "训练次数：4283,Loss:1.2394837141036987\n",
      "训练次数：4284,Loss:1.3276429176330566\n",
      "训练次数：4285,Loss:1.4983410835266113\n",
      "训练次数：4286,Loss:1.2808059453964233\n",
      "训练次数：4287,Loss:1.2568501234054565\n",
      "训练次数：4288,Loss:1.3458383083343506\n",
      "训练次数：4289,Loss:1.2862602472305298\n",
      "训练次数：4290,Loss:1.5128182172775269\n",
      "训练次数：4291,Loss:1.5067905187606812\n",
      "训练次数：4292,Loss:1.402707815170288\n",
      "训练次数：4293,Loss:1.338378667831421\n",
      "训练次数：4294,Loss:1.317667841911316\n",
      "训练次数：4295,Loss:1.2739588022232056\n",
      "训练次数：4296,Loss:1.3331382274627686\n",
      "训练次数：4297,Loss:1.3240587711334229\n",
      "训练次数：4298,Loss:1.5579568147659302\n",
      "训练次数：4299,Loss:1.4187325239181519\n",
      "训练次数：4300,Loss:1.2250169515609741\n",
      "训练次数：4301,Loss:1.3179504871368408\n",
      "训练次数：4302,Loss:1.1022064685821533\n",
      "训练次数：4303,Loss:1.3863224983215332\n",
      "训练次数：4304,Loss:1.5767313241958618\n",
      "训练次数：4305,Loss:1.4872796535491943\n",
      "训练次数：4306,Loss:1.3547917604446411\n",
      "训练次数：4307,Loss:1.3555636405944824\n",
      "训练次数：4308,Loss:1.5737638473510742\n",
      "训练次数：4309,Loss:1.217649221420288\n",
      "训练次数：4310,Loss:1.3200877904891968\n",
      "训练次数：4311,Loss:1.295708179473877\n",
      "训练次数：4312,Loss:1.577561855316162\n",
      "训练次数：4313,Loss:1.41278076171875\n",
      "训练次数：4314,Loss:1.2168833017349243\n",
      "训练次数：4315,Loss:1.4603776931762695\n",
      "训练次数：4316,Loss:1.2566237449645996\n",
      "训练次数：4317,Loss:1.3274210691452026\n",
      "训练次数：4318,Loss:1.0387581586837769\n",
      "训练次数：4319,Loss:1.33936607837677\n",
      "训练次数：4320,Loss:1.1689270734786987\n",
      "训练次数：4321,Loss:1.6353514194488525\n",
      "训练次数：4322,Loss:1.308279275894165\n",
      "训练次数：4323,Loss:1.4616628885269165\n",
      "训练次数：4324,Loss:1.378796935081482\n",
      "训练次数：4325,Loss:1.425097107887268\n",
      "训练次数：4326,Loss:1.312261939048767\n",
      "训练次数：4327,Loss:1.598457932472229\n",
      "训练次数：4328,Loss:1.2962969541549683\n",
      "训练次数：4329,Loss:1.3784695863723755\n",
      "训练次数：4330,Loss:1.4588249921798706\n",
      "训练次数：4331,Loss:1.4474704265594482\n",
      "训练次数：4332,Loss:1.304344654083252\n",
      "训练次数：4333,Loss:1.5370731353759766\n",
      "训练次数：4334,Loss:1.2916913032531738\n",
      "训练次数：4335,Loss:1.3401838541030884\n",
      "训练次数：4336,Loss:1.4938623905181885\n",
      "训练次数：4337,Loss:1.3428974151611328\n",
      "训练次数：4338,Loss:1.2480976581573486\n",
      "训练次数：4339,Loss:1.3921695947647095\n",
      "训练次数：4340,Loss:1.3426684141159058\n",
      "训练次数：4341,Loss:1.4166295528411865\n",
      "训练次数：4342,Loss:1.2712727785110474\n",
      "训练次数：4343,Loss:1.4746425151824951\n",
      "训练次数：4344,Loss:1.3183974027633667\n",
      "训练次数：4345,Loss:1.4082304239273071\n",
      "训练次数：4346,Loss:1.7287828922271729\n",
      "训练次数：4347,Loss:1.536742091178894\n",
      "训练次数：4348,Loss:1.4224612712860107\n",
      "训练次数：4349,Loss:1.3355729579925537\n",
      "训练次数：4350,Loss:1.3836798667907715\n",
      "训练次数：4351,Loss:1.3982346057891846\n",
      "训练次数：4352,Loss:1.2446404695510864\n",
      "训练次数：4353,Loss:1.4569098949432373\n",
      "训练次数：4354,Loss:1.2630337476730347\n",
      "训练次数：4355,Loss:1.4723097085952759\n",
      "训练次数：4356,Loss:1.4027100801467896\n",
      "训练次数：4357,Loss:1.4114984273910522\n",
      "训练次数：4358,Loss:1.3175641298294067\n",
      "训练次数：4359,Loss:1.3752001523971558\n",
      "训练次数：4360,Loss:1.4424740076065063\n",
      "训练次数：4361,Loss:1.1923826932907104\n",
      "训练次数：4362,Loss:1.1220662593841553\n",
      "训练次数：4363,Loss:1.215728759765625\n",
      "训练次数：4364,Loss:1.3279285430908203\n",
      "训练次数：4365,Loss:1.438189148902893\n",
      "训练次数：4366,Loss:1.4788273572921753\n",
      "训练次数：4367,Loss:1.5206851959228516\n",
      "训练次数：4368,Loss:1.1792298555374146\n",
      "训练次数：4369,Loss:1.150124192237854\n",
      "训练次数：4370,Loss:1.2729709148406982\n",
      "训练次数：4371,Loss:1.417513132095337\n",
      "训练次数：4372,Loss:1.4316874742507935\n",
      "训练次数：4373,Loss:1.2786974906921387\n",
      "训练次数：4374,Loss:1.4475704431533813\n",
      "训练次数：4375,Loss:1.3449623584747314\n",
      "训练次数：4376,Loss:1.1722193956375122\n",
      "训练次数：4377,Loss:1.5094799995422363\n",
      "训练次数：4378,Loss:1.337936520576477\n",
      "训练次数：4379,Loss:1.2006731033325195\n",
      "训练次数：4380,Loss:1.4067699909210205\n",
      "训练次数：4381,Loss:1.4178879261016846\n",
      "训练次数：4382,Loss:1.4960285425186157\n",
      "训练次数：4383,Loss:1.2700345516204834\n",
      "训练次数：4384,Loss:1.5082690715789795\n",
      "训练次数：4385,Loss:1.0986790657043457\n",
      "训练次数：4386,Loss:1.4831337928771973\n",
      "训练次数：4387,Loss:1.3163583278656006\n",
      "训练次数：4388,Loss:1.382492184638977\n",
      "训练次数：4389,Loss:1.432059407234192\n",
      "训练次数：4390,Loss:1.4546349048614502\n",
      "训练次数：4391,Loss:1.511375904083252\n",
      "训练次数：4392,Loss:1.5295538902282715\n",
      "训练次数：4393,Loss:1.3728784322738647\n",
      "训练次数：4394,Loss:1.169280767440796\n",
      "训练次数：4395,Loss:1.2723840475082397\n",
      "训练次数：4396,Loss:1.1080915927886963\n",
      "训练次数：4397,Loss:1.3831042051315308\n",
      "训练次数：4398,Loss:1.5345180034637451\n",
      "训练次数：4399,Loss:1.3089035749435425\n",
      "训练次数：4400,Loss:1.1094086170196533\n",
      "训练次数：4401,Loss:1.6765738725662231\n",
      "训练次数：4402,Loss:1.4412951469421387\n",
      "训练次数：4403,Loss:1.418513298034668\n",
      "训练次数：4404,Loss:1.2295691967010498\n",
      "训练次数：4405,Loss:1.4384937286376953\n",
      "训练次数：4406,Loss:1.561169981956482\n",
      "训练次数：4407,Loss:1.345685362815857\n",
      "训练次数：4408,Loss:1.4498815536499023\n",
      "训练次数：4409,Loss:1.6253682374954224\n",
      "训练次数：4410,Loss:1.3327195644378662\n",
      "训练次数：4411,Loss:1.544344186782837\n",
      "训练次数：4412,Loss:1.3646538257598877\n",
      "训练次数：4413,Loss:1.266486406326294\n",
      "训练次数：4414,Loss:1.5091044902801514\n",
      "训练次数：4415,Loss:1.5570701360702515\n",
      "训练次数：4416,Loss:1.4535393714904785\n",
      "训练次数：4417,Loss:1.3104407787322998\n",
      "训练次数：4418,Loss:1.3485758304595947\n",
      "训练次数：4419,Loss:1.4106900691986084\n",
      "训练次数：4420,Loss:1.4296766519546509\n",
      "训练次数：4421,Loss:1.4394127130508423\n",
      "训练次数：4422,Loss:1.35220468044281\n",
      "训练次数：4423,Loss:1.4798775911331177\n",
      "训练次数：4424,Loss:1.4956886768341064\n",
      "训练次数：4425,Loss:1.602414608001709\n",
      "训练次数：4426,Loss:1.3505538702011108\n",
      "训练次数：4427,Loss:1.6490243673324585\n",
      "训练次数：4428,Loss:1.2264976501464844\n",
      "训练次数：4429,Loss:1.348842740058899\n",
      "训练次数：4430,Loss:1.3314155340194702\n",
      "训练次数：4431,Loss:1.5542460680007935\n",
      "训练次数：4432,Loss:1.4769515991210938\n",
      "训练次数：4433,Loss:1.3499327898025513\n",
      "训练次数：4434,Loss:1.5618183612823486\n",
      "训练次数：4435,Loss:1.453955054283142\n",
      "训练次数：4436,Loss:1.6502044200897217\n",
      "训练次数：4437,Loss:1.4530680179595947\n",
      "训练次数：4438,Loss:1.2314059734344482\n",
      "训练次数：4439,Loss:1.441959261894226\n",
      "训练次数：4440,Loss:1.5785197019577026\n",
      "训练次数：4441,Loss:1.3815321922302246\n",
      "训练次数：4442,Loss:1.480692744255066\n",
      "训练次数：4443,Loss:1.28615140914917\n",
      "训练次数：4444,Loss:1.218764066696167\n",
      "训练次数：4445,Loss:1.340944766998291\n",
      "训练次数：4446,Loss:1.2010911703109741\n",
      "训练次数：4447,Loss:1.4328656196594238\n",
      "训练次数：4448,Loss:1.436030626296997\n",
      "训练次数：4449,Loss:1.3529009819030762\n",
      "训练次数：4450,Loss:1.3848768472671509\n",
      "训练次数：4451,Loss:1.4329626560211182\n",
      "训练次数：4452,Loss:1.4440706968307495\n",
      "训练次数：4453,Loss:1.3308658599853516\n",
      "训练次数：4454,Loss:1.311513066291809\n",
      "训练次数：4455,Loss:1.3195024728775024\n",
      "训练次数：4456,Loss:1.318734049797058\n",
      "训练次数：4457,Loss:1.4385557174682617\n",
      "训练次数：4458,Loss:1.4053598642349243\n",
      "训练次数：4459,Loss:1.3357051610946655\n",
      "训练次数：4460,Loss:1.469377040863037\n",
      "训练次数：4461,Loss:1.3432300090789795\n",
      "训练次数：4462,Loss:1.6920864582061768\n",
      "训练次数：4463,Loss:1.348506212234497\n",
      "训练次数：4464,Loss:1.487333059310913\n",
      "训练次数：4465,Loss:1.2134335041046143\n",
      "训练次数：4466,Loss:1.284326195716858\n",
      "训练次数：4467,Loss:1.518278956413269\n",
      "训练次数：4468,Loss:1.404126524925232\n",
      "训练次数：4469,Loss:1.2817518711090088\n",
      "训练次数：4470,Loss:1.4563450813293457\n",
      "训练次数：4471,Loss:1.326988935470581\n",
      "训练次数：4472,Loss:1.3698935508728027\n",
      "训练次数：4473,Loss:1.472747564315796\n",
      "训练次数：4474,Loss:1.3207666873931885\n",
      "训练次数：4475,Loss:1.3844165802001953\n",
      "训练次数：4476,Loss:1.4954640865325928\n",
      "训练次数：4477,Loss:1.3147363662719727\n",
      "训练次数：4478,Loss:1.4457333087921143\n",
      "训练次数：4479,Loss:1.4173767566680908\n",
      "训练次数：4480,Loss:1.1899609565734863\n",
      "训练次数：4481,Loss:1.315536618232727\n",
      "训练次数：4482,Loss:1.2604761123657227\n",
      "训练次数：4483,Loss:1.2444223165512085\n",
      "训练次数：4484,Loss:1.4843116998672485\n",
      "训练次数：4485,Loss:1.2966681718826294\n",
      "训练次数：4486,Loss:1.336031436920166\n",
      "训练次数：4487,Loss:1.471282720565796\n",
      "训练次数：4488,Loss:1.7978037595748901\n",
      "训练次数：4489,Loss:1.2354509830474854\n",
      "训练次数：4490,Loss:1.4102110862731934\n",
      "训练次数：4491,Loss:1.336625576019287\n",
      "训练次数：4492,Loss:1.430084466934204\n",
      "训练次数：4493,Loss:1.231849193572998\n",
      "训练次数：4494,Loss:1.4260555505752563\n",
      "训练次数：4495,Loss:1.3743795156478882\n",
      "训练次数：4496,Loss:1.2944860458374023\n",
      "训练次数：4497,Loss:1.5121073722839355\n",
      "训练次数：4498,Loss:1.233597993850708\n",
      "训练次数：4499,Loss:1.5614540576934814\n",
      "训练次数：4500,Loss:1.3501471281051636\n",
      "训练次数：4501,Loss:1.4373440742492676\n",
      "训练次数：4502,Loss:1.1813844442367554\n",
      "训练次数：4503,Loss:1.4820835590362549\n",
      "训练次数：4504,Loss:1.4847605228424072\n",
      "训练次数：4505,Loss:1.3922008275985718\n",
      "训练次数：4506,Loss:1.6754258871078491\n",
      "训练次数：4507,Loss:1.438141942024231\n",
      "训练次数：4508,Loss:1.3915499448776245\n",
      "训练次数：4509,Loss:1.4886211156845093\n",
      "训练次数：4510,Loss:1.460229754447937\n",
      "训练次数：4511,Loss:1.4857628345489502\n",
      "训练次数：4512,Loss:1.2184057235717773\n",
      "训练次数：4513,Loss:1.1374858617782593\n",
      "训练次数：4514,Loss:1.5895626544952393\n",
      "训练次数：4515,Loss:1.491705060005188\n",
      "训练次数：4516,Loss:1.1290152072906494\n",
      "训练次数：4517,Loss:1.046897530555725\n",
      "训练次数：4518,Loss:1.3186497688293457\n",
      "训练次数：4519,Loss:1.2798173427581787\n",
      "训练次数：4520,Loss:1.4821799993515015\n",
      "训练次数：4521,Loss:1.2900609970092773\n",
      "训练次数：4522,Loss:1.4184099435806274\n",
      "训练次数：4523,Loss:1.4438056945800781\n",
      "训练次数：4524,Loss:1.2111538648605347\n",
      "训练次数：4525,Loss:1.3138386011123657\n",
      "训练次数：4526,Loss:1.3422659635543823\n",
      "训练次数：4527,Loss:1.372506022453308\n",
      "训练次数：4528,Loss:1.2930855751037598\n",
      "训练次数：4529,Loss:1.4648686647415161\n",
      "训练次数：4530,Loss:1.5737698078155518\n",
      "训练次数：4531,Loss:1.3997982740402222\n",
      "训练次数：4532,Loss:1.5017874240875244\n",
      "训练次数：4533,Loss:1.328749418258667\n",
      "训练次数：4534,Loss:1.239101767539978\n",
      "训练次数：4535,Loss:1.3111661672592163\n",
      "训练次数：4536,Loss:1.165408968925476\n",
      "训练次数：4537,Loss:1.2324117422103882\n",
      "训练次数：4538,Loss:1.6892622709274292\n",
      "训练次数：4539,Loss:1.5151069164276123\n",
      "训练次数：4540,Loss:1.2155206203460693\n",
      "训练次数：4541,Loss:1.312835454940796\n",
      "训练次数：4542,Loss:1.5250989198684692\n",
      "训练次数：4543,Loss:1.4125243425369263\n",
      "训练次数：4544,Loss:1.1207655668258667\n",
      "训练次数：4545,Loss:1.4031771421432495\n",
      "训练次数：4546,Loss:1.178128957748413\n",
      "训练次数：4547,Loss:1.3850973844528198\n",
      "训练次数：4548,Loss:1.4090138673782349\n",
      "训练次数：4549,Loss:1.4175877571105957\n",
      "训练次数：4550,Loss:1.5781937837600708\n",
      "训练次数：4551,Loss:1.4527523517608643\n",
      "训练次数：4552,Loss:1.390099287033081\n",
      "训练次数：4553,Loss:1.4465978145599365\n",
      "训练次数：4554,Loss:1.2855710983276367\n",
      "训练次数：4555,Loss:1.3174067735671997\n",
      "训练次数：4556,Loss:1.5672013759613037\n",
      "训练次数：4557,Loss:1.3308006525039673\n",
      "训练次数：4558,Loss:1.242149829864502\n",
      "训练次数：4559,Loss:1.3627911806106567\n",
      "训练次数：4560,Loss:1.6216697692871094\n",
      "训练次数：4561,Loss:1.3277299404144287\n",
      "训练次数：4562,Loss:1.417655348777771\n",
      "训练次数：4563,Loss:1.164846420288086\n",
      "训练次数：4564,Loss:1.25642728805542\n",
      "训练次数：4565,Loss:1.4331330060958862\n",
      "训练次数：4566,Loss:1.341029405593872\n",
      "训练次数：4567,Loss:1.3817218542099\n",
      "训练次数：4568,Loss:1.3254245519638062\n",
      "训练次数：4569,Loss:1.614079236984253\n",
      "训练次数：4570,Loss:1.3486943244934082\n",
      "训练次数：4571,Loss:1.3026015758514404\n",
      "训练次数：4572,Loss:1.1314449310302734\n",
      "训练次数：4573,Loss:1.1660873889923096\n",
      "训练次数：4574,Loss:1.3147854804992676\n",
      "训练次数：4575,Loss:1.5825443267822266\n",
      "训练次数：4576,Loss:1.6388132572174072\n",
      "训练次数：4577,Loss:1.1974396705627441\n",
      "训练次数：4578,Loss:1.5272387266159058\n",
      "训练次数：4579,Loss:1.3694722652435303\n",
      "训练次数：4580,Loss:1.3338227272033691\n",
      "训练次数：4581,Loss:1.4020031690597534\n",
      "训练次数：4582,Loss:1.2425346374511719\n",
      "训练次数：4583,Loss:1.3075921535491943\n",
      "训练次数：4584,Loss:1.3526430130004883\n",
      "训练次数：4585,Loss:1.4129658937454224\n",
      "训练次数：4586,Loss:1.5917353630065918\n",
      "训练次数：4587,Loss:1.4616374969482422\n",
      "训练次数：4588,Loss:1.2524569034576416\n",
      "训练次数：4589,Loss:1.4180455207824707\n",
      "训练次数：4590,Loss:1.5167356729507446\n",
      "训练次数：4591,Loss:1.5107929706573486\n",
      "训练次数：4592,Loss:1.1035515069961548\n",
      "训练次数：4593,Loss:1.197449803352356\n",
      "训练次数：4594,Loss:1.2906767129898071\n",
      "训练次数：4595,Loss:1.435395359992981\n",
      "训练次数：4596,Loss:1.6501387357711792\n",
      "训练次数：4597,Loss:1.352871298789978\n",
      "训练次数：4598,Loss:1.249405860900879\n",
      "训练次数：4599,Loss:1.3604120016098022\n",
      "训练次数：4600,Loss:1.4092023372650146\n",
      "训练次数：4601,Loss:1.4076988697052002\n",
      "训练次数：4602,Loss:1.4729397296905518\n",
      "训练次数：4603,Loss:1.5792522430419922\n",
      "训练次数：4604,Loss:1.505638599395752\n",
      "训练次数：4605,Loss:1.4124194383621216\n",
      "训练次数：4606,Loss:1.3660149574279785\n",
      "训练次数：4607,Loss:1.3874504566192627\n",
      "训练次数：4608,Loss:1.4002435207366943\n",
      "训练次数：4609,Loss:1.1803451776504517\n",
      "训练次数：4610,Loss:1.5785189867019653\n",
      "训练次数：4611,Loss:1.4693692922592163\n",
      "训练次数：4612,Loss:1.4367053508758545\n",
      "训练次数：4613,Loss:1.3427106142044067\n",
      "训练次数：4614,Loss:1.128487467765808\n",
      "训练次数：4615,Loss:1.3676007986068726\n",
      "训练次数：4616,Loss:1.3715579509735107\n",
      "训练次数：4617,Loss:1.583553671836853\n",
      "训练次数：4618,Loss:1.4624239206314087\n",
      "训练次数：4619,Loss:1.1646649837493896\n",
      "训练次数：4620,Loss:1.136889934539795\n",
      "训练次数：4621,Loss:1.469053864479065\n",
      "训练次数：4622,Loss:1.1351048946380615\n",
      "训练次数：4623,Loss:1.3098785877227783\n",
      "训练次数：4624,Loss:1.323979377746582\n",
      "训练次数：4625,Loss:1.3245201110839844\n",
      "训练次数：4626,Loss:1.3054994344711304\n",
      "训练次数：4627,Loss:1.2091196775436401\n",
      "训练次数：4628,Loss:1.2919104099273682\n",
      "训练次数：4629,Loss:1.3566713333129883\n",
      "训练次数：4630,Loss:1.310409665107727\n",
      "训练次数：4631,Loss:1.3079770803451538\n",
      "训练次数：4632,Loss:1.3282737731933594\n",
      "训练次数：4633,Loss:1.1891332864761353\n",
      "训练次数：4634,Loss:1.5727227926254272\n",
      "训练次数：4635,Loss:1.4014551639556885\n",
      "训练次数：4636,Loss:1.2585675716400146\n",
      "训练次数：4637,Loss:1.5331337451934814\n",
      "训练次数：4638,Loss:1.6113914251327515\n",
      "训练次数：4639,Loss:1.2930335998535156\n",
      "训练次数：4640,Loss:1.3855282068252563\n",
      "训练次数：4641,Loss:1.177486777305603\n",
      "训练次数：4642,Loss:1.2350958585739136\n",
      "训练次数：4643,Loss:1.2824904918670654\n",
      "训练次数：4644,Loss:1.2930173873901367\n",
      "训练次数：4645,Loss:1.3883672952651978\n",
      "训练次数：4646,Loss:1.4426605701446533\n",
      "训练次数：4647,Loss:1.185250997543335\n",
      "训练次数：4648,Loss:1.4005812406539917\n",
      "训练次数：4649,Loss:1.1797999143600464\n",
      "训练次数：4650,Loss:1.1333531141281128\n",
      "训练次数：4651,Loss:1.4240527153015137\n",
      "训练次数：4652,Loss:1.423294186592102\n",
      "训练次数：4653,Loss:1.6771948337554932\n",
      "训练次数：4654,Loss:1.2337327003479004\n",
      "训练次数：4655,Loss:1.2340589761734009\n",
      "训练次数：4656,Loss:1.279894471168518\n",
      "训练次数：4657,Loss:1.2495434284210205\n",
      "训练次数：4658,Loss:1.5858535766601562\n",
      "训练次数：4659,Loss:1.4766197204589844\n",
      "训练次数：4660,Loss:1.1929733753204346\n",
      "训练次数：4661,Loss:1.7868406772613525\n",
      "训练次数：4662,Loss:1.3264997005462646\n",
      "训练次数：4663,Loss:1.2828152179718018\n",
      "训练次数：4664,Loss:1.3909779787063599\n",
      "训练次数：4665,Loss:1.3613324165344238\n",
      "训练次数：4666,Loss:1.3060529232025146\n",
      "训练次数：4667,Loss:1.2824140787124634\n",
      "训练次数：4668,Loss:1.4190937280654907\n",
      "训练次数：4669,Loss:1.561359167098999\n",
      "训练次数：4670,Loss:1.252968192100525\n",
      "训练次数：4671,Loss:1.2658759355545044\n",
      "训练次数：4672,Loss:1.4670515060424805\n",
      "训练次数：4673,Loss:1.3092632293701172\n",
      "训练次数：4674,Loss:1.3452985286712646\n",
      "训练次数：4675,Loss:1.2191582918167114\n",
      "训练次数：4676,Loss:1.4694569110870361\n",
      "训练次数：4677,Loss:1.259973406791687\n",
      "训练次数：4678,Loss:1.4770323038101196\n",
      "训练次数：4679,Loss:1.0776376724243164\n",
      "训练次数：4680,Loss:1.215898036956787\n",
      "训练次数：4681,Loss:1.392359972000122\n",
      "训练次数：4682,Loss:1.3936015367507935\n",
      "训练次数：4683,Loss:1.37594735622406\n",
      "训练次数：4684,Loss:1.2339627742767334\n",
      "训练次数：4685,Loss:1.3134822845458984\n",
      "训练次数：4686,Loss:1.1933529376983643\n",
      "训练次数：4687,Loss:1.5207260847091675\n",
      "训练次数：4688,Loss:1.3420298099517822\n",
      "训练次数：4689,Loss:1.2972973585128784\n",
      "训练次数：4690,Loss:1.242277979850769\n",
      "训练次数：4691,Loss:1.426584005355835\n",
      "训练次数：4692,Loss:2.054981231689453\n",
      "-----第 7 轮训练开始-----\n",
      "训练次数：4693,Loss:1.5434824228286743\n",
      "训练次数：4694,Loss:1.3730770349502563\n",
      "训练次数：4695,Loss:1.4884374141693115\n",
      "训练次数：4696,Loss:1.0381029844284058\n",
      "训练次数：4697,Loss:1.3245418071746826\n",
      "训练次数：4698,Loss:1.3453973531723022\n",
      "训练次数：4699,Loss:1.4050532579421997\n",
      "训练次数：4700,Loss:1.2829397916793823\n",
      "训练次数：4701,Loss:1.2572840452194214\n",
      "训练次数：4702,Loss:1.5360732078552246\n",
      "训练次数：4703,Loss:1.475509762763977\n",
      "训练次数：4704,Loss:1.4416258335113525\n",
      "训练次数：4705,Loss:1.4501968622207642\n",
      "训练次数：4706,Loss:1.5577653646469116\n",
      "训练次数：4707,Loss:1.4242011308670044\n",
      "训练次数：4708,Loss:1.1270912885665894\n",
      "训练次数：4709,Loss:1.3335638046264648\n",
      "训练次数：4710,Loss:1.1891969442367554\n",
      "训练次数：4711,Loss:1.3091084957122803\n",
      "训练次数：4712,Loss:1.2426429986953735\n",
      "训练次数：4713,Loss:1.506403923034668\n",
      "训练次数：4714,Loss:1.2502726316452026\n",
      "训练次数：4715,Loss:1.3614259958267212\n",
      "训练次数：4716,Loss:1.399266242980957\n",
      "训练次数：4717,Loss:1.342224359512329\n",
      "训练次数：4718,Loss:1.2181434631347656\n",
      "训练次数：4719,Loss:1.3166286945343018\n",
      "训练次数：4720,Loss:1.235058307647705\n",
      "训练次数：4721,Loss:1.1834272146224976\n",
      "训练次数：4722,Loss:1.1986339092254639\n",
      "训练次数：4723,Loss:1.4950610399246216\n",
      "训练次数：4724,Loss:1.5192925930023193\n",
      "训练次数：4725,Loss:1.267842411994934\n",
      "训练次数：4726,Loss:1.1678564548492432\n",
      "训练次数：4727,Loss:1.3271344900131226\n",
      "训练次数：4728,Loss:1.2189463376998901\n",
      "训练次数：4729,Loss:1.3701831102371216\n",
      "训练次数：4730,Loss:1.4221991300582886\n",
      "训练次数：4731,Loss:1.2503957748413086\n",
      "训练次数：4732,Loss:1.6846078634262085\n",
      "训练次数：4733,Loss:1.4785194396972656\n",
      "训练次数：4734,Loss:1.0951899290084839\n",
      "训练次数：4735,Loss:1.5318752527236938\n",
      "训练次数：4736,Loss:1.5310564041137695\n",
      "训练次数：4737,Loss:1.1686692237854004\n",
      "训练次数：4738,Loss:1.2209076881408691\n",
      "训练次数：4739,Loss:1.4266126155853271\n",
      "训练次数：4740,Loss:1.5308324098587036\n",
      "训练次数：4741,Loss:1.3297888040542603\n",
      "训练次数：4742,Loss:1.518860101699829\n",
      "训练次数：4743,Loss:1.2992286682128906\n",
      "训练次数：4744,Loss:1.4770385026931763\n",
      "训练次数：4745,Loss:1.3726087808609009\n",
      "训练次数：4746,Loss:1.4307528734207153\n",
      "训练次数：4747,Loss:1.2625828981399536\n",
      "训练次数：4748,Loss:1.2304224967956543\n",
      "训练次数：4749,Loss:1.4177618026733398\n",
      "训练次数：4750,Loss:1.2069262266159058\n",
      "训练次数：4751,Loss:1.1748546361923218\n",
      "训练次数：4752,Loss:1.2484209537506104\n",
      "训练次数：4753,Loss:1.3480349779129028\n",
      "训练次数：4754,Loss:1.4814547300338745\n",
      "训练次数：4755,Loss:1.417359709739685\n",
      "训练次数：4756,Loss:1.3824235200881958\n",
      "训练次数：4757,Loss:1.4695417881011963\n",
      "训练次数：4758,Loss:1.3836655616760254\n",
      "训练次数：4759,Loss:1.3112106323242188\n",
      "训练次数：4760,Loss:1.4053601026535034\n",
      "训练次数：4761,Loss:1.13197660446167\n",
      "训练次数：4762,Loss:1.2524436712265015\n",
      "训练次数：4763,Loss:1.504978895187378\n",
      "训练次数：4764,Loss:1.1413540840148926\n",
      "训练次数：4765,Loss:1.3570481538772583\n",
      "训练次数：4766,Loss:1.2638788223266602\n",
      "训练次数：4767,Loss:1.4179179668426514\n",
      "训练次数：4768,Loss:1.4777867794036865\n",
      "训练次数：4769,Loss:1.432471752166748\n",
      "训练次数：4770,Loss:1.498618721961975\n",
      "训练次数：4771,Loss:1.223900318145752\n",
      "训练次数：4772,Loss:1.617177963256836\n",
      "训练次数：4773,Loss:1.5170838832855225\n",
      "训练次数：4774,Loss:1.2096136808395386\n",
      "训练次数：4775,Loss:1.4271926879882812\n",
      "训练次数：4776,Loss:1.28372323513031\n",
      "训练次数：4777,Loss:1.4418914318084717\n",
      "训练次数：4778,Loss:1.3208719491958618\n",
      "训练次数：4779,Loss:1.300749659538269\n",
      "训练次数：4780,Loss:1.0282143354415894\n",
      "训练次数：4781,Loss:1.3570469617843628\n",
      "训练次数：4782,Loss:1.3226408958435059\n",
      "训练次数：4783,Loss:1.3735792636871338\n",
      "训练次数：4784,Loss:1.4169491529464722\n",
      "训练次数：4785,Loss:1.1305164098739624\n",
      "训练次数：4786,Loss:1.2968157529830933\n",
      "训练次数：4787,Loss:1.2902402877807617\n",
      "训练次数：4788,Loss:1.546751618385315\n",
      "训练次数：4789,Loss:1.1751552820205688\n",
      "训练次数：4790,Loss:1.468888282775879\n",
      "训练次数：4791,Loss:1.1608386039733887\n",
      "训练次数：4792,Loss:1.1363996267318726\n",
      "训练次数：4793,Loss:1.3490246534347534\n",
      "训练次数：4794,Loss:1.200443148612976\n",
      "训练次数：4795,Loss:1.3652126789093018\n",
      "训练次数：4796,Loss:1.4443622827529907\n",
      "训练次数：4797,Loss:1.5143463611602783\n",
      "训练次数：4798,Loss:1.2428945302963257\n",
      "训练次数：4799,Loss:1.3142833709716797\n",
      "训练次数：4800,Loss:1.5135478973388672\n",
      "训练次数：4801,Loss:1.2713727951049805\n",
      "训练次数：4802,Loss:1.1589211225509644\n",
      "训练次数：4803,Loss:1.3391327857971191\n",
      "训练次数：4804,Loss:1.2870367765426636\n",
      "训练次数：4805,Loss:1.2949856519699097\n",
      "训练次数：4806,Loss:1.1249366998672485\n",
      "训练次数：4807,Loss:1.2664728164672852\n",
      "训练次数：4808,Loss:1.4962161779403687\n",
      "训练次数：4809,Loss:1.2710909843444824\n",
      "训练次数：4810,Loss:1.2781858444213867\n",
      "训练次数：4811,Loss:1.5561890602111816\n",
      "训练次数：4812,Loss:1.3925048112869263\n",
      "训练次数：4813,Loss:1.417995572090149\n",
      "训练次数：4814,Loss:1.3376137018203735\n",
      "训练次数：4815,Loss:1.2590978145599365\n",
      "训练次数：4816,Loss:1.3749678134918213\n",
      "训练次数：4817,Loss:1.4668443202972412\n",
      "训练次数：4818,Loss:1.2692195177078247\n",
      "训练次数：4819,Loss:1.3634593486785889\n",
      "训练次数：4820,Loss:1.197393774986267\n",
      "训练次数：4821,Loss:1.371484398841858\n",
      "训练次数：4822,Loss:1.1932730674743652\n",
      "训练次数：4823,Loss:1.2484592199325562\n",
      "训练次数：4824,Loss:1.210949420928955\n",
      "训练次数：4825,Loss:1.5008536577224731\n",
      "训练次数：4826,Loss:1.0389735698699951\n",
      "训练次数：4827,Loss:1.2637265920639038\n",
      "训练次数：4828,Loss:1.305579662322998\n",
      "训练次数：4829,Loss:1.47444486618042\n",
      "训练次数：4830,Loss:1.4459320306777954\n",
      "训练次数：4831,Loss:1.129952311515808\n",
      "训练次数：4832,Loss:1.4844127893447876\n",
      "训练次数：4833,Loss:1.6678478717803955\n",
      "训练次数：4834,Loss:1.4745745658874512\n",
      "训练次数：4835,Loss:1.2726948261260986\n",
      "训练次数：4836,Loss:1.2516789436340332\n",
      "训练次数：4837,Loss:1.1854562759399414\n",
      "训练次数：4838,Loss:1.3476736545562744\n",
      "训练次数：4839,Loss:1.1657507419586182\n",
      "训练次数：4840,Loss:1.4400215148925781\n",
      "训练次数：4841,Loss:1.1772282123565674\n",
      "训练次数：4842,Loss:1.1455280780792236\n",
      "训练次数：4843,Loss:1.2982265949249268\n",
      "训练次数：4844,Loss:1.368939995765686\n",
      "训练次数：4845,Loss:1.351271390914917\n",
      "训练次数：4846,Loss:1.1149413585662842\n",
      "训练次数：4847,Loss:1.3648332357406616\n",
      "训练次数：4848,Loss:1.3868061304092407\n",
      "训练次数：4849,Loss:1.3175740242004395\n",
      "训练次数：4850,Loss:1.1428651809692383\n",
      "训练次数：4851,Loss:1.4394128322601318\n",
      "训练次数：4852,Loss:1.55266535282135\n",
      "训练次数：4853,Loss:1.354237675666809\n",
      "训练次数：4854,Loss:1.2224102020263672\n",
      "训练次数：4855,Loss:1.5582103729248047\n",
      "训练次数：4856,Loss:1.5228887796401978\n",
      "训练次数：4857,Loss:1.395911693572998\n",
      "训练次数：4858,Loss:1.714247226715088\n",
      "训练次数：4859,Loss:1.2951092720031738\n",
      "训练次数：4860,Loss:1.2645094394683838\n",
      "训练次数：4861,Loss:1.259737491607666\n",
      "训练次数：4862,Loss:1.2541561126708984\n",
      "训练次数：4863,Loss:1.4974048137664795\n",
      "训练次数：4864,Loss:1.364850640296936\n",
      "训练次数：4865,Loss:1.3665740489959717\n",
      "训练次数：4866,Loss:1.6803897619247437\n",
      "训练次数：4867,Loss:1.1720356941223145\n",
      "训练次数：4868,Loss:1.4231538772583008\n",
      "训练次数：4869,Loss:1.2454338073730469\n",
      "训练次数：4870,Loss:1.4171507358551025\n",
      "训练次数：4871,Loss:1.3760157823562622\n",
      "训练次数：4872,Loss:1.4358961582183838\n",
      "训练次数：4873,Loss:1.2131856679916382\n",
      "训练次数：4874,Loss:1.2209867238998413\n",
      "训练次数：4875,Loss:1.2463458776474\n",
      "训练次数：4876,Loss:1.32187020778656\n",
      "训练次数：4877,Loss:1.5768406391143799\n",
      "训练次数：4878,Loss:1.389958381652832\n",
      "训练次数：4879,Loss:1.341589331626892\n",
      "训练次数：4880,Loss:1.2804731130599976\n",
      "训练次数：4881,Loss:1.3718699216842651\n",
      "训练次数：4882,Loss:1.393762469291687\n",
      "训练次数：4883,Loss:1.1473027467727661\n",
      "训练次数：4884,Loss:1.4041575193405151\n",
      "训练次数：4885,Loss:1.6767864227294922\n",
      "训练次数：4886,Loss:1.2369645833969116\n",
      "训练次数：4887,Loss:1.299122929573059\n",
      "训练次数：4888,Loss:1.3976733684539795\n",
      "训练次数：4889,Loss:1.3516820669174194\n",
      "训练次数：4890,Loss:1.4863901138305664\n",
      "训练次数：4891,Loss:1.4688150882720947\n",
      "训练次数：4892,Loss:1.320845127105713\n",
      "训练次数：4893,Loss:1.1099061965942383\n",
      "训练次数：4894,Loss:1.5153932571411133\n",
      "训练次数：4895,Loss:1.3457914590835571\n",
      "训练次数：4896,Loss:1.378058910369873\n",
      "训练次数：4897,Loss:1.3736109733581543\n",
      "训练次数：4898,Loss:1.5055588483810425\n",
      "训练次数：4899,Loss:1.2761772871017456\n",
      "训练次数：4900,Loss:1.4418576955795288\n",
      "训练次数：4901,Loss:1.2621177434921265\n",
      "训练次数：4902,Loss:1.6389055252075195\n",
      "训练次数：4903,Loss:1.1702287197113037\n",
      "训练次数：4904,Loss:1.28364098072052\n",
      "训练次数：4905,Loss:1.2677273750305176\n",
      "训练次数：4906,Loss:1.41515052318573\n",
      "训练次数：4907,Loss:1.5518094301223755\n",
      "训练次数：4908,Loss:1.37871515750885\n",
      "训练次数：4909,Loss:1.2817387580871582\n",
      "训练次数：4910,Loss:1.2483806610107422\n",
      "训练次数：4911,Loss:1.3207013607025146\n",
      "训练次数：4912,Loss:1.298581600189209\n",
      "训练次数：4913,Loss:1.3066074848175049\n",
      "训练次数：4914,Loss:1.4094877243041992\n",
      "训练次数：4915,Loss:1.3671388626098633\n",
      "训练次数：4916,Loss:1.446044921875\n",
      "训练次数：4917,Loss:1.4163895845413208\n",
      "训练次数：4918,Loss:1.2970359325408936\n",
      "训练次数：4919,Loss:1.3548109531402588\n",
      "训练次数：4920,Loss:1.2061314582824707\n",
      "训练次数：4921,Loss:1.2631982564926147\n",
      "训练次数：4922,Loss:1.5071219205856323\n",
      "训练次数：4923,Loss:1.243324637413025\n",
      "训练次数：4924,Loss:1.3865970373153687\n",
      "训练次数：4925,Loss:1.3691765069961548\n",
      "训练次数：4926,Loss:1.470340609550476\n",
      "训练次数：4927,Loss:1.4528687000274658\n",
      "训练次数：4928,Loss:1.4861077070236206\n",
      "训练次数：4929,Loss:1.5426000356674194\n",
      "训练次数：4930,Loss:1.3458787202835083\n",
      "训练次数：4931,Loss:1.2042347192764282\n",
      "训练次数：4932,Loss:1.225932240486145\n",
      "训练次数：4933,Loss:1.397387981414795\n",
      "训练次数：4934,Loss:1.35014808177948\n",
      "训练次数：4935,Loss:1.259703278541565\n",
      "训练次数：4936,Loss:1.6977790594100952\n",
      "训练次数：4937,Loss:1.293404459953308\n",
      "训练次数：4938,Loss:1.5222150087356567\n",
      "训练次数：4939,Loss:1.2645026445388794\n",
      "训练次数：4940,Loss:1.4217543601989746\n",
      "训练次数：4941,Loss:1.5079752206802368\n",
      "训练次数：4942,Loss:1.5166832208633423\n",
      "训练次数：4943,Loss:1.3274383544921875\n",
      "训练次数：4944,Loss:1.3492789268493652\n",
      "训练次数：4945,Loss:1.358361840248108\n",
      "训练次数：4946,Loss:1.3330638408660889\n",
      "训练次数：4947,Loss:1.3447126150131226\n",
      "训练次数：4948,Loss:1.2509853839874268\n",
      "训练次数：4949,Loss:1.3666248321533203\n",
      "训练次数：4950,Loss:1.1488258838653564\n",
      "训练次数：4951,Loss:1.478434443473816\n",
      "训练次数：4952,Loss:1.341005802154541\n",
      "训练次数：4953,Loss:1.2604554891586304\n",
      "训练次数：4954,Loss:1.3295977115631104\n",
      "训练次数：4955,Loss:1.3157477378845215\n",
      "训练次数：4956,Loss:1.479030728340149\n",
      "训练次数：4957,Loss:1.0454866886138916\n",
      "训练次数：4958,Loss:1.1817001104354858\n",
      "训练次数：4959,Loss:1.3369226455688477\n",
      "训练次数：4960,Loss:1.3599289655685425\n",
      "训练次数：4961,Loss:1.1855512857437134\n",
      "训练次数：4962,Loss:1.3998867273330688\n",
      "训练次数：4963,Loss:1.3109142780303955\n",
      "训练次数：4964,Loss:1.2702765464782715\n",
      "训练次数：4965,Loss:1.2407745122909546\n",
      "训练次数：4966,Loss:1.2034764289855957\n",
      "训练次数：4967,Loss:1.2411377429962158\n",
      "训练次数：4968,Loss:1.2327659130096436\n",
      "训练次数：4969,Loss:1.6456116437911987\n",
      "训练次数：4970,Loss:1.3808107376098633\n",
      "训练次数：4971,Loss:1.453273057937622\n",
      "训练次数：4972,Loss:1.091399908065796\n",
      "训练次数：4973,Loss:1.2660536766052246\n",
      "训练次数：4974,Loss:1.413968801498413\n",
      "训练次数：4975,Loss:1.180241584777832\n",
      "训练次数：4976,Loss:1.4657772779464722\n",
      "训练次数：4977,Loss:1.2831790447235107\n",
      "训练次数：4978,Loss:1.3021544218063354\n",
      "训练次数：4979,Loss:1.3636624813079834\n",
      "训练次数：4980,Loss:1.3802435398101807\n",
      "训练次数：4981,Loss:1.5152101516723633\n",
      "训练次数：4982,Loss:1.525092363357544\n",
      "训练次数：4983,Loss:1.3145878314971924\n",
      "训练次数：4984,Loss:1.3437200784683228\n",
      "训练次数：4985,Loss:1.2729381322860718\n",
      "训练次数：4986,Loss:1.1999775171279907\n",
      "训练次数：4987,Loss:1.2394685745239258\n",
      "训练次数：4988,Loss:1.2825604677200317\n",
      "训练次数：4989,Loss:1.5198707580566406\n",
      "训练次数：4990,Loss:1.2680587768554688\n",
      "训练次数：4991,Loss:1.2150431871414185\n",
      "训练次数：4992,Loss:1.4171675443649292\n",
      "训练次数：4993,Loss:1.2435204982757568\n",
      "训练次数：4994,Loss:1.2423797845840454\n",
      "训练次数：4995,Loss:1.3081897497177124\n",
      "训练次数：4996,Loss:1.23361074924469\n",
      "训练次数：4997,Loss:1.1930961608886719\n",
      "训练次数：4998,Loss:1.5547361373901367\n",
      "训练次数：4999,Loss:1.2963333129882812\n",
      "训练次数：5000,Loss:1.437731385231018\n",
      "训练次数：5001,Loss:1.4416354894638062\n",
      "训练次数：5002,Loss:1.2833843231201172\n",
      "训练次数：5003,Loss:1.237629771232605\n",
      "训练次数：5004,Loss:1.3854538202285767\n",
      "训练次数：5005,Loss:1.3963462114334106\n",
      "训练次数：5006,Loss:1.3473014831542969\n",
      "训练次数：5007,Loss:1.236081600189209\n",
      "训练次数：5008,Loss:1.4209010601043701\n",
      "训练次数：5009,Loss:1.5006450414657593\n",
      "训练次数：5010,Loss:1.4590257406234741\n",
      "训练次数：5011,Loss:1.2347657680511475\n",
      "训练次数：5012,Loss:1.329738736152649\n",
      "训练次数：5013,Loss:1.4184801578521729\n",
      "训练次数：5014,Loss:1.4372494220733643\n",
      "训练次数：5015,Loss:1.3185232877731323\n",
      "训练次数：5016,Loss:1.4400355815887451\n",
      "训练次数：5017,Loss:1.3225280046463013\n",
      "训练次数：5018,Loss:1.3534882068634033\n",
      "训练次数：5019,Loss:1.1603726148605347\n",
      "训练次数：5020,Loss:1.3050589561462402\n",
      "训练次数：5021,Loss:1.3370084762573242\n",
      "训练次数：5022,Loss:1.3325731754302979\n",
      "训练次数：5023,Loss:1.2599083185195923\n",
      "训练次数：5024,Loss:1.2747694253921509\n",
      "训练次数：5025,Loss:1.3275517225265503\n",
      "训练次数：5026,Loss:1.315362572669983\n",
      "训练次数：5027,Loss:1.4505181312561035\n",
      "训练次数：5028,Loss:1.4154808521270752\n",
      "训练次数：5029,Loss:1.3625580072402954\n",
      "训练次数：5030,Loss:1.0801221132278442\n",
      "训练次数：5031,Loss:1.3064552545547485\n",
      "训练次数：5032,Loss:1.313490629196167\n",
      "训练次数：5033,Loss:1.5978244543075562\n",
      "训练次数：5034,Loss:1.3875846862792969\n",
      "训练次数：5035,Loss:1.2262216806411743\n",
      "训练次数：5036,Loss:1.2474446296691895\n",
      "训练次数：5037,Loss:1.3377913236618042\n",
      "训练次数：5038,Loss:1.101644515991211\n",
      "训练次数：5039,Loss:1.1897464990615845\n",
      "训练次数：5040,Loss:1.3780169486999512\n",
      "训练次数：5041,Loss:1.2928189039230347\n",
      "训练次数：5042,Loss:1.2374407052993774\n",
      "训练次数：5043,Loss:1.3524713516235352\n",
      "训练次数：5044,Loss:1.4363951683044434\n",
      "训练次数：5045,Loss:1.195349097251892\n",
      "训练次数：5046,Loss:1.4026005268096924\n",
      "训练次数：5047,Loss:1.3401148319244385\n",
      "训练次数：5048,Loss:1.3153736591339111\n",
      "训练次数：5049,Loss:1.3213942050933838\n",
      "训练次数：5050,Loss:1.3975826501846313\n",
      "训练次数：5051,Loss:1.1825473308563232\n",
      "训练次数：5052,Loss:1.505781888961792\n",
      "训练次数：5053,Loss:1.3588674068450928\n",
      "训练次数：5054,Loss:1.0146064758300781\n",
      "训练次数：5055,Loss:1.3897737264633179\n",
      "训练次数：5056,Loss:1.2502532005310059\n",
      "训练次数：5057,Loss:1.0013748407363892\n",
      "训练次数：5058,Loss:1.1670446395874023\n",
      "训练次数：5059,Loss:1.4383395910263062\n",
      "训练次数：5060,Loss:1.4823366403579712\n",
      "训练次数：5061,Loss:1.2332030534744263\n",
      "训练次数：5062,Loss:1.291266679763794\n",
      "训练次数：5063,Loss:0.9754054546356201\n",
      "训练次数：5064,Loss:1.4112756252288818\n",
      "训练次数：5065,Loss:1.1903892755508423\n",
      "训练次数：5066,Loss:1.2723324298858643\n",
      "训练次数：5067,Loss:1.4833812713623047\n",
      "训练次数：5068,Loss:1.2312883138656616\n",
      "训练次数：5069,Loss:1.220404863357544\n",
      "训练次数：5070,Loss:1.3178986310958862\n",
      "训练次数：5071,Loss:1.2231132984161377\n",
      "训练次数：5072,Loss:1.4841718673706055\n",
      "训练次数：5073,Loss:1.4319733381271362\n",
      "训练次数：5074,Loss:1.332741141319275\n",
      "训练次数：5075,Loss:1.2688758373260498\n",
      "训练次数：5076,Loss:1.3061670064926147\n",
      "训练次数：5077,Loss:1.212301254272461\n",
      "训练次数：5078,Loss:1.2502193450927734\n",
      "训练次数：5079,Loss:1.2790273427963257\n",
      "训练次数：5080,Loss:1.4691953659057617\n",
      "训练次数：5081,Loss:1.3632211685180664\n",
      "训练次数：5082,Loss:1.1830052137374878\n",
      "训练次数：5083,Loss:1.233763337135315\n",
      "训练次数：5084,Loss:0.9922785758972168\n",
      "训练次数：5085,Loss:1.3248461484909058\n",
      "训练次数：5086,Loss:1.5199722051620483\n",
      "训练次数：5087,Loss:1.4263923168182373\n",
      "训练次数：5088,Loss:1.2457996606826782\n",
      "训练次数：5089,Loss:1.2731884717941284\n",
      "训练次数：5090,Loss:1.5014445781707764\n",
      "训练次数：5091,Loss:1.1482890844345093\n",
      "训练次数：5092,Loss:1.2600123882293701\n",
      "训练次数：5093,Loss:1.2220772504806519\n",
      "训练次数：5094,Loss:1.5242149829864502\n",
      "训练次数：5095,Loss:1.3327528238296509\n",
      "训练次数：5096,Loss:1.1606128215789795\n",
      "训练次数：5097,Loss:1.3820643424987793\n",
      "训练次数：5098,Loss:1.1836366653442383\n",
      "训练次数：5099,Loss:1.2627907991409302\n",
      "训练次数：5100,Loss:0.98564612865448\n",
      "训练次数：5101,Loss:1.2625889778137207\n",
      "训练次数：5102,Loss:1.1078529357910156\n",
      "训练次数：5103,Loss:1.5640058517456055\n",
      "训练次数：5104,Loss:1.2405518293380737\n",
      "训练次数：5105,Loss:1.4118412733078003\n",
      "训练次数：5106,Loss:1.3056410551071167\n",
      "训练次数：5107,Loss:1.3914326429367065\n",
      "训练次数：5108,Loss:1.2172712087631226\n",
      "训练次数：5109,Loss:1.494663119316101\n",
      "训练次数：5110,Loss:1.2496819496154785\n",
      "训练次数：5111,Loss:1.3595516681671143\n",
      "训练次数：5112,Loss:1.3808374404907227\n",
      "训练次数：5113,Loss:1.4074608087539673\n",
      "训练次数：5114,Loss:1.2361944913864136\n",
      "训练次数：5115,Loss:1.4380680322647095\n",
      "训练次数：5116,Loss:1.2271326780319214\n",
      "训练次数：5117,Loss:1.2875514030456543\n",
      "训练次数：5118,Loss:1.4084999561309814\n",
      "训练次数：5119,Loss:1.2744230031967163\n",
      "训练次数：5120,Loss:1.1572129726409912\n",
      "训练次数：5121,Loss:1.3387372493743896\n",
      "训练次数：5122,Loss:1.2553956508636475\n",
      "训练次数：5123,Loss:1.385178804397583\n",
      "训练次数：5124,Loss:1.217140793800354\n",
      "训练次数：5125,Loss:1.4072626829147339\n",
      "训练次数：5126,Loss:1.261527419090271\n",
      "训练次数：5127,Loss:1.306660532951355\n",
      "训练次数：5128,Loss:1.6572378873825073\n",
      "训练次数：5129,Loss:1.452349066734314\n",
      "训练次数：5130,Loss:1.3266971111297607\n",
      "训练次数：5131,Loss:1.2766685485839844\n",
      "训练次数：5132,Loss:1.282906174659729\n",
      "训练次数：5133,Loss:1.2858197689056396\n",
      "训练次数：5134,Loss:1.143261432647705\n",
      "训练次数：5135,Loss:1.3906307220458984\n",
      "训练次数：5136,Loss:1.1874369382858276\n",
      "训练次数：5137,Loss:1.4418946504592896\n",
      "训练次数：5138,Loss:1.333441138267517\n",
      "训练次数：5139,Loss:1.3734045028686523\n",
      "训练次数：5140,Loss:1.2513861656188965\n",
      "训练次数：5141,Loss:1.3456488847732544\n",
      "训练次数：5142,Loss:1.3823225498199463\n",
      "训练次数：5143,Loss:1.1436642408370972\n",
      "训练次数：5144,Loss:1.0671237707138062\n",
      "训练次数：5145,Loss:1.1455427408218384\n",
      "训练次数：5146,Loss:1.2672371864318848\n",
      "训练次数：5147,Loss:1.3646492958068848\n",
      "训练次数：5148,Loss:1.45742666721344\n",
      "训练次数：5149,Loss:1.4771406650543213\n",
      "训练次数：5150,Loss:1.1118091344833374\n",
      "训练次数：5151,Loss:1.0646816492080688\n",
      "训练次数：5152,Loss:1.23814857006073\n",
      "训练次数：5153,Loss:1.3445883989334106\n",
      "训练次数：5154,Loss:1.3994992971420288\n",
      "训练次数：5155,Loss:1.1986048221588135\n",
      "训练次数：5156,Loss:1.3937705755233765\n",
      "训练次数：5157,Loss:1.2582920789718628\n",
      "训练次数：5158,Loss:1.118984341621399\n",
      "训练次数：5159,Loss:1.4503573179244995\n",
      "训练次数：5160,Loss:1.2716423273086548\n",
      "训练次数：5161,Loss:1.127866268157959\n",
      "训练次数：5162,Loss:1.36219322681427\n",
      "训练次数：5163,Loss:1.3508747816085815\n",
      "训练次数：5164,Loss:1.4027650356292725\n",
      "训练次数：5165,Loss:1.2160310745239258\n",
      "训练次数：5166,Loss:1.411641240119934\n",
      "训练次数：5167,Loss:1.0396437644958496\n",
      "训练次数：5168,Loss:1.413404107093811\n",
      "训练次数：5169,Loss:1.267159342765808\n",
      "训练次数：5170,Loss:1.254960060119629\n",
      "训练次数：5171,Loss:1.3268461227416992\n",
      "训练次数：5172,Loss:1.4327759742736816\n",
      "训练次数：5173,Loss:1.4782829284667969\n",
      "训练次数：5174,Loss:1.4641813039779663\n",
      "训练次数：5175,Loss:1.3193049430847168\n",
      "训练次数：5176,Loss:1.0786739587783813\n",
      "训练次数：5177,Loss:1.220255970954895\n",
      "训练次数：5178,Loss:1.0445786714553833\n",
      "训练次数：5179,Loss:1.2743116617202759\n",
      "训练次数：5180,Loss:1.480049967765808\n",
      "训练次数：5181,Loss:1.2739189863204956\n",
      "训练次数：5182,Loss:1.0423845052719116\n",
      "训练次数：5183,Loss:1.6349133253097534\n",
      "训练次数：5184,Loss:1.3893910646438599\n",
      "训练次数：5185,Loss:1.3242383003234863\n",
      "训练次数：5186,Loss:1.173735499382019\n",
      "训练次数：5187,Loss:1.4183313846588135\n",
      "训练次数：5188,Loss:1.5016584396362305\n",
      "训练次数：5189,Loss:1.29177725315094\n",
      "训练次数：5190,Loss:1.4005428552627563\n",
      "训练次数：5191,Loss:1.5717262029647827\n",
      "训练次数：5192,Loss:1.2632713317871094\n",
      "训练次数：5193,Loss:1.4934381246566772\n",
      "训练次数：5194,Loss:1.2736395597457886\n",
      "训练次数：5195,Loss:1.1916583776474\n",
      "训练次数：5196,Loss:1.4369784593582153\n",
      "训练次数：5197,Loss:1.5113130807876587\n",
      "训练次数：5198,Loss:1.3846033811569214\n",
      "训练次数：5199,Loss:1.2449450492858887\n",
      "训练次数：5200,Loss:1.2955763339996338\n",
      "训练次数：5201,Loss:1.3347954750061035\n",
      "训练次数：5202,Loss:1.40653395652771\n",
      "训练次数：5203,Loss:1.3358923196792603\n",
      "训练次数：5204,Loss:1.3176145553588867\n",
      "训练次数：5205,Loss:1.392168402671814\n",
      "训练次数：5206,Loss:1.4394493103027344\n",
      "训练次数：5207,Loss:1.5415935516357422\n",
      "训练次数：5208,Loss:1.2936229705810547\n",
      "训练次数：5209,Loss:1.5836046934127808\n",
      "训练次数：5210,Loss:1.1650094985961914\n",
      "训练次数：5211,Loss:1.2700620889663696\n",
      "训练次数：5212,Loss:1.2767342329025269\n",
      "训练次数：5213,Loss:1.5197789669036865\n",
      "训练次数：5214,Loss:1.4029792547225952\n",
      "训练次数：5215,Loss:1.274722695350647\n",
      "训练次数：5216,Loss:1.5186514854431152\n",
      "训练次数：5217,Loss:1.368565320968628\n",
      "训练次数：5218,Loss:1.5804154872894287\n",
      "训练次数：5219,Loss:1.414986252784729\n",
      "训练次数：5220,Loss:1.1404435634613037\n",
      "训练次数：5221,Loss:1.3647897243499756\n",
      "训练次数：5222,Loss:1.510143756866455\n",
      "训练次数：5223,Loss:1.311561942100525\n",
      "训练次数：5224,Loss:1.4041153192520142\n",
      "训练次数：5225,Loss:1.2283869981765747\n",
      "训练次数：5226,Loss:1.136062502861023\n",
      "训练次数：5227,Loss:1.3042569160461426\n",
      "训练次数：5228,Loss:1.1104350090026855\n",
      "训练次数：5229,Loss:1.3689000606536865\n",
      "训练次数：5230,Loss:1.3975027799606323\n",
      "训练次数：5231,Loss:1.3033320903778076\n",
      "训练次数：5232,Loss:1.2833534479141235\n",
      "训练次数：5233,Loss:1.3463329076766968\n",
      "训练次数：5234,Loss:1.3791173696517944\n",
      "训练次数：5235,Loss:1.2600542306900024\n",
      "训练次数：5236,Loss:1.2447459697723389\n",
      "训练次数：5237,Loss:1.2475999593734741\n",
      "训练次数：5238,Loss:1.2589242458343506\n",
      "训练次数：5239,Loss:1.3726022243499756\n",
      "训练次数：5240,Loss:1.3562625646591187\n",
      "训练次数：5241,Loss:1.2831827402114868\n",
      "训练次数：5242,Loss:1.456146240234375\n",
      "训练次数：5243,Loss:1.3049201965332031\n",
      "训练次数：5244,Loss:1.6418941020965576\n",
      "训练次数：5245,Loss:1.3011394739151\n",
      "训练次数：5246,Loss:1.4233293533325195\n",
      "训练次数：5247,Loss:1.1151143312454224\n",
      "训练次数：5248,Loss:1.2376035451889038\n",
      "训练次数：5249,Loss:1.4574332237243652\n",
      "训练次数：5250,Loss:1.3492350578308105\n",
      "训练次数：5251,Loss:1.2105070352554321\n",
      "训练次数：5252,Loss:1.405084490776062\n",
      "训练次数：5253,Loss:1.2764434814453125\n",
      "训练次数：5254,Loss:1.3259953260421753\n",
      "训练次数：5255,Loss:1.4227509498596191\n",
      "训练次数：5256,Loss:1.2264569997787476\n",
      "训练次数：5257,Loss:1.3665778636932373\n",
      "训练次数：5258,Loss:1.4507982730865479\n",
      "训练次数：5259,Loss:1.2864395380020142\n",
      "训练次数：5260,Loss:1.3524742126464844\n",
      "训练次数：5261,Loss:1.340852975845337\n",
      "训练次数：5262,Loss:1.1550700664520264\n",
      "训练次数：5263,Loss:1.204352617263794\n",
      "训练次数：5264,Loss:1.213340163230896\n",
      "训练次数：5265,Loss:1.1994556188583374\n",
      "训练次数：5266,Loss:1.4399751424789429\n",
      "训练次数：5267,Loss:1.2380839586257935\n",
      "训练次数：5268,Loss:1.2857290506362915\n",
      "训练次数：5269,Loss:1.3571165800094604\n",
      "训练次数：5270,Loss:1.7694602012634277\n",
      "训练次数：5271,Loss:1.1878279447555542\n",
      "训练次数：5272,Loss:1.3265626430511475\n",
      "训练次数：5273,Loss:1.285857081413269\n",
      "训练次数：5274,Loss:1.367353916168213\n",
      "训练次数：5275,Loss:1.1646896600723267\n",
      "训练次数：5276,Loss:1.4027191400527954\n",
      "训练次数：5277,Loss:1.2963206768035889\n",
      "训练次数：5278,Loss:1.2330796718597412\n",
      "训练次数：5279,Loss:1.4573171138763428\n",
      "训练次数：5280,Loss:1.1423265933990479\n",
      "训练次数：5281,Loss:1.5299557447433472\n",
      "训练次数：5282,Loss:1.3097840547561646\n",
      "训练次数：5283,Loss:1.3661961555480957\n",
      "训练次数：5284,Loss:1.1094346046447754\n",
      "训练次数：5285,Loss:1.4499554634094238\n",
      "训练次数：5286,Loss:1.4078222513198853\n",
      "训练次数：5287,Loss:1.3643503189086914\n",
      "训练次数：5288,Loss:1.5781581401824951\n",
      "训练次数：5289,Loss:1.3573342561721802\n",
      "训练次数：5290,Loss:1.3231981992721558\n",
      "训练次数：5291,Loss:1.4278950691223145\n",
      "训练次数：5292,Loss:1.374468207359314\n",
      "训练次数：5293,Loss:1.427113652229309\n",
      "训练次数：5294,Loss:1.147589087486267\n",
      "训练次数：5295,Loss:1.050650715827942\n",
      "训练次数：5296,Loss:1.5983190536499023\n",
      "训练次数：5297,Loss:1.4610487222671509\n",
      "训练次数：5298,Loss:1.052335500717163\n",
      "训练次数：5299,Loss:0.9725466966629028\n",
      "训练次数：5300,Loss:1.2274329662322998\n",
      "训练次数：5301,Loss:1.2167903184890747\n",
      "训练次数：5302,Loss:1.4206229448318481\n",
      "训练次数：5303,Loss:1.2114737033843994\n",
      "训练次数：5304,Loss:1.3591474294662476\n",
      "训练次数：5305,Loss:1.40180242061615\n",
      "训练次数：5306,Loss:1.1650071144104004\n",
      "训练次数：5307,Loss:1.2501336336135864\n",
      "训练次数：5308,Loss:1.2639743089675903\n",
      "训练次数：5309,Loss:1.3198071718215942\n",
      "训练次数：5310,Loss:1.2115073204040527\n",
      "训练次数：5311,Loss:1.4100950956344604\n",
      "训练次数：5312,Loss:1.5562543869018555\n",
      "训练次数：5313,Loss:1.3418030738830566\n",
      "训练次数：5314,Loss:1.463646411895752\n",
      "训练次数：5315,Loss:1.2748684883117676\n",
      "训练次数：5316,Loss:1.1596744060516357\n",
      "训练次数：5317,Loss:1.2328157424926758\n",
      "训练次数：5318,Loss:1.1023632287979126\n",
      "训练次数：5319,Loss:1.1695752143859863\n",
      "训练次数：5320,Loss:1.6260218620300293\n",
      "训练次数：5321,Loss:1.4376238584518433\n",
      "训练次数：5322,Loss:1.1536800861358643\n",
      "训练次数：5323,Loss:1.2605135440826416\n",
      "训练次数：5324,Loss:1.451143741607666\n",
      "训练次数：5325,Loss:1.38274347782135\n",
      "训练次数：5326,Loss:1.0772687196731567\n",
      "训练次数：5327,Loss:1.3393878936767578\n",
      "训练次数：5328,Loss:1.0946160554885864\n",
      "训练次数：5329,Loss:1.2815613746643066\n",
      "训练次数：5330,Loss:1.340950846672058\n",
      "训练次数：5331,Loss:1.3412238359451294\n",
      "训练次数：5332,Loss:1.5169110298156738\n",
      "训练次数：5333,Loss:1.3820669651031494\n",
      "训练次数：5334,Loss:1.3269765377044678\n",
      "训练次数：5335,Loss:1.3916696310043335\n",
      "训练次数：5336,Loss:1.2315394878387451\n",
      "训练次数：5337,Loss:1.248110055923462\n",
      "训练次数：5338,Loss:1.5149977207183838\n",
      "训练次数：5339,Loss:1.2554261684417725\n",
      "训练次数：5340,Loss:1.1748677492141724\n",
      "训练次数：5341,Loss:1.3278228044509888\n",
      "训练次数：5342,Loss:1.5896023511886597\n",
      "训练次数：5343,Loss:1.2662378549575806\n",
      "训练次数：5344,Loss:1.3682941198349\n",
      "训练次数：5345,Loss:1.09415864944458\n",
      "训练次数：5346,Loss:1.1944332122802734\n",
      "训练次数：5347,Loss:1.3631248474121094\n",
      "训练次数：5348,Loss:1.2772291898727417\n",
      "训练次数：5349,Loss:1.3182687759399414\n",
      "训练次数：5350,Loss:1.2692668437957764\n",
      "训练次数：5351,Loss:1.5500580072402954\n",
      "训练次数：5352,Loss:1.2890985012054443\n",
      "训练次数：5353,Loss:1.1821022033691406\n",
      "训练次数：5354,Loss:1.0457282066345215\n",
      "训练次数：5355,Loss:1.138427734375\n",
      "训练次数：5356,Loss:1.2632577419281006\n",
      "训练次数：5357,Loss:1.484609603881836\n",
      "训练次数：5358,Loss:1.599397897720337\n",
      "训练次数：5359,Loss:1.1384268999099731\n",
      "训练次数：5360,Loss:1.4423038959503174\n",
      "训练次数：5361,Loss:1.2827807664871216\n",
      "训练次数：5362,Loss:1.2804909944534302\n",
      "训练次数：5363,Loss:1.3235948085784912\n",
      "训练次数：5364,Loss:1.2119001150131226\n",
      "训练次数：5365,Loss:1.251045823097229\n",
      "训练次数：5366,Loss:1.2762510776519775\n",
      "训练次数：5367,Loss:1.3204362392425537\n",
      "训练次数：5368,Loss:1.526910662651062\n",
      "训练次数：5369,Loss:1.454673171043396\n",
      "训练次数：5370,Loss:1.1787278652191162\n",
      "训练次数：5371,Loss:1.3218973875045776\n",
      "训练次数：5372,Loss:1.4545625448226929\n",
      "训练次数：5373,Loss:1.4290882349014282\n",
      "训练次数：5374,Loss:1.0422173738479614\n",
      "训练次数：5375,Loss:1.1309936046600342\n",
      "训练次数：5376,Loss:1.2363258600234985\n",
      "训练次数：5377,Loss:1.3411861658096313\n",
      "训练次数：5378,Loss:1.5979268550872803\n",
      "训练次数：5379,Loss:1.2676913738250732\n",
      "训练次数：5380,Loss:1.1887056827545166\n",
      "训练次数：5381,Loss:1.318036437034607\n",
      "训练次数：5382,Loss:1.3543727397918701\n",
      "训练次数：5383,Loss:1.3590279817581177\n",
      "训练次数：5384,Loss:1.3733915090560913\n",
      "训练次数：5385,Loss:1.5038301944732666\n",
      "训练次数：5386,Loss:1.3991765975952148\n",
      "训练次数：5387,Loss:1.337023138999939\n",
      "训练次数：5388,Loss:1.2994104623794556\n",
      "训练次数：5389,Loss:1.3202781677246094\n",
      "训练次数：5390,Loss:1.3385249376296997\n",
      "训练次数：5391,Loss:1.0945589542388916\n",
      "训练次数：5392,Loss:1.4936621189117432\n",
      "训练次数：5393,Loss:1.4307183027267456\n",
      "训练次数：5394,Loss:1.3653148412704468\n",
      "训练次数：5395,Loss:1.3093339204788208\n",
      "训练次数：5396,Loss:1.0662720203399658\n",
      "训练次数：5397,Loss:1.3257555961608887\n",
      "训练次数：5398,Loss:1.3074246644973755\n",
      "训练次数：5399,Loss:1.5281529426574707\n",
      "训练次数：5400,Loss:1.3910908699035645\n",
      "训练次数：5401,Loss:1.096776008605957\n",
      "训练次数：5402,Loss:1.0563243627548218\n",
      "训练次数：5403,Loss:1.388576865196228\n",
      "训练次数：5404,Loss:1.0437686443328857\n",
      "训练次数：5405,Loss:1.2258100509643555\n",
      "训练次数：5406,Loss:1.254295825958252\n",
      "训练次数：5407,Loss:1.2776901721954346\n",
      "训练次数：5408,Loss:1.2187281847000122\n",
      "训练次数：5409,Loss:1.1341564655303955\n",
      "训练次数：5410,Loss:1.2245196104049683\n",
      "训练次数：5411,Loss:1.2988601922988892\n",
      "训练次数：5412,Loss:1.2540148496627808\n",
      "训练次数：5413,Loss:1.2529504299163818\n",
      "训练次数：5414,Loss:1.2678438425064087\n",
      "训练次数：5415,Loss:1.1296626329421997\n",
      "训练次数：5416,Loss:1.5114316940307617\n",
      "训练次数：5417,Loss:1.3360568284988403\n",
      "训练次数：5418,Loss:1.1896032094955444\n",
      "训练次数：5419,Loss:1.4763396978378296\n",
      "训练次数：5420,Loss:1.536033272743225\n",
      "训练次数：5421,Loss:1.2203623056411743\n",
      "训练次数：5422,Loss:1.3189016580581665\n",
      "训练次数：5423,Loss:1.1189525127410889\n",
      "训练次数：5424,Loss:1.1232601404190063\n",
      "训练次数：5425,Loss:1.2180511951446533\n",
      "训练次数：5426,Loss:1.224151372909546\n",
      "训练次数：5427,Loss:1.3327529430389404\n",
      "训练次数：5428,Loss:1.3636707067489624\n",
      "训练次数：5429,Loss:1.1494154930114746\n",
      "训练次数：5430,Loss:1.3444486856460571\n",
      "训练次数：5431,Loss:1.1424795389175415\n",
      "训练次数：5432,Loss:1.073777437210083\n",
      "训练次数：5433,Loss:1.3679425716400146\n",
      "训练次数：5434,Loss:1.3389389514923096\n",
      "训练次数：5435,Loss:1.6611794233322144\n",
      "训练次数：5436,Loss:1.1360645294189453\n",
      "训练次数：5437,Loss:1.1860359907150269\n",
      "训练次数：5438,Loss:1.2254619598388672\n",
      "训练次数：5439,Loss:1.1937471628189087\n",
      "训练次数：5440,Loss:1.5472126007080078\n",
      "训练次数：5441,Loss:1.3954652547836304\n",
      "训练次数：5442,Loss:1.1598725318908691\n",
      "训练次数：5443,Loss:1.7119042873382568\n",
      "训练次数：5444,Loss:1.2491952180862427\n",
      "训练次数：5445,Loss:1.222522497177124\n",
      "训练次数：5446,Loss:1.353136420249939\n",
      "训练次数：5447,Loss:1.2936054468154907\n",
      "训练次数：5448,Loss:1.2304749488830566\n",
      "训练次数：5449,Loss:1.191962718963623\n",
      "训练次数：5450,Loss:1.323918104171753\n",
      "训练次数：5451,Loss:1.4831252098083496\n",
      "训练次数：5452,Loss:1.178376317024231\n",
      "训练次数：5453,Loss:1.199686884880066\n",
      "训练次数：5454,Loss:1.4018473625183105\n",
      "训练次数：5455,Loss:1.2503197193145752\n",
      "训练次数：5456,Loss:1.264335036277771\n",
      "训练次数：5457,Loss:1.1716095209121704\n",
      "训练次数：5458,Loss:1.390534520149231\n",
      "训练次数：5459,Loss:1.1749733686447144\n",
      "训练次数：5460,Loss:1.4207258224487305\n",
      "训练次数：5461,Loss:1.009458303451538\n",
      "训练次数：5462,Loss:1.1450598239898682\n",
      "训练次数：5463,Loss:1.3388077020645142\n",
      "训练次数：5464,Loss:1.3243738412857056\n",
      "训练次数：5465,Loss:1.342646837234497\n",
      "训练次数：5466,Loss:1.1499435901641846\n",
      "训练次数：5467,Loss:1.2855266332626343\n",
      "训练次数：5468,Loss:1.1530593633651733\n",
      "训练次数：5469,Loss:1.4577748775482178\n",
      "训练次数：5470,Loss:1.2526004314422607\n",
      "训练次数：5471,Loss:1.2146292924880981\n",
      "训练次数：5472,Loss:1.1553648710250854\n",
      "训练次数：5473,Loss:1.3521077632904053\n",
      "训练次数：5474,Loss:1.889897346496582\n",
      "-----第 8 轮训练开始-----\n",
      "训练次数：5475,Loss:1.4908546209335327\n",
      "训练次数：5476,Loss:1.3191211223602295\n",
      "训练次数：5477,Loss:1.440380573272705\n",
      "训练次数：5478,Loss:0.9856055974960327\n",
      "训练次数：5479,Loss:1.306096076965332\n",
      "训练次数：5480,Loss:1.288633942604065\n",
      "训练次数：5481,Loss:1.3268463611602783\n",
      "训练次数：5482,Loss:1.2296706438064575\n",
      "训练次数：5483,Loss:1.1895180940628052\n",
      "训练次数：5484,Loss:1.472410798072815\n",
      "训练次数：5485,Loss:1.384511113166809\n",
      "训练次数：5486,Loss:1.379950761795044\n",
      "训练次数：5487,Loss:1.380928874015808\n",
      "训练次数：5488,Loss:1.5063470602035522\n",
      "训练次数：5489,Loss:1.3401453495025635\n",
      "训练次数：5490,Loss:1.0555261373519897\n",
      "训练次数：5491,Loss:1.2879400253295898\n",
      "训练次数：5492,Loss:1.1427364349365234\n",
      "训练次数：5493,Loss:1.2301082611083984\n",
      "训练次数：5494,Loss:1.1977959871292114\n",
      "训练次数：5495,Loss:1.4598127603530884\n",
      "训练次数：5496,Loss:1.2109955549240112\n",
      "训练次数：5497,Loss:1.3046846389770508\n",
      "训练次数：5498,Loss:1.3563874959945679\n",
      "训练次数：5499,Loss:1.3011201620101929\n",
      "训练次数：5500,Loss:1.1568354368209839\n",
      "训练次数：5501,Loss:1.251047968864441\n",
      "训练次数：5502,Loss:1.178061842918396\n",
      "训练次数：5503,Loss:1.1224415302276611\n",
      "训练次数：5504,Loss:1.1062407493591309\n",
      "训练次数：5505,Loss:1.4378856420516968\n",
      "训练次数：5506,Loss:1.4632441997528076\n",
      "训练次数：5507,Loss:1.1692078113555908\n",
      "训练次数：5508,Loss:1.096063256263733\n",
      "训练次数：5509,Loss:1.2812474966049194\n",
      "训练次数：5510,Loss:1.1436108350753784\n",
      "训练次数：5511,Loss:1.27542245388031\n",
      "训练次数：5512,Loss:1.3539718389511108\n",
      "训练次数：5513,Loss:1.154917597770691\n",
      "训练次数：5514,Loss:1.5850569009780884\n",
      "训练次数：5515,Loss:1.3660271167755127\n",
      "训练次数：5516,Loss:1.0058035850524902\n",
      "训练次数：5517,Loss:1.489770770072937\n",
      "训练次数：5518,Loss:1.451263189315796\n",
      "训练次数：5519,Loss:1.0658031702041626\n",
      "训练次数：5520,Loss:1.1807231903076172\n",
      "训练次数：5521,Loss:1.395113468170166\n",
      "训练次数：5522,Loss:1.4719724655151367\n",
      "训练次数：5523,Loss:1.2240614891052246\n",
      "训练次数：5524,Loss:1.4567596912384033\n",
      "训练次数：5525,Loss:1.225517988204956\n",
      "训练次数：5526,Loss:1.397515058517456\n",
      "训练次数：5527,Loss:1.3201947212219238\n",
      "训练次数：5528,Loss:1.3461827039718628\n",
      "训练次数：5529,Loss:1.214374303817749\n",
      "训练次数：5530,Loss:1.1511098146438599\n",
      "训练次数：5531,Loss:1.3520636558532715\n",
      "训练次数：5532,Loss:1.1225285530090332\n",
      "训练次数：5533,Loss:1.0913370847702026\n",
      "训练次数：5534,Loss:1.1921414136886597\n",
      "训练次数：5535,Loss:1.273032546043396\n",
      "训练次数：5536,Loss:1.4162050485610962\n",
      "训练次数：5537,Loss:1.346089482307434\n",
      "训练次数：5538,Loss:1.3373363018035889\n",
      "训练次数：5539,Loss:1.4175429344177246\n",
      "训练次数：5540,Loss:1.303406834602356\n",
      "训练次数：5541,Loss:1.1859887838363647\n",
      "训练次数：5542,Loss:1.3163044452667236\n",
      "训练次数：5543,Loss:1.0942569971084595\n",
      "训练次数：5544,Loss:1.212778091430664\n",
      "训练次数：5545,Loss:1.4372249841690063\n",
      "训练次数：5546,Loss:1.0734955072402954\n",
      "训练次数：5547,Loss:1.3087183237075806\n",
      "训练次数：5548,Loss:1.2215607166290283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     61\u001b[0m     imgs, targets \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 62\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtudui\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets) \u001b[38;5;66;03m# 计算实际输出与目标输出的差距\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# 优化器对模型调优\u001b[39;00m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mTudui.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:224\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_jit_internal.py:627\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:827\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    826\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # 展平后变成 64*4*4 了\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10('D:\\\\深度学习\\\\100_土堆数据集\\\\dataset',train=True,transform=torchvision.transforms.ToTensor(),download=True)       \n",
    "test_data = torchvision.datasets.CIFAR10('D:\\\\深度学习\\\\100_土堆数据集\\\\dataset',train=False,transform=torchvision.transforms.ToTensor(),download=True)       \n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "tudui = Tudui() \n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 1e-2 就是 0.01 的意思\n",
    "optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  \n",
    "\n",
    "# 设置网络的一些参数\n",
    "# 记录训练的次数\n",
    "total_train_step = 0\n",
    "\n",
    "# 训练的轮次\n",
    "epoch = 10\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f\"-----第 {i+1} 轮训练开始-----\")\n",
    "    \n",
    "    # 训练步骤开始\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = tudui(imgs)\n",
    "        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距\n",
    "        \n",
    "        # 优化器对模型调优\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward() # 反向传播，计算损失函数的梯度\n",
    "        optimizer.step()   # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        \n",
    "        print(f\"训练次数：{total_train_step},Loss:{loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. item作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ==========================================\n",
    "# 1. 创建一个标量张量 (Scalar Tensor)\n",
    "# ==========================================\n",
    "# 语法: torch.tensor(数值)\n",
    "# 作用: 创建一个 0 维度的张量，里面只包含一个数 5。\n",
    "# 注意: 这还是一个 PyTorch 的对象，不是普通的 Python 整数。\n",
    "a = torch.tensor(3)\n",
    "\n",
    "# ==========================================\n",
    "# 2. 打印张量本身\n",
    "# ==========================================\n",
    "# 输出: tensor(5)\n",
    "# 说明: 打印出来会带着 \"tensor(...)\" 的壳，表示它还是个张量类型。\n",
    "print(a)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 提取数值 (关键点)\n",
    "# ==========================================\n",
    "# 语法: .item()\n",
    "# 作用: 将张量中的数据提取出来，转换成 Python 原生的数字 (int 或 float)。\n",
    "# 限制: 只能用于包含\"唯一一个元素\"的张量！(如果张量里有多个数，用这个会报错)\n",
    "# 输出: 5\n",
    "print(a.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为什么需要 .item()？\n",
    "  - 精度更高：.item() 取出的是 Python 的高精度数字。\n",
    "  - 兼容性：很多 Python 库（如 matplotlib 画图、json 保存数据）不认识 PyTorch 的 Tensor 对象，只认识 Python 的原生数字。所以画图或保存时必须先用 .item() 转一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 查看训练损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在pytorch中，tensor有一个requires_grad参数，如果设置为True，则反向传播时，该tensor就会自动求导。\n",
    "\n",
    "2. tensor的requires_grad的属性默认为False，若一个节点（叶子变量：自己创建的tensor）requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True（即使其他相依赖的tensor的requires_grad = False）\n",
    "\n",
    "3. 当requires_grad设置为False时，反向传播时就不会自动求导了，因此大大节约了显存或者说内存。\n",
    "\n",
    "4. with torch.no_grad的作用在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。\n",
    "\n",
    "5. 即使一个tensor（命名为x）的requires_grad = True，在with torch.no_grad计算，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。\n",
    "\n",
    "6. torch.no_grad()：停止计算梯度，不能进行反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "训练次数：100,Loss:2.2851486206054688\n",
      "训练次数：200,Loss:2.2869608402252197\n",
      "训练次数：300,Loss:2.247696876525879\n",
      "训练次数：400,Loss:2.174241781234741\n",
      "训练次数：500,Loss:2.0526342391967773\n",
      "训练次数：600,Loss:2.035698175430298\n",
      "训练次数：700,Loss:1.966726541519165\n",
      "整体测试集上的Loss:308.77239990234375\n",
      "-----第 2 轮训练开始-----\n",
      "训练次数：800,Loss:1.8625493049621582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# 4. 反向传播: 计算 Loss 对每个参数的梯度 (寻找下山的路)\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# 5. 参数更新: 根据梯度调整参数 (迈出一步)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# =========================================================================\n",
    "# 1. 定义神经网络模型\n",
    "# =========================================================================\n",
    "# 继承 nn.Module，这是所有 PyTorch 神经网络的基类\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        # 使用 Sequential 将层串联起来，代码更简洁\n",
    "        self.model1 = nn.Sequential(\n",
    "            # --- 第1层卷积 ---\n",
    "            # 输入: 3通道 (RGB图片), 输出: 32通道, 核大小: 5x5\n",
    "            # Padding=2 计算: (32 - 5 + 2*2)/1 + 1 = 32 (尺寸不变)\n",
    "            nn.Conv2d(3, 32, 5, 1, 2), \n",
    "            \n",
    "            # --- 第1层池化 ---\n",
    "            # 尺寸减半: 32x32 -> 16x16\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 第2层卷积 ---\n",
    "            # 输入: 32通道, 输出: 32通道\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),\n",
    "            \n",
    "            # --- 第2层池化 ---\n",
    "            # 尺寸减半: 16x16 -> 8x8\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 第3层卷积 ---\n",
    "            # 输入: 32通道, 输出: 64通道 (特征数量增加)\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            \n",
    "            # --- 第3层池化 ---\n",
    "            # 尺寸减半: 8x8 -> 4x4\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 展平层 ---\n",
    "            # 将 (Batch, 64, 4, 4) 的立体数据拉平成 (Batch, 1024) 的一维向量\n",
    "            nn.Flatten(), \n",
    "            \n",
    "            # --- 全连接层 ---\n",
    "            # 64*4*4 = 1024 是上一层输出的总特征数\n",
    "            nn.Linear(64*4*4, 64),\n",
    "            \n",
    "            # --- 输出层 ---\n",
    "            # 10 代表 CIFAR10 数据集的 10 个分类\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 前向传播：数据 x 输入，经过 model1 处理，得到输出\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# =========================================================================\n",
    "# 2. 准备数据集\n",
    "# =========================================================================\n",
    "# train=True: 下载并加载训练集 (50000张)\n",
    "train_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=True, transform=torchvision.transforms.ToTensor())       \n",
    "# train=False: 下载并加载测试集 (10000张)\n",
    "test_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=False, transform=torchvision.transforms.ToTensor())       \n",
    "\n",
    "# 打印数据集长度，确认数据加载成功\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size)) # 输出 50000\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))   # 输出 10000\n",
    "\n",
    "# 利用 DataLoader 加载数据集\n",
    "# batch_size=64: 每次从仓库里拿 64 张图打包给模型\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# =========================================================================\n",
    "# 3. 创建模型、损失函数与优化器\n",
    "# =========================================================================\n",
    "# 实例化模型\n",
    "tudui = Tudui() \n",
    "\n",
    "# 损失函数: CrossEntropyLoss (交叉熵)\n",
    "# 专门用于分类任务，内部包含了 Softmax 和 Log 计算\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器: SGD (随机梯度下降)\n",
    "# learning_rate = 0.01，控制参数更新的步长\n",
    "learning = 0.01 \n",
    "optimizer = torch.optim.SGD(tudui.parameters(), lr=learning) \n",
    "\n",
    "# =========================================================================\n",
    "# 4. 设置训练流程参数\n",
    "# =========================================================================\n",
    "# 记录总的训练步数 (每训练一个 Batch 算一步)\n",
    "total_train_step = 0\n",
    "# 记录总的测试步数 (每测试完一轮算一步)\n",
    "total_test_step = 0\n",
    "# 训练轮数 (Epoch): 把整个数据集看 10 遍\n",
    "epoch = 10\n",
    "\n",
    "# =========================================================================\n",
    "# 5. 开始训练循环\n",
    "# =========================================================================\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # ---------------------------\n",
    "    # 训练步骤 (Training Loop)\n",
    "    # ---------------------------\n",
    "    # 每次从 dataloader 取出一批数据 (imgs: 64张图, targets: 64个标签)\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        \n",
    "        # 1. 前向传播: 模型通过图片预测结果\n",
    "        outputs = tudui(imgs)\n",
    "        \n",
    "        # 2. 计算损失: 比较预测结果 outputs 和真实标签 targets 的差距\n",
    "        loss = loss_fn(outputs, targets) \n",
    "        \n",
    "        # 3. 梯度清零: 把上一次计算的梯度清理掉 (避免累加)\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # 4. 反向传播: 计算 Loss 对每个参数的梯度 (寻找下山的路)\n",
    "        loss.backward() \n",
    "        \n",
    "        # 5. 参数更新: 根据梯度调整参数 (迈出一步)\n",
    "        optimizer.step() \n",
    "        \n",
    "        # 记录训练次数\n",
    "        total_train_step = total_train_step + 1\n",
    "        \n",
    "        # 每隔 100 次打印一下 Log，避免控制台刷屏太快\n",
    "        if total_train_step % 100 == 0:\n",
    "            # .item() 将 Tensor 转换为 Python 标准数值\n",
    "            print(f\"训练次数：{total_train_step},Loss:{loss}\") \n",
    "    \n",
    "    # ---------------------------\n",
    "    # 测试步骤 (Testing Loop)\n",
    "    # ---------------------------\n",
    "    # 每一轮训练结束后，用测试集验证一下模型现在的水平\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    # torch.no_grad(): 告诉 PyTorch 下面的代码只需要前向计算，\n",
    "    # 不需要计算梯度 (能省内存，加速计算)，因为测试不需要更新参数。\n",
    "    with torch.no_grad(): \n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            outputs = tudui(imgs)\n",
    "            \n",
    "            # 计算这批数据的 Loss\n",
    "            loss = loss_fn(outputs, targets) \n",
    "            \n",
    "            # 将 Loss 累加，算出整套测试集的总误差\n",
    "            total_test_loss = total_test_loss + loss\n",
    "            \n",
    "    print(f\"整体测试集上的Loss:{total_test_loss}\")\n",
    "    \n",
    "    total_test_step = total_test_step + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 核心比喻：平时做作业 vs. 期末考试\n",
    "你可以把神经网络想象成一个学生。\n",
    "\n",
    "场景 A：训练阶段 (Training) -> requires_grad=True\n",
    "状态：学生在做练习题。\n",
    "\n",
    "目的：为了学会。做错了要找原因，要改正。\n",
    "\n",
    "操作：学生不仅要写出答案，还要记住做题的每一步思路（这就是 PyTorch 里的“计算图”）。\n",
    "\n",
    "为什么：因为等会儿老师改卷子（算 Loss），发现错了，学生需要回过头去检查是哪一步想岔了（反向传播），然后修正自己的脑子（更新参数）。\n",
    "\n",
    "代价：因为要记每一步的思路，脑子（显存/内存）很累，记得东西很多。\n",
    "\n",
    "场景 B：测试阶段 (Testing/Validation) -> with torch.no_grad()\n",
    "状态：你的代码里写的那段 test_dataloader 循环，就是让学生去期末考试。\n",
    "\n",
    "目的：只是为了拿个分，看看学得怎么样。\n",
    "\n",
    "操作：学生看到题，直接写答案就行了。\n",
    "\n",
    "为什么：因为是考试，就算做错了，现在也不允许改（不更新参数，不调用 optimizer.step()）。既然不需要改，那何必浪费脑子去记做题步骤呢？\n",
    "\n",
    "结果：直接写答案，脑子很轻松（省显存），做题速度也快（加速）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 保存每一轮后参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "训练次数：100, Loss:2.2954013347625732\n",
      "训练次数：200, Loss:2.2876718044281006\n",
      "训练次数：300, Loss:2.2744815349578857\n",
      "训练次数：400, Loss:2.213883876800537\n",
      "训练次数：500, Loss:2.1374871730804443\n",
      "训练次数：600, Loss:2.0653278827667236\n",
      "训练次数：700, Loss:2.0152084827423096\n",
      "整体测试集上的Loss:311.2083228826523\n",
      "模型已保存\n",
      "-----第 2 轮训练开始-----\n",
      "训练次数：800, Loss:1.8508447408676147\n",
      "训练次数：900, Loss:1.8060863018035889\n",
      "训练次数：1000, Loss:1.9003418684005737\n",
      "训练次数：1100, Loss:2.0008785724639893\n",
      "训练次数：1200, Loss:1.6996160745620728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m imgs, targets \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# 1. 前向传播 (Forward)\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtudui\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# 2. 计算损失 (Loss)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets) \n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 57\u001b[0m, in \u001b[0;36mTudui.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# 前向传播：数据 x 只要穿过 model1 就行了\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:224\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_jit_internal.py:627\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:827\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    826\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ==========================================\n",
    "# 1. 搭建神经网络 (Model Definition)\n",
    "# ==========================================\n",
    "# 继承 nn.Module，这是所有 PyTorch 网络的基类\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        # 使用 Sequential 容器，把层按顺序串起来，代码更简洁\n",
    "        self.model1 = nn.Sequential(\n",
    "            # --- 第1层：卷积 ---\n",
    "            # 输入: 3通道 (RGB图片)\n",
    "            # 输出: 32通道 (提取了32种特征)\n",
    "            # 计算: (32 - 5 + 2*2)/1 + 1 = 32 (padding=2 维持了尺寸不变)\n",
    "            nn.Conv2d(3, 32, 5, 1, 2), \n",
    "            \n",
    "            # --- 第1层：池化 ---\n",
    "            # 作用: 下采样，减少数据量。 32x32 -> 16x16\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 第2层：卷积 ---\n",
    "            # 输入: 32通道 -> 输出: 32通道 (尺寸维持 16x16)\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),\n",
    "            \n",
    "            # --- 第2层：池化 ---\n",
    "            # 16x16 -> 8x8\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 第3层：卷积 ---\n",
    "            # 输入: 32通道 -> 输出: 64通道 (特征数量翻倍)\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            \n",
    "            # --- 第3层：池化 ---\n",
    "            # 8x8 -> 4x4 (这是进全连接层前的最终尺寸)\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # --- 展平层 ---\n",
    "            # 作用: 把立体的 Feature Map (64, 4, 4) 拉平成一维向量\n",
    "            # 长度 = 64 * 4 * 4 = 1024\n",
    "            nn.Flatten(), \n",
    "            \n",
    "            # --- 全连接层 ---\n",
    "            # 1024 -> 64\n",
    "            nn.Linear(64*4*4, 64),\n",
    "            \n",
    "            # --- 输出层 ---\n",
    "            # 64 -> 10 (对应 CIFAR10 的 10 个分类)\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 前向传播：数据 x 只要穿过 model1 就行了\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 2. 准备数据集 (Data Preparation)\n",
    "# ==========================================\n",
    "# 路径前的 r 表示 raw string，防止 Windows 路径中的反斜杠 \\ 被转义\n",
    "# train=True: 加载训练集 (50000张)\n",
    "train_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=True, transform=torchvision.transforms.ToTensor(), download=True)       \n",
    "# train=False: 加载测试集 (10000张)\n",
    "test_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=False, transform=torchvision.transforms.ToTensor(), download=True)        \n",
    "\n",
    "# 获取数据集长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size)) # 输出 50000\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))   # 输出 10000\n",
    "\n",
    "# DataLoader: 负责把数据打包成 Batch\n",
    "# batch_size=64: 每次给模型喂 64 张图\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 初始化训练组件 (Setup)\n",
    "# ==========================================\n",
    "# 实例化模型\n",
    "tudui = Tudui() \n",
    "\n",
    "# 损失函数: 交叉熵 (专门用于多分类任务)\n",
    "# 它会自动计算 Softmax 和 Log\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器: SGD (随机梯度下降)\n",
    "# learning_rate = 0.01: 控制每次参数更新的幅度\n",
    "learning = 0.01 \n",
    "optimizer = torch.optim.SGD(tudui.parameters(), learning) \n",
    "\n",
    "# 计数器\n",
    "total_train_step = 0 # 记录训练了多少个 batch (步数)\n",
    "total_test_step = 0  # 记录测试了多少次\n",
    "epoch = 10           # 总共训练 10 轮\n",
    "\n",
    "# ==========================================\n",
    "# 4. 训练与测试循环 (Main Loop)\n",
    "# ==========================================\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # ---------------------------\n",
    "    # A. 训练阶段 (Training)\n",
    "    # ---------------------------\n",
    "    # (可选) tudui.train() : 如果有Dropout或BatchNorm层，这里需要开启训练模式\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        \n",
    "        # 1. 前向传播 (Forward)\n",
    "        outputs = tudui(imgs)\n",
    "        \n",
    "        # 2. 计算损失 (Loss)\n",
    "        loss = loss_fn(outputs, targets) \n",
    "        \n",
    "        # 3. 反向传播与优化 (Optimization) --- 核心三步曲 ---\n",
    "        optimizer.zero_grad()  # 梯度清零 (把上一步剩下的梯度倒掉)\n",
    "        loss.backward()        # 反向传播 (计算当前梯度)\n",
    "        optimizer.step()       # 更新参数 (根据梯度修改权重)\n",
    "        \n",
    "        # 4. 打印日志\n",
    "        total_train_step = total_train_step + 1\n",
    "        # 每训练 100 个 Batch 打印一次，避免刷屏\n",
    "        if total_train_step % 100 == 0:\n",
    "            # loss.item() 把 Tensor 里的数值取出来，方便打印\n",
    "            print(\"训练次数：{}, Loss:{}\".format(total_train_step, loss.item())) \n",
    "    \n",
    "    # ---------------------------\n",
    "    # B. 测试阶段 (Testing)\n",
    "    # ---------------------------\n",
    "    # 每一轮训练完，用测试集验证一下效果\n",
    "    # (可选) tudui.eval() : 开启验证模式\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    # with torch.no_grad(): 强制关闭梯度计算引擎\n",
    "    # 作用：1.省显存 2.加速计算 (因为测试不需要反向传播)\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            outputs = tudui(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # 累加 Loss，算出整套测试集的总误差\n",
    "            total_test_loss = total_test_loss + loss.item() \n",
    "            \n",
    "    print(\"整体测试集上的Loss:{}\".format(total_test_loss))\n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    # ---------------------------\n",
    "    # C. 模型保存 (Saving)\n",
    "    # ---------------------------\n",
    "    # 保存每一轮训练后的模型\n",
    "    # 格式：tudui_0.pth, tudui_1.pth ...\n",
    "    torch.save(tudui, f\"D:\\\\深度学习\\\\Pytorch的使用\\\\model\\\\tudui_{i}.pth\")\n",
    "    print(\"模型已保存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. argmax作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 模拟模型的输出 (Outputs)\n",
    "# 假设 Batch_Size = 2 (有2个样本)\n",
    "# 假设有 2 个分类 (二分类问题)\n",
    "# 形状: (2, 2) -> (样本数, 类别数)\n",
    "outputs = torch.tensor([[0.1, 0.2],   # 第1个样本: 预测类别0的分数0.1, 类别1的分数0.2\n",
    "                        [0.05, 0.4]]) # 第2个样本: 预测类别0的分数0.05, 类别1的分数0.4\n",
    "\n",
    "# ==========================================\n",
    "# 2. 理解 argmax (寻找最大值的索引)\n",
    "# ==========================================\n",
    "\n",
    "# 语法: argmax(0) -> 纵向压缩 (跨行比较)\n",
    "# 逻辑: \n",
    "#   - 比较第0列 (0.1 vs 0.05) -> 最大是 0.1 -> 索引是 0\n",
    "#   - 比较第1列 (0.2 vs 0.4)  -> 最大是 0.4 -> 索引是 1\n",
    "# 结果: tensor([0, 1])\n",
    "# 含义: \"在这一列里，哪一行的数最大？\" (通常不用于计算准确率)\n",
    "print(outputs.argmax(0)) \n",
    "\n",
    "# 语法: argmax(1) -> 横向压缩 (跨列比较) 【这是我们最常用的！】\n",
    "# 逻辑:\n",
    "#   - 看第1行 (0.1, 0.2)  -> 最大是 0.2 -> 索引是 1 (预测为类别1)\n",
    "#   - 看第2行 (0.05, 0.4) -> 最大是 0.4 -> 索引是 1 (预测为类别1)\n",
    "# 结果: tensor([1, 1])\n",
    "# 含义: \"对于这个样本，哪个类别的分数最高？\"\n",
    "print(outputs.argmax(1)) \n",
    "\n",
    "# ==========================================\n",
    "# 3. 模拟计算准确率\n",
    "# ==========================================\n",
    "\n",
    "# ⚠️ 注意：为了演示计算过程，这里沿用了你代码里的 argmax(0)\n",
    "# 但在真实训练中，这里通常应该写 outputs.argmax(1)\n",
    "preds = outputs.argmax(0) # 结果是 [0, 1]\n",
    "\n",
    "# 真实标签 (Targets)\n",
    "# 第1个样本是类别0，第2个样本是类别1\n",
    "targets = torch.tensor([0, 1])\n",
    "\n",
    "# 核心步骤: 比较与求和\n",
    "# 1. (preds == targets): 比较预测值和真实值是否相等\n",
    "#    [0, 1] == [0, 1] -> [True, True]\n",
    "# 2. .sum(): 统计 True 的个数 (True当做1, False当做0)\n",
    "#    True + True = 2\n",
    "print((preds == targets).sum()) # 输出: tensor(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 打印正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "训练次数：100,Loss:2.2823076248168945\n",
      "训练次数：200,Loss:2.274754524230957\n",
      "训练次数：300,Loss:2.2235279083251953\n",
      "训练次数：400,Loss:2.1669116020202637\n",
      "训练次数：500,Loss:2.0755064487457275\n",
      "训练次数：600,Loss:2.0025384426116943\n",
      "训练次数：700,Loss:2.0317695140838623\n",
      "整体测试集上的Loss：319.204256772995\n",
      "整体测试集上的正确率：0.2687999904155731\n",
      "模型已保存\n",
      "-----第 2 轮训练开始-----\n",
      "训练次数：800,Loss:1.9110145568847656\n",
      "训练次数：900,Loss:1.9112340211868286\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----第 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m 轮训练开始-----\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# 训练步骤开始\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     70\u001b[0m     imgs, targets \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     71\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tudui(imgs)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # 展平后变成 64*4*4 了\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=True, transform=torchvision.transforms.ToTensor(), download=True)       \n",
    "# train=False: 加载测试集 (10000张)\n",
    "test_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=False, transform=torchvision.transforms.ToTensor(), download=True)               \n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "tudui = Tudui() \n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 1e-2 就是 0.01 的意思\n",
    "optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  \n",
    "\n",
    "# 设置网络的一些参数\n",
    "# 记录训练的次数\n",
    "total_train_step = 0\n",
    "# 记录测试的次数\n",
    "total_test_step = 0\n",
    "\n",
    "# 训练的轮次\n",
    "epoch = 10\n",
    "\n",
    "# 添加 tensorboard\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # 训练步骤开始\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = tudui(imgs)\n",
    "        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距\n",
    "        \n",
    "        # 优化器对模型调优\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward() # 反向传播，计算损失函数的梯度\n",
    "        optimizer.step()   # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            print(\"训练次数：{},Loss:{}\".format(total_train_step,loss.item()))  # 方式二：获得loss值\n",
    "    \n",
    "    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():  # 没有梯度了\n",
    "        for data in test_dataloader: # 测试数据集提取数据\n",
    "            imgs, targets = data\n",
    "            outputs = tudui(imgs)\n",
    "            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失\n",
    "            total_test_loss = total_test_loss + loss.item() # 所有loss\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "    print(\"整体测试集上的Loss：{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率：{}\".format(total_accuracy/test_data_size))\n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    torch.save(tudui, f\"D:\\\\深度学习\\\\Pytorch的使用\\\\model\\\\tudui_{i+1}.pth\")\n",
    "    print(\"模型已保存\")\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 特殊层作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。\n",
    "\n",
    "② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。\n",
    "\n",
    "③ 不启用 Batch Normalization 和 Dropout。\n",
    "如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。\n",
    "\n",
    "④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。\n",
    "\n",
    "⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "训练次数：100，Loss：2.292330265045166\n",
      "训练次数：200，Loss：2.2909886837005615\n",
      "训练次数：300，Loss：2.2775135040283203\n",
      "训练次数：400，Loss：2.2197389602661133\n",
      "训练次数：500，Loss：2.1354541778564453\n",
      "训练次数：600，Loss：2.034959077835083\n",
      "训练次数：700，Loss：2.0130105018615723\n",
      "整体测试集上的Loss：319.69296860694885\n",
      "整体测试集上的正确率：0.2678999900817871\n",
      "模型已保存\n",
      "-----第 2 轮训练开始-----\n",
      "训练次数：800，Loss：1.8924949169158936\n",
      "训练次数：900，Loss：1.8564952611923218\n",
      "训练次数：1000，Loss：1.9163199663162231\n",
      "训练次数：1100，Loss：1.972761631011963\n",
      "训练次数：1200，Loss：1.698002815246582\n",
      "训练次数：1300，Loss：1.6668578386306763\n",
      "训练次数：1400，Loss：1.7467551231384277\n",
      "训练次数：1500，Loss：1.8171281814575195\n",
      "整体测试集上的Loss：294.6422094106674\n",
      "整体测试集上的正确率：0.3321000039577484\n",
      "模型已保存\n",
      "-----第 3 轮训练开始-----\n",
      "训练次数：1600，Loss：1.7753604650497437\n",
      "训练次数：1700，Loss：1.637514591217041\n",
      "训练次数：1800，Loss：1.936806559562683\n",
      "训练次数：1900，Loss：1.710182785987854\n",
      "训练次数：2000，Loss：1.9697281122207642\n",
      "训练次数：2100，Loss：1.507324457168579\n",
      "训练次数：2200，Loss：1.4598215818405151\n",
      "训练次数：2300，Loss：1.8211809396743774\n",
      "整体测试集上的Loss：268.6419733762741\n",
      "整体测试集上的正确率：0.37880000472068787\n",
      "模型已保存\n",
      "-----第 4 轮训练开始-----\n",
      "训练次数：2400，Loss：1.696730136871338\n",
      "训练次数：2500，Loss：1.3451323509216309\n",
      "训练次数：2600，Loss：1.614168643951416\n",
      "训练次数：2700，Loss：1.5963644981384277\n",
      "训练次数：2800，Loss：1.4918489456176758\n",
      "训练次数：2900，Loss：1.6028531789779663\n",
      "训练次数：3000，Loss：1.3561456203460693\n",
      "训练次数：3100，Loss：1.5363717079162598\n",
      "整体测试集上的Loss：260.29946398735046\n",
      "整体测试集上的正确率：0.39590001106262207\n",
      "模型已保存\n",
      "-----第 5 轮训练开始-----\n",
      "训练次数：3200，Loss：1.3781168460845947\n",
      "训练次数：3300，Loss：1.4570066928863525\n",
      "训练次数：3400，Loss：1.4464694261550903\n",
      "训练次数：3500，Loss：1.5474085807800293\n",
      "训练次数：3600，Loss：1.5136005878448486\n",
      "训练次数：3700，Loss：1.3479602336883545\n",
      "训练次数：3800，Loss：1.2738752365112305\n",
      "训练次数：3900，Loss：1.483515977859497\n",
      "整体测试集上的Loss：243.80596554279327\n",
      "整体测试集上的正确率：0.4325999915599823\n",
      "模型已保存\n",
      "-----第 6 轮训练开始-----\n",
      "训练次数：4000，Loss：1.376009464263916\n",
      "训练次数：4100，Loss：1.4102662801742554\n",
      "训练次数：4200，Loss：1.5586539506912231\n",
      "训练次数：4300，Loss：1.202476978302002\n",
      "训练次数：4400，Loss：1.0953962802886963\n",
      "训练次数：4500，Loss：1.3712406158447266\n",
      "训练次数：4600，Loss：1.3603018522262573\n",
      "整体测试集上的Loss：229.96637046337128\n",
      "整体测试集上的正确率：0.4652999937534332\n",
      "模型已保存\n",
      "-----第 7 轮训练开始-----\n",
      "训练次数：4700，Loss：1.3486863374710083\n",
      "训练次数：4800，Loss：1.4762073755264282\n",
      "训练次数：4900，Loss：1.3585703372955322\n",
      "训练次数：5000，Loss：1.3923097848892212\n",
      "训练次数：5100，Loss：0.9942217469215393\n",
      "训练次数：5200，Loss：1.3098843097686768\n",
      "训练次数：5300，Loss：1.1401594877243042\n",
      "训练次数：5400，Loss：1.3566023111343384\n",
      "整体测试集上的Loss：217.51882588863373\n",
      "整体测试集上的正确率：0.499099999666214\n",
      "模型已保存\n",
      "-----第 8 轮训练开始-----\n",
      "训练次数：5500，Loss：1.25923752784729\n",
      "训练次数：5600，Loss：1.188690185546875\n",
      "训练次数：5700，Loss：1.223688006401062\n",
      "训练次数：5800，Loss：1.2504695653915405\n",
      "训练次数：5900，Loss：1.3733277320861816\n",
      "训练次数：6000，Loss：1.5391217470169067\n",
      "训练次数：6100，Loss：1.0835392475128174\n",
      "训练次数：6200，Loss：1.0833714008331299\n",
      "整体测试集上的Loss：206.92583346366882\n",
      "整体测试集上的正确率：0.5250999927520752\n",
      "模型已保存\n",
      "-----第 9 轮训练开始-----\n",
      "训练次数：6300，Loss：1.4169930219650269\n",
      "训练次数：6400，Loss：1.0912859439849854\n",
      "训练次数：6500，Loss：1.5215225219726562\n",
      "训练次数：6600，Loss：1.044875979423523\n",
      "训练次数：6700，Loss：1.091143012046814\n",
      "训练次数：6800，Loss：1.1253798007965088\n",
      "训练次数：6900，Loss：1.106575608253479\n",
      "训练次数：7000，Loss：0.8582056164741516\n",
      "整体测试集上的Loss：198.72406196594238\n",
      "整体测试集上的正确率：0.5472999811172485\n",
      "模型已保存\n",
      "-----第 10 轮训练开始-----\n",
      "训练次数：7100，Loss：1.2614285945892334\n",
      "训练次数：7200，Loss：1.0269255638122559\n",
      "训练次数：7300，Loss：1.104204773902893\n",
      "训练次数：7400，Loss：0.8442661166191101\n",
      "训练次数：7500，Loss：1.2488468885421753\n",
      "训练次数：7600，Loss：1.2494431734085083\n",
      "训练次数：7700，Loss：0.8363684415817261\n",
      "训练次数：7800，Loss：1.2570160627365112\n",
      "整体测试集上的Loss：193.20221555233002\n",
      "整体测试集上的正确率：0.5633000135421753\n",
      "模型已保存\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # 展平后变成 64*4*4 了\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=True, transform=torchvision.transforms.ToTensor())       \n",
    "# train=False: 加载测试集 (10000张)\n",
    "test_data = torchvision.datasets.CIFAR10(r'D:\\深度学习\\100_土堆数据集\\dataset', train=False, transform=torchvision.transforms.ToTensor())               \n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "tudui = Tudui() \n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 1e-2 就是 0.01 的意思\n",
    "optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  \n",
    "\n",
    "# 设置网络的一些参数\n",
    "# 记录训练的次数\n",
    "total_train_step = 0\n",
    "# 记录测试的次数\n",
    "total_test_step = 0\n",
    "\n",
    "# 训练的轮次\n",
    "epoch = 10\n",
    "\n",
    "# 添加 tensorboard\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # 训练步骤开始\n",
    "    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = tudui(imgs)\n",
    "        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距\n",
    "        \n",
    "        # 优化器对模型调优\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward() # 反向传播，计算损失函数的梯度\n",
    "        optimizer.step()   # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            print(\"训练次数：{},Loss:{}\".format(total_train_step,loss.item()))  # 方式二：获得loss值\n",
    "    \n",
    "    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）\n",
    "    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():  # 没有梯度了\n",
    "        for data in test_dataloader: # 测试数据集提取数据\n",
    "            imgs, targets = data\n",
    "            outputs = tudui(imgs)\n",
    "            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失\n",
    "            total_test_loss = total_test_loss + loss.item() # 所有loss\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "    print(\"整体测试集上的Loss:{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率:{}\".format(total_accuracy/test_data_size))\n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    torch.save(tudui, f\"D:\\\\深度学习\\\\Pytorch的使用\\\\model\\\\tudui_{i+1}.pth\") \n",
    "    print(\"模型已保存\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以把深度学习模型想象成一个**“学生”**。\n",
    "\n",
    "model.train() 是 “平时做练习题” 的状态。\n",
    "\n",
    "model.eval() 是 “期末考试” 的状态。\n",
    "\n",
    "这张图解释了，为什么在“做题”和“考试”时，大脑（模型）里的两个特殊结构（Dropout 和 Batch Normalization）必须表现得不一样。\n",
    "\n",
    "我用大白话给你翻译一下这 5 点核心内容：\n",
    "\n",
    "1. 核心主角：Dropout 和 BN 层\n",
    "普通的层（比如 卷积层 Conv2d、全连接层 Linear）在训练和测试时表现是一模一样的。 唯独这两个家伙是“双面人”，如果不告诉它们现在是考试，它们会捣乱。\n",
    "\n",
    "2. Dropout 层（防止学生死记硬背）\n",
    "训练模式 (train)：\n",
    "\n",
    "行为：“随机扔掉一部分课本”。\n",
    "\n",
    "目的：模型会随机“断开”一些神经元（比如 30% 不工作）。这样强迫剩下的神经元加倍努力，防止模型“偷懒”或者死记硬背（过拟合）。\n",
    "\n",
    "考试模式 (eval)：\n",
    "\n",
    "行为：“带上所有课本进考场”。\n",
    "\n",
    "目的：这时候不需要训练了，我们需要火力全开，利用所有的神经元来给出最精准的答案。\n",
    "\n",
    "如果不加 eval() 会怎样？：考试时模型还在随机扔掉神经元，导致预测结果忽高忽低，不准确。\n",
    "\n",
    "3. Batch Normalization (BN) 层（防止评分标准乱变）\n",
    "这是最容易坑新手的点！BN 层的作用是对数据进行“标准化”（比如减均值，除方差）。\n",
    "\n",
    "训练模式 (train)：\n",
    "\n",
    "行为：“按这一批次 (Batch) 的同学算平均分”。\n",
    "\n",
    "目的：每次来 64 张图，BN 层就算出这 64 张图的平均值和方差，用来处理数据。同时，它会悄悄拿个小本子记下来：“这一轮的平均值大概是多少”，用来估算全局的情况。\n",
    "\n",
    "考试模式 (eval)：\n",
    "\n",
    "行为：“按全校的历史平均分算分”。\n",
    "\n",
    "目的：考试时可能只有 1 张图进来，你没法算“这一批的平均值”（一张图没有方差）。所以，BN 层会拿出那个小本子，用训练时记录下来的全局平均值来处理数据。\n",
    "\n",
    "如果不加 eval() 会怎样？：\n",
    "\n",
    "数据污染：模型会根据测试集的数据去更新那个“小本子”，这相当于在考试的时候偷看题目修改答案，是不合规的。\n",
    "\n",
    "预测错误：如果测试时 batch_size=1，计算出的方差可能出问题，导致预测结果完全离谱。\n",
    "\n",
    "4. 总结：那段话到底在说什么？\n",
    "图里第 ④ 点是重点警告：\n",
    "\n",
    "警告： 如果你拿测试数据跑模型，却忘了加 model.eval()： 即便你没有写 optimizer.step()（没有反向传播更新权重），BN 层的那个“小本子”（统计参数）依然会被修改！\n",
    "\n",
    "这意味着你的模型在测试阶段被“改动”了，这是绝对不行的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
