{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GPU训练30轮次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "训练次数:100,Loss:2.282930374145508\n",
      "训练次数:200,Loss:2.268775463104248\n",
      "训练次数:300,Loss:2.2180774211883545\n",
      "训练次数:400,Loss:2.087719440460205\n",
      "训练次数:500,Loss:2.013606309890747\n",
      "训练次数:600,Loss:1.978783130645752\n",
      "训练次数:700,Loss:1.9969037771224976\n",
      "整体测试集上的Loss:311.38084638118744\n",
      "整体测试集上的正确率:0.28929999470710754\n",
      "模型已保存\n",
      "-----第 2 轮训练开始-----\n",
      "训练次数:800,Loss:1.8477585315704346\n",
      "训练次数:900,Loss:1.8340317010879517\n",
      "训练次数:1000,Loss:1.8973135948181152\n",
      "训练次数:1100,Loss:1.9406659603118896\n",
      "训练次数:1200,Loss:1.663216471672058\n",
      "训练次数:1300,Loss:1.6241077184677124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 131\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# === 训练阶段 ===\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# 语法：将模型设置为训练模式。\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# 作用：启用 Dropout 和 BatchNorm 层（虽然这个简单模型里没有，但这是好习惯）。\u001b[39;00m\n\u001b[0;32m    129\u001b[0m tudui\u001b[38;5;241m.\u001b[39mtrain() \n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m    132\u001b[0m     imgs, targets \u001b[38;5;241m=\u001b[39m data            \n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# 语法：将数据移动到设备 (GPU)。\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# 注意：对于 Tensor 数据，.to(device) 不是原地操作，必须把返回值重新赋给变量！\u001b[39;00m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.12\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ----------------------\n",
    "# 1. 硬件配置\n",
    "# ----------------------\n",
    "# 语法：检查当前环境是否支持 NVIDIA 显卡 (CUDA)。\n",
    "# 作用：如果有显卡，代码后续会将模型和数据搬运到显卡上加速训练；否则使用 CPU。\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------\n",
    "# 2. 定义神经网络模型\n",
    "# ----------------------\n",
    "# 语法：继承 nn.Module 类，这是 PyTorch 中构建所有网络的基类。\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # 语法：nn.Sequential 是一个容器，数据会按照定义的顺序依次通过这些层。\n",
    "        # 作用：避免写很多行 self.conv1, self.relu1... 让代码更整洁。\n",
    "        self.model1 = nn.Sequential(\n",
    "            # Layer 1: 卷积层\n",
    "            # 语法：输入通道3(RGB图片)，输出32个特征图，卷积核5x5，步长1，填充2。\n",
    "            # 作用：提取图像的初步特征（如边缘、纹理）。\n",
    "            nn.Conv2d(3, 32, 5, 1, 2), \n",
    "            \n",
    "            # Layer 2: 池化层\n",
    "            # 语法：最大池化，窗口大小2x2。\n",
    "            # 作用：下采样，将图片尺寸缩小一半（特征图变小），减少计算量，保留主要特征。\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Layer 3: 卷积层\n",
    "            # 语法：输入接上一层的32，输出32。\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),\n",
    "            \n",
    "            # Layer 4: 池化层\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Layer 5: 卷积层\n",
    "            # 语法：输出通道增加到64，提取更抽象的高级特征。\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            \n",
    "            # Layer 6: 池化层\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Layer 7: 展平层\n",
    "            # 语法：将多维的特征图 (Batch_size, 64, 4, 4) 展平成一维向量 (Batch_size, 64*4*4)。\n",
    "            # 作用：卷积层输出是立体的，全连接层需要一维线性的输入，所以必须展平。\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Layer 8: 全连接层 (Linear)\n",
    "            # 语法：输入节点数 1024 (64*4*4)，输出节点数 64。\n",
    "            # 作用：将提取的特征进行线性组合。\n",
    "            nn.Linear(64*4*4, 64),\n",
    "            \n",
    "            # Layer 9: 输出层\n",
    "            # 语法：输入64，输出10。\n",
    "            # 作用：CIFAR10 有10个分类，所以最后输出必须是 10 个数字（代表10个类别的概率/分数）。\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    # 语法：前向传播函数，必须重写。\n",
    "    # 作用：定义数据 x 进入网络后怎么走。这里直接通过 model1 容器即可。\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------\n",
    "# 3. 准备数据\n",
    "# ----------------------\n",
    "# 语法：下载并加载 CIFAR10 训练集。\n",
    "# 参数 root: 保存路径；train=True: 下载训练集；transform: 将图片转换为 Tensor 格式（归一化到 0-1）。\n",
    "train_data = torchvision.datasets.CIFAR10(r\"D:\\深度学习\\100_土堆数据集\\dataset\", train=True, transform=torchvision.transforms.ToTensor())      \n",
    "# 语法：下载并加载 CIFAR10 测试集 (train=False)。\n",
    "test_data = torchvision.datasets.CIFAR10(r\"D:\\深度学习\\100_土堆数据集\\dataset\", train=False, transform=torchvision.transforms.ToTensor())      \n",
    "\n",
    "# 语法：获取数据集长度。\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# ----------------------\n",
    "# 4. 加载数据 (DataLoader)\n",
    "# ----------------------\n",
    "# 语法：创建数据加载器。\n",
    "# 作用：batch_size=64 表示一次打包 64 张图片喂给模型。DataLoader 会自动处理打乱数据(shuffle)和打包。\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# ----------------------\n",
    "# 5. 初始化模型与配置\n",
    "# ----------------------\n",
    "# 实例化模型\n",
    "tudui = Tudui() \n",
    "# 语法：将模型移动到 GPU 或 CPU。\n",
    "# 注意：对于 nn.Module 模型对象，.to(device) 是原地操作(in-place)，不需要重新赋值，但写赋值也没错。\n",
    "tudui = tudui.to(device) \n",
    "\n",
    "# 损失函数\n",
    "# 语法：创建交叉熵损失函数，专门用于多分类任务。\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.to(device) # 将损失函数也移动到设备上\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 学习率\n",
    "# 语法：创建 SGD (随机梯度下降) 优化器。\n",
    "# 参数：tudui.parameters() 告诉优化器要更新哪些参数（就是网络里的权重）。\n",
    "optimizer = torch.optim.SGD(tudui.parameters(), learning)   \n",
    "\n",
    "# ----------------------\n",
    "# 6. 训练辅助变量\n",
    "# ----------------------\n",
    "total_train_step = 0 # 记录总共训练了多少步\n",
    "total_test_step = 0  # 记录总共测试了多少轮\n",
    "epoch = 30           # 训练轮数：把整个数据集学 30 遍\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# 7. 开始训练循环\n",
    "# ----------------------\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # === 训练阶段 ===\n",
    "    # 语法：将模型设置为训练模式。\n",
    "    # 作用：启用 Dropout 和 BatchNorm 层（虽然这个简单模型里没有，但这是好习惯）。\n",
    "    tudui.train() \n",
    "    \n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data            \n",
    "        \n",
    "        # 语法：将数据移动到设备 (GPU)。\n",
    "        # 注意：对于 Tensor 数据，.to(device) 不是原地操作，必须把返回值重新赋给变量！\n",
    "        imgs = imgs.to(device) \n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # 1. 前向传播：图片扔进网络，算出预测值\n",
    "        outputs = tudui(imgs)\n",
    "        \n",
    "        # 2. 计算损失：看预测值和真实标签 targets 差多少\n",
    "        loss = loss_fn(outputs, targets) \n",
    "        \n",
    "        # 3. 反向传播与优化 (标准三部曲)\n",
    "        optimizer.zero_grad()   # 梯度清零：清除上一步残留的梯度，否则会累加\n",
    "        loss.backward()         # 反向传播：计算新的梯度\n",
    "        optimizer.step()        # 更新参数：根据梯度调整权重\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        \n",
    "        # 每训练 100 次打印一次信息\n",
    "        if total_train_step % 100 == 0:\n",
    "            # .item() 将 Tensor 类型的 loss 转换为标准 Python 数字\n",
    "            print(\"训练次数:{},Loss:{}\".format(total_train_step, loss.item())) \n",
    "            \n",
    "    \n",
    "    # === 测试/验证阶段 ===\n",
    "    # 语法：将模型设置为评估模式。\n",
    "    # 作用：冻结 Dropout 和 BatchNorm，保证测试结果稳定。\n",
    "    tudui.eval() \n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    # 语法：停止梯度计算。\n",
    "    # 作用：测试阶段不需要反向传播，关掉梯度可以极大节省显存并加速计算。\n",
    "    with torch.no_grad(): \n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = tudui(imgs)\n",
    "            \n",
    "            # 累加 Loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            \n",
    "            # 计算正确率\n",
    "            # outputs.argmax(1): 找出每一行（每张图）概率最大的那个类别的索引（即预测结果）\n",
    "            # (predicted == targets): 比较预测是否正确，得到 True/False\n",
    "            # .sum(): 将 True 视为 1 累加，得到这一批次猜对的数量\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "    # 打印本轮测试结果\n",
    "    print(\"整体测试集上的Loss:{}\".format(total_test_loss))\n",
    "    # 正确率 = 猜对的总数 / 测试集总数\n",
    "    print(\"整体测试集上的正确率:{}\".format(total_accuracy/test_data_size))\n",
    "    \n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    # === 保存模型 ===\n",
    "    # 语法：保存整个模型对象（结构+参数）。\n",
    "    # 注意：更推荐用 state_dict 保存，但作为新手直接 save 整个模型最方便。\n",
    "    torch.save(tudui, \"D:\\\\深度学习\\\\100_土堆数据集\\\\model\\\\tudui_{}.pth\".format(i)) \n",
    "    print(\"模型已保存\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 验证狗是否识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 完整的模型验证(测试，demo)套路，利用已经训练好的模型，然后给它提供输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=307x173 at 0x20E1CEC8130>\n",
      "torch.Size([3, 32, 32])\n",
      "tensor([[-1.7995, -1.0654,  0.8879,  1.2731,  1.1375,  1.8303,  1.7666,  1.2773,\n",
      "         -2.8012, -1.4486]])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# ==========================================\n",
    "# 1. 读取并预处理图片\n",
    "# ==========================================\n",
    "image_path = 'D:\\\\深度学习\\\\100_土堆数据集\\\\imgs\\\\dog.png'\n",
    "image = Image.open(image_path) # 使用 PIL 库打开图片\n",
    "\n",
    "# 【坑点1】格式转换\n",
    "# PNG 图片通常是 4 通道 (RGBA，多了一个透明度通道)，但我们的模型只接收 3 通道 (RGB)。\n",
    "#如果不加这行，进网络的第一层卷积就会报错：Given groups=1, weight of size [32, 3, 5, 5], expected input[1, 4, 32, 32]...\n",
    "image = image.convert(\"RGB\")  \n",
    "print(image)\n",
    "\n",
    "# 定义变换：必须和训练时的变换保持一致\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)), # 缩放到模型要求的尺寸 (32x32)\n",
    "    torchvision.transforms.ToTensor()       # 转为 Tensor，并归一化到 [0,1]\n",
    "])\n",
    "\n",
    "image = transform(image)\n",
    "print(image.shape) # 输出: torch.Size([3, 32, 32]) -> (通道, 高, 宽)\n",
    "\n",
    "# ==========================================\n",
    "# 2. 定义模型结构\n",
    "# ==========================================\n",
    "# ⚠️ 注意：使用 torch.load 加载整个模型时，\n",
    "# 当前代码文件里必须有这个类的定义，否则程序不知道 \"Tudui\" 是个什么东西。\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 3. 加载模型并预测\n",
    "# ==========================================\n",
    "\n",
    "# 【坑点2】设备映射 map_location\n",
    "# 如果你的模型是在 GPU (cuda) 上训练并保存的，但现在想在只有 CPU 的电脑上运行，\n",
    "# 必须加上 map_location=torch.device('cpu')，否则会报错。\n",
    "# 修改前：\n",
    "# model = torch.load('D:\\\\...\\\\tudui_1.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# 修改后 (加上 weights_only=False)：\n",
    "model = torch.load('D:\\\\深度学习\\\\100_土堆数据集\\\\model\\\\tudui_1.pth', map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "# 【坑点3】增加 Batch 维度\n",
    "# 现在的 image 是 3维的 (3, 32, 32)。\n",
    "# 但 PyTorch 模型要求的输入必须是 4维的 (Batch_Size, Channel, Height, Width)。\n",
    "# 所以需要 reshape 变成 (1, 3, 32, 32)，那个 \"1\" 就是 Batch_Size。\n",
    "image = torch.reshape(image,(1,3,32,32)) \n",
    "\n",
    "# 切换到测试模式 (虽然这个简单模型没影响，但这是好习惯)\n",
    "model.eval()\n",
    "\n",
    "# 开启无梯度模式 (省内存，不记录计算图)\n",
    "with torch.no_grad(): \n",
    "    output = model(image)\n",
    "\n",
    "# 打印原始输出 (10个数字，代表10个类别的分数)\n",
    "print(output)\n",
    "\n",
    "# 打印最终预测结果\n",
    "# argmax(1) 表示在横向(第1维度)取最大值的索引\n",
    "# 比如输出是 [-1.2, 3.5, 0.1...]，3.5 最大，索引是 1，则预测结果是类别 1 (dog)\n",
    "print(output.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 验证飞机是否识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=245x181 at 0x20E1D27CE50>\n",
      "torch.Size([3, 32, 32])\n",
      "Tudui(\n",
      "  (model1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-3.1593,  1.0889, -0.1619,  1.5965, -0.2149,  1.8753,  3.4098, -0.1995,\n",
      "         -2.3709,  0.8905]], grad_fn=<AddmmBackward0>)\n",
      "tensor([6])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "image_path = r'D:\\深度学习\\100_土堆数据集\\imgs\\plane.png'\n",
    "image = Image.open(image_path) # PIL类型的Image\n",
    "image = image.convert(\"RGB\")  # 4通道的RGBA转为3通道的RGB图片\n",
    "print(image)\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32,32)),   \n",
    "                                            torchvision.transforms.ToTensor()])\n",
    "\n",
    "image = transform(image)\n",
    "print(image.shape)\n",
    "\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "model = torch.load('D:\\\\深度学习\\\\100_土堆数据集\\\\model\\\\tudui_1.pth', map_location=torch.device('cpu'), weights_only=False) \n",
    "print(model)\n",
    "image = torch.reshape(image,(1,3,32,32)) # 转为四维，符合网络输入需求\n",
    "model.eval()\n",
    "with torch.no_grad():  # 不进行梯度计算，减少内存计算\n",
    "    output = model(image)\n",
    "output = model(image)\n",
    "print(output)\n",
    "print(output.argmax(1)) # 概率最大类别的输出"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
