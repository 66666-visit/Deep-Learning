{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ NLP åŸºç¡€ï¼šæ–‡æœ¬é¢„å¤„ç†çŸ¥è¯†ç‚¹æ¢³ç†\n",
    "\n",
    "> **åŸºäºæ–‡ä»¶**ï¼š`247_æ–‡æœ¬é¢„å¤„ç†.ipynb`\n",
    "> **æ ¸å¿ƒç›®æ ‡**ï¼šå°†äººç±»è¯»å¾—æ‡‚çš„â€œæ–‡æœ¬â€ï¼Œè½¬æ¢ä¸ºè®¡ç®—æœºç®—å¾—åŠ¨çš„â€œæ•°å­—â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1. æ ¸å¿ƒæµç¨‹æ€»è§ˆ\n",
    "\n",
    "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ç¬¬ä¸€æ­¥æ°¸è¿œæ˜¯é¢„å¤„ç†ã€‚æƒ³è±¡ä½ è¦æ•™ä¸€ä¸ªåªè®¤è¯†æ•°å­—çš„å¤–æ˜Ÿäººè¯»åœ°çƒçš„ä¹¦ï¼Œæµç¨‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "1.  **åŸå§‹æ–‡æœ¬**ï¼š `\"I love AI\"`\n",
    "    â†“ *(Tokenization åˆ‡åˆ†)*\n",
    "2.  **è¯å…ƒ (Tokens)**ï¼š `[\"I\", \"love\", \"AI\"]`\n",
    "    â†“ *(Vocabulary å¯¹ç…§å­—å…¸)*\n",
    "3.  **è¯è¡¨æ˜ å°„**ï¼š `{\"I\": 10, \"love\": 20, \"AI\": 30}`\n",
    "    â†“ *(Corpus è½¬æ¢)*\n",
    "4.  **æœ€ç»ˆè¾“å‡º (Corpus)**ï¼š `[10, 20, 30]`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. çŸ¥è¯†ç‚¹è¯¦è§£\n",
    "\n",
    "### çŸ¥è¯†ç‚¹ 1ï¼šè¯»å–æ•°æ®é›† (Reading the Dataset)\n",
    "* **æ¦‚å¿µ**ï¼šå°†åŸå§‹æ–‡æœ¬æ–‡ä»¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚\n",
    "* **é€šä¿—è§£é‡Š**ï¼šå°±åƒè¦æŠŠå¤§è±¡è£…å†°ç®±ï¼Œç¬¬ä¸€æ­¥å¾—å…ˆæ‰¾åˆ°å¤§è±¡ã€‚ä»£ç é¦–å…ˆè¦æŠŠä¹¦ï¼ˆå¦‚ã€Šæ—¶å…‰æœºå™¨ã€‹ï¼‰çš„å†…å®¹è¯»å–æˆå­—ç¬¦ä¸²ã€‚\n",
    "* **ç®€å•ä¾‹å­**ï¼š\n",
    "    å‡è®¾æˆ‘ä»¬çš„ä¹¦é‡Œåªæœ‰ä¸€å¥è¯ï¼š\n",
    "    > *\"The time machine is fun.\"*\n",
    "    è¿™å°±æ˜¯æˆ‘ä»¬çš„**åŸå§‹æ•°æ®**ã€‚\n",
    "\n",
    "### çŸ¥è¯†ç‚¹ 2ï¼šè¯å…ƒåŒ– (Tokenization)\n",
    "* **æ¦‚å¿µ**ï¼šå°†é•¿æ–‡æœ¬â€œåˆ‡â€æˆæœ€å°çš„å¤„ç†å•ä½ã€‚\n",
    "* **é€šä¿—è§£é‡Š**ï¼šè®¡ç®—æœºå¤„ç†é•¿å¥å­å¾ˆç´¯ï¼Œå®ƒæ›´å–œæ¬¢å¤„ç†ä¸€ä¸ªä¸ªå°ç§¯æœ¨ã€‚è¿™äº›å°ç§¯æœ¨å°±å« **Tokenï¼ˆè¯å…ƒï¼‰**ã€‚\n",
    "* **ä»£ç å¯¹åº”**ï¼šæ–‡ä»¶ä¸­å®šä¹‰äº† `tokenize` å‡½æ•°ã€‚\n",
    "* **å¸¸è§åˆ‡æ³•**ï¼š\n",
    "    * **æŒ‰å•è¯åˆ‡ (Word-level)**ï¼š`['The', 'time', 'machine', 'is', 'fun']` ï¼ˆæœ€å¸¸ç”¨ï¼‰\n",
    "    * **æŒ‰å­—ç¬¦åˆ‡ (Char-level)**ï¼š`['T', 'h', 'e', ' ', 't', 'i', ...]`\n",
    "\n",
    "### çŸ¥è¯†ç‚¹ 3ï¼šæ„å»ºè¯è¡¨ (Vocabulary)\n",
    "* **æ¦‚å¿µ**ï¼šå»ºç«‹ä¸€ä¸ªâ€œå•è¯ <-> æ•°å­—ç´¢å¼•â€çš„æ˜ å°„è¡¨ã€‚\n",
    "* **é€šä¿—è§£é‡Š**ï¼šç»™æ¯ä¸ªè¯å‘ä¸€ä¸ªå”¯ä¸€çš„â€œèº«ä»½è¯å·â€ã€‚å¦‚æœä¸å‘èº«ä»½è¯ï¼Œè®¡ç®—æœºä¸çŸ¥é“ 'apple' å’Œ 'banana' æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚\n",
    "* **ä»£ç å¯¹åº”**ï¼šæ–‡ä»¶ä¸­å®šä¹‰äº† `Vocab` ç±»ã€‚\n",
    "* **æ ¸å¿ƒé€»è¾‘**ï¼š\n",
    "    1.  **ç»Ÿè®¡é¢‘ç‡**ï¼šæ•°æ•°æ¯ä¸ªè¯å‡ºç°äº†å‡ æ¬¡ã€‚\n",
    "    2.  **è¿‡æ»¤**ï¼šå»æ‰å‡ºç°æ¬¡æ•°å¤ªå°‘ï¼ˆ< min_freqï¼‰çš„ç”Ÿåƒ»è¯ï¼Œå‡å°‘è®¡ç®—é‡ã€‚\n",
    "    3.  **ç‰¹æ®Šæ ‡è®°**ï¼š\n",
    "        * `<unk>` (unknown)ï¼šé‡åˆ°æ²¡è§è¿‡çš„è¯ï¼Œç»Ÿç»Ÿå«è¿™ä¸ªä»£å·ã€‚\n",
    "        * `<pad>`ï¼šè¡¥é½é•¿åº¦ç”¨çš„å ä½ç¬¦ã€‚\n",
    "        * `<bos>` / `<eos>`ï¼šå¥å­çš„å¼€å§‹ (Begin) å’Œç»“æŸ (End)ã€‚\n",
    "* **ç®€å•ä¾‹å­**ï¼š\n",
    "    ```python\n",
    "    # è¯è¡¨ (Vocab)\n",
    "    {\n",
    "      \"The\": 0,\n",
    "      \"time\": 1,\n",
    "      \"machine\": 2,\n",
    "      \"<unk>\": 3\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### çŸ¥è¯†ç‚¹ 4ï¼šè¯­æ–™åº“è½¬æ¢ (Corpus Construction)\n",
    "* **æ¦‚å¿µ**ï¼šå°†æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºç´¢å¼•åºåˆ—ã€‚\n",
    "* **é€šä¿—è§£é‡Š**ï¼šè¿™æ˜¯æœ€åä¸€æ­¥ï¼Œæ‹¿ç€â€œè¯è¡¨â€è¿™ä¸ªå­—å…¸ï¼ŒæŠŠæ•´æœ¬ä¹¦ç¿»è¯‘æˆä¸€ä¸²æ•°å­—ã€‚è¿™ä¸²æ•°å­—æ‰æ˜¯æ¨¡å‹çœŸæ­£åƒè¿›å»çš„æ•°æ®ã€‚\n",
    "* **ä»£ç å¯¹åº”**ï¼šä»£ç ä¸­é€šå¸¸é€šè¿‡åˆ—è¡¨æ¨å¯¼å¼ `[vocab[token] for token in tokens]` å®ç°ã€‚\n",
    "* **ç®€å•ä¾‹å­**ï¼š\n",
    "    * **è¾“å…¥**ï¼š`\"The time machine\"`\n",
    "    * **æŸ¥è¡¨è½¬æ¢**ï¼š`[0, 1, 2]`\n",
    "    * **è¾“å…¥ï¼ˆå«ç”Ÿè¯ï¼‰**ï¼š`\"The apple\"` ï¼ˆå‡è®¾ apple ä¸åœ¨è¯è¡¨ä¸­ï¼‰\n",
    "    * **æŸ¥è¡¨è½¬æ¢**ï¼š`[0, 3]` ï¼ˆ3 ä»£è¡¨ unknownï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## 3. æ€»ç»“\n",
    "\n",
    "è¿™ä»½ä»£ç å±•ç¤ºäº†æ·±åº¦å­¦ä¹  NLP ä»»åŠ¡çš„**ç¬¬ä¸€å—åŸºçŸ³**ã€‚\n",
    "\n",
    "| æ­¥éª¤ | è¾“å…¥ (Input) | è¾“å‡º (Output) | ä½œç”¨ |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1. è¯»å–** | æ–‡æœ¬æ–‡ä»¶ | å­—ç¬¦ä¸² | è·å–åŸå§‹ç´ æ |\n",
    "| **2. è¯å…ƒåŒ–** | å­—ç¬¦ä¸² | Token åˆ—è¡¨ | æŠŠå¥å­åˆ‡ç¢æˆè¯ |\n",
    "| **3. å»ºè¯è¡¨** | Token åˆ—è¡¨ | å­—å…¸ (Map) | ç»™æ¯ä¸ªè¯åˆ†é… ID |\n",
    "| **4. è½¬è¯­æ–™** | Token åˆ—è¡¨ | ç´¢å¼•åˆ—è¡¨ (Int List) | **æ•°å­—åŒ–**ï¼Œä¾›æ¨¡å‹è®¡ç®— |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. æ–‡æœ¬é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections  \n",
    "# [è¯­æ³•]: å¯¼å…¥ Python æ ‡å‡†åº“ collections (å®¹å™¨æ•°æ®ç±»å‹)\n",
    "# [ä½œç”¨]: \n",
    "# 1. åœ¨ NLP ä¸­ï¼Œæˆ‘ä»¬æœ€å¸¸ç”¨çš„æ˜¯ collections.Counter è¿™ä¸ªå·¥å…·ã€‚\n",
    "# 2. å®ƒçš„ä½œç”¨æ˜¯â€œæ•°æ•°â€ã€‚æ¯”å¦‚ç»™ä½ ä¸€ç¯‡æ–‡ç« ï¼Œå®ƒèƒ½è‡ªåŠ¨ç»Ÿè®¡å‡ºæ¯ä¸ªå•è¯å‡ºç°äº†å¤šå°‘æ¬¡ï¼ˆè¯é¢‘ç»Ÿè®¡ï¼‰ã€‚\n",
    "# 3. ç»Ÿè®¡å®Œè¯é¢‘åï¼Œæˆ‘ä»¬æ‰èƒ½æ„å»ºâ€œè¯è¡¨ (Vocab)â€ï¼ŒæŠŠé‚£äº›å‡ºç°æ¬¡æ•°å¤ªå°‘çš„ç”Ÿåƒ»è¯è¿‡æ»¤æ‰ã€‚\n",
    "\n",
    "import re  \n",
    "# [è¯­æ³•]: å¯¼å…¥ Python æ ‡å‡†åº“ re (Regular Expression æ­£åˆ™è¡¨è¾¾å¼)\n",
    "# [ä½œç”¨]: \n",
    "# 1. å®ƒæ˜¯æ–‡æœ¬æ¸…æ´—çš„ç¥å™¨ã€‚\n",
    "# 2. åŸå§‹æ–‡æœ¬é‡Œæœ‰å¾ˆå¤šæˆ‘ä»¬è¦å»æ‰çš„è„æ•°æ®ï¼Œæ¯”å¦‚æ ‡ç‚¹ç¬¦å· (,, !, ?)ã€ç‰¹æ®Šå­—ç¬¦ (@, #) æˆ–è€…å¤šä½™çš„ç©ºæ ¼ã€‚\n",
    "# 3. ä½ çš„ä»£ç ä¸­ç”¨å®ƒæ¥æŠŠæ‰€æœ‰â€œéå­—æ¯â€çš„å­—ç¬¦æ›¿æ¢æˆç©ºæ ¼ï¼Œåªä¿ç•™çº¯è‹±æ–‡å•è¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘  å°†æ•°æ®é›†è¯»å–åˆ°ç”±æ–‡æœ¬è¡Œç»„æˆçš„åˆ—è¡¨ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¯»å–ã€‚\n",
      "æ€»è¡Œæ•°: 3221\n",
      "ç¬¬ä¸€è¡Œ: the time machine by h g wells\n",
      "ç¬¬äºŒè¡Œ: \n",
      "ç¬¬åè¡Œ: twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "import os  # [è¯­æ³•]: å¯¼å…¥æ ‡å‡†åº“ os (Operating System)ã€‚ [ä½œç”¨]: ç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€å¤„ç†æ–‡ä»¶è·¯å¾„ã€‚\n",
    "import re  # [è¯­æ³•]: å¯¼å…¥æ ‡å‡†åº“ re (Regular Expression)ã€‚ [ä½œç”¨]: ç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å’Œæ›¿æ¢ï¼Œæ˜¯æ–‡æœ¬æ¸…æ´—çš„æ ¸å¿ƒå·¥å…·ã€‚\n",
    "import requests  # [è¯­æ³•]: å¯¼å…¥ç¬¬ä¸‰æ–¹åº“ requestsã€‚ [ä½œç”¨]: ç”¨äºå‘é€ HTTP è¯·æ±‚ï¼Œä»äº’è”ç½‘ä¸Šä¸‹è½½æ–‡ä»¶ã€‚\n",
    "\n",
    "# ==========================================\n",
    "# 1. å®šä¹‰ä¸‹è½½å‡½æ•° (æ›¿ä»£ d2l.DATA_HUB)\n",
    "# ==========================================\n",
    "def download_time_machine():\n",
    "    # [è¯­æ³•]: å˜é‡èµ‹å€¼ (å­—ç¬¦ä¸²)ã€‚\n",
    "    # [ä½œç”¨]: å®šä¹‰æ•°æ®é›†çš„ä¸‹è½½é“¾æ¥ (URL)ã€‚\n",
    "    url = \"http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt\"\n",
    "    \n",
    "    # [è¯­æ³•]: å˜é‡èµ‹å€¼ (å­—ç¬¦ä¸²)ã€‚\n",
    "    # [ä½œç”¨]: å®šä¹‰æ–‡ä»¶ä¸‹è½½åä¿å­˜åˆ°æœ¬åœ°çš„æ–‡ä»¶åã€‚\n",
    "    save_path = \"timemachine.txt\"\n",
    "\n",
    "    # [è¯­æ³•]: if åˆ¤æ–­ + os.path.exists å‡½æ•°ã€‚\n",
    "    # [ä½œç”¨]: æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²ç»æœ‰äº†è¿™ä¸ªæ–‡ä»¶ã€‚å¦‚æœæœ‰ï¼Œå°±ä¸é‡å¤ä¸‹è½½ï¼ŒèŠ‚çœæ—¶é—´ã€‚\n",
    "    if not os.path.exists(save_path):\n",
    "        print(\"æ­£åœ¨ä¸‹è½½ time_machine æ•°æ®é›†...\")\n",
    "        \n",
    "        # [è¯­æ³•]: requests.get æ–¹æ³•ã€‚\n",
    "        # [ä½œç”¨]: å‘æœåŠ¡å™¨å‘é€ GET è¯·æ±‚ï¼Œè·å–æ–‡ä»¶å†…å®¹ã€‚\n",
    "        response = requests.get(url) \n",
    "        \n",
    "        # [è¯­æ³•]: with open(...) ä¸Šä¸‹æ–‡ç®¡ç†å™¨ + 'wb' æ¨¡å¼ã€‚\n",
    "        # [ä½œç”¨]: ä»¥â€œäºŒè¿›åˆ¶å†™å…¥æ¨¡å¼â€æ‰“å¼€æ–‡ä»¶ã€‚å¦‚æœä½ ä¸‹è½½çš„æ˜¯æ–‡æœ¬ã€å›¾ç‰‡æˆ– zipï¼Œç”¨ 'wb' æœ€ç¨³å¦¥ã€‚\n",
    "        with open(save_path, 'wb') as f:\n",
    "            # [è¯­æ³•]: æ–‡ä»¶å¯¹è±¡çš„ write æ–¹æ³•ã€‚\n",
    "            # [ä½œç”¨]: å°†ä¸‹è½½ä¸‹æ¥çš„äºŒè¿›åˆ¶å†…å®¹ (response.content) å†™å…¥ç¡¬ç›˜ã€‚\n",
    "            f.write(response.content) \n",
    "            \n",
    "        print(\"ä¸‹è½½å®Œæˆï¼\")\n",
    "    else:\n",
    "        print(\"æ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¯»å–ã€‚\")\n",
    "    \n",
    "    # [è¯­æ³•]: return è¯­å¥ã€‚\n",
    "    # [ä½œç”¨]: è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œä¾›åé¢çš„å‡½æ•°è¯»å–ä½¿ç”¨ã€‚\n",
    "    return save_path\n",
    "\n",
    "# ==========================================\n",
    "# 2. è¯»å–å‡½æ•° (é€»è¾‘ä¿æŒä¸å˜)\n",
    "# ==========================================\n",
    "def read_time_machine():\n",
    "    \"\"\"å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½ä¸ºæ–‡æœ¬è¡Œçš„åˆ—è¡¨ã€‚\"\"\"\n",
    "    \n",
    "    # [è¯­æ³•]: å‡½æ•°è°ƒç”¨ã€‚\n",
    "    # [ä½œç”¨]: è°ƒç”¨ä¸Šé¢å†™å¥½çš„ä¸‹è½½å‡½æ•°ï¼Œç¡®ä¿æ–‡ä»¶åœ¨æœ¬åœ°ï¼Œå¹¶è·å–æ–‡ä»¶åã€‚\n",
    "    file_path = download_time_machine()\n",
    "    \n",
    "    # [è¯­æ³•]: with open(...) + 'r' æ¨¡å¼ + encoding å‚æ•°ã€‚\n",
    "    # [ä½œç”¨]: ä»¥â€œåªè¯»æ–‡æœ¬æ¨¡å¼â€æ‰“å¼€æ–‡ä»¶ã€‚æŒ‡å®š utf-8 ç¼–ç æ˜¯ä¸ºäº†é˜²æ­¢ä¸­æ–‡æˆ–ç‰¹æ®Šå­—ç¬¦ä¹±ç ã€‚\n",
    "    with open(file_path, 'r', encoding='utf-8') as f: \n",
    "        # [è¯­æ³•]: readlines() æ–¹æ³•ã€‚\n",
    "        # [ä½œç”¨]: ä¸€æ¬¡æ€§è¯»å–æ–‡ä»¶æ‰€æœ‰å†…å®¹ï¼Œå¹¶æŒ‰è¡Œåˆ‡å‰²ï¼Œè¿”å›ä¸€ä¸ªåˆ—è¡¨ (List)ã€‚\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # [è¯­æ³•]: åˆ—è¡¨æ¨å¯¼å¼ (List Comprehension) + é“¾å¼è°ƒç”¨ã€‚\n",
    "    # [ä½œç”¨]: è¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆå¾ªç¯ã€‚å®ƒéå† lines ä¸­çš„æ¯ä¸€è¡Œ (line)ï¼Œå¤„ç†å®Œåç”Ÿæˆä¸€ä¸ªæ–°çš„åˆ—è¡¨ã€‚\n",
    "    # ç»†èŠ‚æ‹†è§£:\n",
    "    # 1. re.sub('[^A-Za-z]+', ' ', line): æ­£åˆ™æ›¿æ¢ã€‚[^A-Za-z] è¡¨ç¤ºâ€œéå­—æ¯çš„å­—ç¬¦â€ï¼ŒæŠŠå®ƒä»¬ç»Ÿç»Ÿæ¢æˆç©ºæ ¼ã€‚\n",
    "    # 2. .strip(): å»æ‰å­—ç¬¦ä¸²å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½ç¬¦ (æ¢è¡Œç¬¦ \\n, ç©ºæ ¼)ã€‚\n",
    "    # 3. .lower(): æŠŠæ‰€æœ‰å¤§å†™å­—æ¯è½¬æˆå°å†™ï¼Œç»Ÿä¸€æ ¼å¼ (å¦‚ The -> the)ã€‚\n",
    "    return [ re.sub('[^A-Za-z]+', ' ', line).strip().lower()  for line in lines]\n",
    "\n",
    "# ==========================================\n",
    "# 3. è¿è¡Œæµ‹è¯•\n",
    "# ==========================================\n",
    "# [è¯­æ³•]: å‡½æ•°è°ƒç”¨ã€‚\n",
    "# [ä½œç”¨]: æ‰§è¡Œè¯»å–æµç¨‹ï¼Œè·å–æ¸…æ´—åçš„æ‰€æœ‰æ–‡æœ¬è¡Œã€‚\n",
    "lines = read_time_machine()\n",
    "\n",
    "# [è¯­æ³•]: f-string æ ¼å¼åŒ–å­—ç¬¦ä¸² + len() å‡½æ•°ã€‚\n",
    "# [ä½œç”¨]: è®¡ç®—åˆ—è¡¨é•¿åº¦ï¼Œæ‰“å°æ€»å…±æœ‰å¤šå°‘è¡Œæ–‡æœ¬ã€‚\n",
    "print(f\"æ€»è¡Œæ•°: {len(lines)}\")\n",
    "\n",
    "# [è¯­æ³•]: åˆ—è¡¨ç´¢å¼• (List Indexing)ã€‚\n",
    "# [ä½œç”¨]: è®¿é—®åˆ—è¡¨çš„ç¬¬ 1 ä¸ªå…ƒç´  (ç´¢å¼• 0)ã€‚\n",
    "print(\"ç¬¬ä¸€è¡Œ:\", lines[0])\n",
    "print(\"ç¬¬äºŒè¡Œ:\", lines[1])\n",
    "\n",
    "# [è¯­æ³•]: åˆ—è¡¨ç´¢å¼•ã€‚\n",
    "# [ä½œç”¨]: è®¿é—®åˆ—è¡¨çš„ç¬¬ 11 ä¸ªå…ƒç´  (ç´¢å¼• 10)ã€‚\n",
    "print(\"ç¬¬åè¡Œ:\", lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­£åˆ™è¡¨è¾¾å¼ç¬¦å·è¯¦è§£ [^A-Za-z]+ç¬¦å·è¯­æ³•å«ä¹‰ä½œç”¨[]å­—ç¬¦é›†åˆåŒ¹é…æ–¹æ‹¬å·ä¸­åŒ…å«çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ã€‚^å–åç¬¦å·å½“å®ƒå‡ºç°åœ¨æ–¹æ‹¬å·çš„å¼€å¤´æ—¶ï¼Œè¡¨ç¤ºâ€œåŒ¹é…ä¸åœ¨é›†åˆä¸­çš„å­—ç¬¦â€ã€‚A-Za-zå­—æ¯èŒƒå›´ä»£è¡¨æ‰€æœ‰å¤§å†™ï¼ˆA-Zï¼‰å’Œå°å†™ï¼ˆa-zï¼‰çš„è‹±æ–‡å­—æ¯ã€‚+é‡è¯ï¼ˆ1æ¬¡æˆ–å¤šæ¬¡ï¼‰è¡¨ç¤ºå‰é¢çš„æ¨¡å¼å¿…é¡»è¿ç»­å‡ºç°è‡³å°‘ä¸€æ¬¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘¡ æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªæ ‡è®°åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    # [è¯­æ³•] å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å—ä¸¤ä¸ªå‚æ•°ï¼šlines (å¿…é¡») å’Œ token (å¯é€‰ï¼Œé»˜è®¤å€¼ä¸º 'word')ã€‚\n",
    "    # [ä½œç”¨] å°è£…åˆ†è¯é€»è¾‘ï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚å°†æ–‡æœ¬è¡Œè½¬æ¢æˆå•è¯åˆ—è¡¨æˆ–å­—ç¬¦åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬è¡Œåˆ—è¡¨è¿›è¡Œåˆ†è¯å¤„ç†ã€‚\n",
    "    \"\"\"\n",
    "    # å¦‚æœä»¤ç‰Œç±»å‹ä¸º 'word'\n",
    "    if token == 'word':\n",
    "        # [è¯­æ³•] åˆ—è¡¨æ¨å¯¼å¼ (List Comprehension)ã€‚éå† lines ä¸­çš„æ¯ä¸€è¡Œ (line)ï¼Œå¯¹æ¯è¡Œè°ƒç”¨ .split() æ–¹æ³•ã€‚\n",
    "        # [ä½œç”¨] æŒ‰â€œç©ºæ ¼â€åˆ‡åˆ†æ¯è¡Œæ–‡æœ¬ã€‚ä¾‹å¦‚ \"Hello World\" -> ['Hello', 'World']ã€‚è¿™æ˜¯æœ€åŸºæœ¬çš„è‹±æ–‡åˆ†è¯ã€‚\n",
    "        # æ³¨æ„ line.split() å’Œ line.split(' ') å®Œå…¨ä¸ä¸€æ ·ï¼Œå·¦è¾¹æ˜¯\"æ™ºèƒ½æ¨¡å¼\"\n",
    "        return [line.split() for line in lines]\n",
    "    \n",
    "    # å¦‚æœä»¤ç‰Œç±»å‹ä¸º 'char'\n",
    "    elif token == 'char':\n",
    "        # [è¯­æ³•] åˆ—è¡¨æ¨å¯¼å¼ã€‚éå† lines ä¸­çš„æ¯ä¸€è¡Œï¼Œä½¿ç”¨ list() æ„é€ å‡½æ•°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨ã€‚\n",
    "        # [ä½œç”¨] æŒ‰â€œå­—ç¬¦â€åˆ‡åˆ†ã€‚ä¾‹å¦‚ \"Hi\" -> ['H', 'i']ã€‚å¸¸ç”¨äºå­—ç¬¦çº§æ¨¡å‹æˆ–å¤„ç†ä¸­æ–‡ï¼ˆå› ä¸ºä¸­æ–‡æ²¡æœ‰ç©ºæ ¼åˆ†éš”ï¼‰ã€‚\n",
    "        return [list(line) for line in lines]\n",
    "    \n",
    "    else:\n",
    "        # [è¯­æ³•] else åˆ†æ”¯ï¼Œå¤„ç†å‰é¢ if å’Œ elif éƒ½ä¸æ»¡è¶³çš„æƒ…å†µã€‚\n",
    "        # [ä½œç”¨] é”™è¯¯å¤„ç†æœºåˆ¶ã€‚é˜²æ­¢ç”¨æˆ·è¾“å…¥äº†ä¸æ”¯æŒçš„åˆ†è¯ç±»å‹ï¼ˆå¦‚ 'sentence'ï¼‰ï¼Œæç¤ºå‚æ•°é”™è¯¯ã€‚\n",
    "        print('é”™ä½ï¼šæœªçŸ¥ä»¤ç‰Œç±»å‹ï¼š' + token)\n",
    "\n",
    "# ================= è°ƒç”¨éƒ¨åˆ† =================\n",
    "\n",
    "# [è¯­æ³•] è°ƒç”¨ tokenize å‡½æ•°ï¼Œä¼ å…¥ lines å˜é‡ï¼Œtoken å‚æ•°çœç•¥ï¼ˆä½¿ç”¨é»˜è®¤å€¼ 'word'ï¼‰ï¼Œå¹¶å°†ç»“æœèµ‹å€¼ç»™ tokensã€‚\n",
    "# [ä½œç”¨] æ‰§è¡Œå®é™…çš„åˆ†è¯æ“ä½œã€‚å‡è®¾ lines æ˜¯ä¹‹å‰è¯»å–çš„æ–‡æœ¬åˆ—è¡¨ï¼Œç°åœ¨ tokens å˜æˆäº†ä¸€ä¸ªäºŒç»´åˆ—è¡¨ï¼ˆåˆ—è¡¨çš„åˆ—è¡¨ï¼‰ã€‚\n",
    "tokens = tokenize(lines)\n",
    "\n",
    "# [è¯­æ³•] for å¾ªç¯ï¼Œrange(11) ç”Ÿæˆä» 0 åˆ° 10 çš„æ•´æ•°åºåˆ—ã€‚\n",
    "# [ä½œç”¨] ä»…éå†å‰ 11 è¡Œæ•°æ®ã€‚ç”¨äºå¿«é€Ÿé¢„è§ˆæ•°æ®ï¼Œé¿å…æ‰“å°æ•´ä¸ªæ•°æ®é›†å¯¼è‡´å±å¹•åˆ·å±ã€‚\n",
    "for i in range(11):\n",
    "    # [è¯­æ³•] ä½¿ç”¨ç´¢å¼• i è®¿é—®åˆ—è¡¨ tokens ä¸­çš„å…ƒç´ ï¼Œå¹¶æ‰“å°åˆ°æ§åˆ¶å°ã€‚\n",
    "    # [ä½œç”¨] æ‰“å°ç¬¬ i è¡Œçš„åˆ†è¯ç»“æœã€‚å¦‚æœåŸæ–‡æœ¬æœ‰ç©ºè¡Œï¼Œsplit() ä¼šè¿”å›ç©ºåˆ—è¡¨ []ã€‚\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘¢ æ„å»ºä¸€ä¸ªå­—å…¸ï¼Œé€šå¸¸ä¹Ÿå«åšè¯è¡¨ï¼ˆvocabularyï¼‰ï¼Œç”¨æ¥ä½ å°†å­—ç¬¦ä¸²æ ‡è®°æ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections  \n",
    "# [è¯­æ³•] å¯¼å…¥ Python æ ‡å‡†åº“ä¸­çš„ collections æ¨¡å—ã€‚\n",
    "# [ä½œç”¨] ä¸ºäº†åœ¨åç»­ä»£ç ä¸­ä½¿ç”¨ collections.Counter ç±»æ¥é«˜æ•ˆç»Ÿè®¡è¯é¢‘ã€‚\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"\n",
    "    ç»Ÿè®¡æ ‡è®°çš„é¢‘ç‡ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        tokens (list): \n",
    "            [è¯­æ³•] åˆ—è¡¨ç±»å‹çš„å‚æ•°ã€‚\n",
    "            [ä½œç”¨] å¾…ç»Ÿè®¡çš„è¯­æ–™ï¼Œå¯èƒ½æ˜¯ä¸€ç»´ ['hi', 'you'] æˆ–äºŒç»´ [['hi'], ['you']]ã€‚\n",
    "    \"\"\"\n",
    "    # [è¯­æ³•] \n",
    "    # 1. len(tokens) == 0: æ£€æŸ¥åˆ—è¡¨æ˜¯å¦ä¸ºç©ºã€‚\n",
    "    # 2. isinstance(tokens[0], list): æ£€æŸ¥åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦ä¹Ÿæ˜¯åˆ—è¡¨ã€‚\n",
    "    # [ä½œç”¨] åˆ¤æ–­è¾“å…¥æ•°æ®çš„æ ¼å¼ã€‚å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œæˆ–è€…åˆ—è¡¨é‡Œè£…çš„æ˜¯åˆ—è¡¨ï¼ˆè¯´æ˜æ˜¯äºŒç»´çš„å¥å­åˆ—è¡¨ï¼‰ï¼Œåˆ™éœ€è¦è¿›è¡Œå±•å¹³æ“ä½œã€‚\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # [è¯­æ³•] åŒå±‚åµŒå¥—çš„åˆ—è¡¨æ¨å¯¼å¼ (Flatten logic)ã€‚\n",
    "        # ç­‰ä»·äº:\n",
    "        # temp = []\n",
    "        # for line in tokens:\n",
    "        #     for token in line:\n",
    "        #         temp.append(token)\n",
    "        # [ä½œç”¨] å°†äºŒç»´åˆ—è¡¨å±•å¹³ä¸ºä¸€ç»´åˆ—è¡¨ã€‚ä¾‹å¦‚ [['a', 'b'], ['c']] -> ['a', 'b', 'c']ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç»Ÿè®¡ã€‚\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    \n",
    "    # [è¯­æ³•] å®ä¾‹åŒ– collections.Counter ç±»ã€‚\n",
    "    # [ä½œç”¨] å¯¹åˆ—è¡¨ä¸­çš„å…ƒç´ è¿›è¡Œå“ˆå¸Œè®¡æ•°ï¼Œè¿”å›ä¸€ä¸ª Counter å¯¹è±¡ï¼ˆå¦‚ {'a': 1, 'b': 1, 'c': 1}ï¼‰ã€‚\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"æ–‡æœ¬è¯è¡¨ç±»ï¼Œç”¨äºæ„å»º è¯<->ç´¢å¼• çš„æ˜ å°„å…³ç³»\"\"\"\n",
    "    \n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è¯è¡¨å¯¹è±¡ã€‚\n",
    "\n",
    "        å‚æ•°è¯¦è§£:\n",
    "            tokens (list): \n",
    "                [è¯­æ³•] é»˜è®¤ä¸º None çš„åˆ—è¡¨å‚æ•°ã€‚\n",
    "                [ä½œç”¨] è¾“å…¥çš„è¯­æ–™åº“ï¼Œå¯ä»¥æ˜¯ä¸€ç»´çš„å•è¯åˆ—è¡¨ï¼Œä¹Ÿå¯ä»¥æ˜¯äºŒç»´çš„å¥å­åˆ—è¡¨ã€‚\n",
    "            min_freq (int): \n",
    "                [è¯­æ³•] é»˜è®¤ä¸º 0 çš„æ•´æ•°å‚æ•°ã€‚\n",
    "                [ä½œç”¨] æœ€å°é¢‘ç‡é˜ˆå€¼ã€‚å‡ºç°æ¬¡æ•°å°‘äºè¿™ä¸ªæ•°çš„å•è¯ä¼šè¢«ä¸¢å¼ƒï¼ˆè§†ä¸ºæœªçŸ¥è¯ï¼‰ã€‚\n",
    "            reserved_tokens (list): \n",
    "                [è¯­æ³•] é»˜è®¤ä¸º None çš„åˆ—è¡¨å‚æ•°ã€‚\n",
    "                [ä½œç”¨] éœ€è¦å¼ºåˆ¶ä¿ç•™çš„ç‰¹æ®Šæ ‡è®°ï¼Œä¾‹å¦‚ ['<pad>', '<bos>', '<eos>']ã€‚\n",
    "        \"\"\"\n",
    "        # [è¯­æ³•] if æ¡ä»¶åˆ¤æ–­è¯­å¥ï¼Œåˆ¤æ–­ tokens æ˜¯å¦ä¸º Noneã€‚\n",
    "        # [ä½œç”¨] é˜²å¾¡æ€§ç¼–ç¨‹ã€‚å¦‚æœç”¨æˆ·åˆå§‹åŒ–æ—¶ä»€ä¹ˆéƒ½æ²¡ä¼ ï¼Œå°†å…¶è®¾ä¸ºç©ºåˆ—è¡¨ï¼Œé˜²æ­¢åç»­ count_corpus æŠ¥é”™ã€‚\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        \n",
    "        # [è¯­æ³•] if æ¡ä»¶åˆ¤æ–­è¯­å¥ï¼Œåˆ¤æ–­ reserved_tokens æ˜¯å¦ä¸º Noneã€‚\n",
    "        # [ä½œç”¨] å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šä¿ç•™æ ‡è®°ï¼Œå°†å…¶åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­è¿›è¡Œåˆ—è¡¨æ‹¼æ¥æ“ä½œã€‚\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        \n",
    "        # [è¯­æ³•] å‡½æ•°è°ƒç”¨ï¼Œå°† tokens ä½œä¸ºå‚æ•°ä¼ å…¥ count_corpus å‡½æ•°ã€‚\n",
    "        # [ä½œç”¨] ç»Ÿè®¡è¯­æ–™åº“ä¸­æ¯ä¸ª token å‡ºç°çš„æ¬¡æ•°ï¼Œè¿”å›ä¸€ä¸ª Counter å¯¹è±¡ï¼ˆç±»ä¼¼äºå­—å…¸ {è¯: æ¬¡æ•°}ï¼‰ã€‚\n",
    "        counter = count_corpus(tokens) \n",
    "        \n",
    "        # [è¯­æ³•] \n",
    "        # 1. counter.items(): è·å–å­—å…¸çš„ (é”®, å€¼) å…ƒç»„åˆ—è¡¨ã€‚\n",
    "        # 2. key=lambda x: x[1]: ä½¿ç”¨åŒ¿åå‡½æ•°æŒ‡å®šæ’åºä¾æ®ä¸ºå…ƒç»„çš„ç¬¬2ä¸ªå…ƒç´ ï¼ˆå³é¢‘ç‡ï¼‰ã€‚\n",
    "        # 3. reverse=True: æŒ‡å®šä¸ºé™åºæ’åºã€‚\n",
    "        # 4. sorted(): å†…ç½®æ’åºå‡½æ•°ã€‚\n",
    "        # [ä½œç”¨] å°†è¯é¢‘ç»Ÿè®¡ç»“æœæŒ‰å‡ºç°é¢‘ç‡ä»é«˜åˆ°ä½æ’åºã€‚è¿™æ ·åšæ˜¯ä¸ºäº†åç»­å¤„ç†æ–¹ä¾¿ï¼Œä¹Ÿç¬¦åˆè¯è¡¨çš„ä¸€èˆ¬æ„å»ºé€»è¾‘ï¼ˆå¸¸ç”¨è¯åœ¨å‰ï¼‰ã€‚\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # [è¯­æ³•] å¤šé‡èµ‹å€¼è¯­å¥ï¼›åˆ—è¡¨æ‹¼æ¥æ“ä½œ (+)ã€‚\n",
    "        # [ä½œç”¨] \n",
    "        # 1. self.unk = 0: è®¾å®šæœªçŸ¥è¯æ ‡è®° <unk> çš„ç´¢å¼•æ°¸è¿œä¸º 0ã€‚\n",
    "        # 2. uniq_tokens: åˆå§‹åŒ–å”¯ä¸€è¯åˆ—è¡¨ï¼Œé¦–å…ˆåŠ å…¥ <unk> å’Œç”¨æˆ·å®šä¹‰çš„ä¿ç•™æ ‡è®°ï¼ˆå¦‚ <pad>ï¼‰ã€‚\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens \n",
    "        \n",
    "        # [è¯­æ³•] \n",
    "        # 1. uniq_tokens += [...]: åˆ—è¡¨çš„åŸåœ°æ‹¼æ¥ã€‚\n",
    "        # 2. [token for ... if ...]: å¸¦æœ‰è¿‡æ»¤æ¡ä»¶çš„åˆ—è¡¨æ¨å¯¼å¼ã€‚\n",
    "        # [ä½œç”¨] æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ï¼š\n",
    "        # éå†æ’åºåçš„è¯é¢‘åˆ—è¡¨ï¼Œå°†æ»¡è¶³ \"é¢‘ç‡ >= min_freq\" ä¸” \"ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­\" çš„è¯æ·»åŠ åˆ° uniq_tokens ä¸­ã€‚\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs\n",
    "                        if freq >= min_freq and token not in uniq_tokens]\n",
    "        \n",
    "        # [è¯­æ³•] å¤šé‡èµ‹å€¼ï¼›åˆå§‹åŒ–ç©ºåˆ—è¡¨ [] å’Œç©ºå­—å…¸ dict()ã€‚\n",
    "        # [ä½œç”¨] \n",
    "        # self.idx_to_token: ç”¨äº \"ç´¢å¼• -> è¯\" (è§£ç )ã€‚\n",
    "        # self.token_to_idx: ç”¨äº \"è¯ -> ç´¢å¼•\" (ç¼–ç )ã€‚\n",
    "        self.idx_to_token, self.token_to_idx = [], {}\n",
    "        \n",
    "        # [è¯­æ³•] for å¾ªç¯éå†åˆ—è¡¨ uniq_tokensã€‚\n",
    "        # [ä½œç”¨] éå†æœ€ç»ˆç­›é€‰å‡ºçš„è¯è¡¨ï¼Œå»ºç«‹åŒå‘æ˜ å°„å…³ç³»ã€‚\n",
    "        for token in uniq_tokens:\n",
    "            # [è¯­æ³•] åˆ—è¡¨çš„ append æ–¹æ³•ã€‚\n",
    "            # [ä½œç”¨] å°† token æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚åˆ—è¡¨çš„è‡ªç„¶ä¸‹æ ‡ï¼ˆ0, 1, 2...ï¼‰å³ä¸ºè¯¥è¯çš„ç´¢å¼•ã€‚\n",
    "            self.idx_to_token.append(token)\n",
    "            \n",
    "            # [è¯­æ³•] å­—å…¸èµ‹å€¼æ“ä½œï¼›len() è·å–åˆ—è¡¨é•¿åº¦ã€‚\n",
    "            # [ä½œç”¨] è®°å½•æ˜ å°„å…³ç³»ï¼šé”®æ˜¯ tokenï¼Œå€¼æ˜¯å®ƒåœ¨ idx_to_token ä¸­çš„ä¸‹æ ‡ (å³å½“å‰é•¿åº¦ - 1)ã€‚\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        [è¯­æ³•] Python é­”æœ¯æ–¹æ³• (Magic Method)ï¼Œå®šä¹‰å½“å¯¹å¯¹è±¡è°ƒç”¨ len() æ—¶è¡Œä¸ºã€‚\n",
    "        [ä½œç”¨] è¿”å›è¯è¡¨çš„å¤§å°ã€‚\n",
    "        \"\"\"\n",
    "        # [è¯­æ³•] len() å‡½æ•°è·å–åˆ—è¡¨é•¿åº¦ã€‚\n",
    "        # [ä½œç”¨] è¿”å›å”¯ä¸€è¯åˆ—è¡¨çš„é•¿åº¦ï¼Œå³è¯è¡¨ä¸­è¯çš„æ€»æ•°ã€‚\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        \"\"\"\n",
    "        [è¯­æ³•] Python é­”æœ¯æ–¹æ³•ï¼Œå®šä¹‰å¯¹è±¡ä½¿ç”¨ç´¢å¼•æ“ä½œç¬¦ object[key] æ—¶çš„è¡Œä¸ºã€‚\n",
    "        [ä½œç”¨] ç»™å®šè¯ï¼ˆæˆ–è¯åˆ—è¡¨ï¼‰ï¼Œè¿”å›å¯¹åº”çš„ç´¢å¼•ï¼ˆæˆ–ç´¢å¼•åˆ—è¡¨ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        # [è¯­æ³•] not isinstance(..., (list, tuple)): ç±»å‹æ£€æŸ¥ï¼Œåˆ¤æ–­ tokens æ˜¯å¦**ä¸æ˜¯**åˆ—è¡¨æˆ–å…ƒç»„ã€‚\n",
    "        # [ä½œç”¨] å¤„ç†å•ä¸ªè¯çš„æƒ…å†µã€‚å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸² 'apple'ï¼Œèµ°è¿™ä¸ªåˆ†æ”¯ã€‚\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            # [è¯­æ³•] å­—å…¸çš„ .get(key, default) æ–¹æ³•ã€‚\n",
    "            # [ä½œç”¨] æ ¸å¿ƒæŸ¥è¡¨ï¼šå¦‚æœè¯åœ¨è¯è¡¨ä¸­ï¼Œè¿”å›ç´¢å¼•ï¼›å¦‚æœä¸åœ¨ï¼Œè¿”å› self.unk (å³ 0)ã€‚è¿™æ˜¯å¤„ç† OOV (Out Of Vocabulary) çš„å…³é”®ã€‚\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        \n",
    "        # [è¯­æ³•] åˆ—è¡¨æ¨å¯¼å¼ï¼›é€’å½’è°ƒç”¨ self.__getitem__ã€‚\n",
    "        # [ä½œç”¨] å¤„ç†è¯åˆ—è¡¨çš„æƒ…å†µã€‚å¦‚æœä¼ å…¥çš„æ˜¯ ['apple', 'banana']ï¼Œéå†åˆ—è¡¨å¯¹æ¯ä¸ªè¯é€’å½’è°ƒç”¨æœ¬å‡½æ•°ï¼Œè¿”å›ç´¢å¼•åˆ—è¡¨ã€‚\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        \"\"\"\n",
    "        [è¯­æ³•] æ™®é€šå®ä¾‹æ–¹æ³•ã€‚\n",
    "        [ä½œç”¨] ç»™å®šç´¢å¼•ï¼ˆæˆ–ç´¢å¼•åˆ—è¡¨ï¼‰ï¼Œè¿”å›å¯¹åº”çš„è¯ï¼ˆæˆ–è¯åˆ—è¡¨ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        # [è¯­æ³•] ç±»å‹æ£€æŸ¥ï¼Œåˆ¤æ–­ indices æ˜¯å¦**ä¸æ˜¯**åˆ—è¡¨æˆ–å…ƒç»„ã€‚\n",
    "        # [ä½œç”¨] å¤„ç†å•ä¸ªç´¢å¼•çš„æƒ…å†µã€‚å¦‚æœä¼ å…¥æ˜¯æ•´æ•° 1ï¼Œèµ°è¿™ä¸ªåˆ†æ”¯ã€‚\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            # [è¯­æ³•] åˆ—è¡¨ç´¢å¼•è®¿é—®ã€‚\n",
    "            # [ä½œç”¨] ç›´æ¥è¿”å›è¯¥ç´¢å¼•å¯¹åº”çš„è¯ã€‚\n",
    "            return self.idx_to_token[indices]\n",
    "        \n",
    "        # [è¯­æ³•] åˆ—è¡¨æ¨å¯¼å¼ã€‚\n",
    "        # [ä½œç”¨] å¤„ç†ç´¢å¼•åˆ—è¡¨çš„æƒ…å†µã€‚å¦‚æœä¼ å…¥ [1, 2]ï¼Œéå†åˆ—è¡¨å¹¶å°†æ¯ä¸ªç´¢å¼•è½¬æ¢ä¸ºè¯ï¼Œè¿”å›è¯åˆ—è¡¨ã€‚\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘£ æ„å»ºè¯æ±‡è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºä¸€ä¸ª Vocab å¯¹è±¡ï¼Œå°†æ ‡è®°åˆ—è¡¨ tokens ä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œç”¨äºæ„å»ºè¯è¡¨\n",
    "vocab = Vocab(tokens)\n",
    "# è·å–è¯è¡¨ä¸­çš„å‰ 10 ä¸ªæ ‡è®°åŠå…¶å¯¹åº”çš„ç´¢å¼•å€¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨è¿›è¡Œæ‰“å°è¾“å‡º\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘£ å°†æ¯ä¸€è¡Œæ–‡æœ¬è½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "word: ['i']\n",
      "indices: [2]\n",
      "word: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "indices: [1, 19, 71, 16, 37, 11, 115, 42, 680, 6, 586, 4, 108]\n",
      "word: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "indices: [7, 1420, 5, 2185, 587, 6, 126, 25, 330, 127, 439, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_37852\\2730249240.py:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if len(tokens[i]) is not 0 :\n"
     ]
    }
   ],
   "source": [
    "# éå†ç´¢å¼•åˆ—è¡¨ [0, 10]\n",
    "for i in range(10):\n",
    "    # æ‰“å°å½“å‰ç´¢å¼• i å¤„çš„æ ‡è®°ï¼ˆå•è¯ï¼‰\n",
    "    if len(tokens[i]) is not 0 :\n",
    "        print('word:', tokens[i])\n",
    "        # è·å–å½“å‰æ ‡è®°åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•å€¼\n",
    "        # æ‰“å°å½“å‰æ ‡è®°åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•å€¼ï¼ˆå¯¹åº”çš„ç´¢å¼•å€¼æˆ–æœªçŸ¥æ ‡è®°ç´¢å¼•ï¼‰\n",
    "        print('indices:',vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘¤ å°†æ‰€æœ‰å†…å®¹æ‰“åŒ…åˆ°load_corpus_time_machineå‡½æ•°ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¯»å–ã€‚\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    # [è¯­æ³•] å‡½æ•°å®šä¹‰ï¼Œå¸¦æœ‰ä¸€ä¸ªé»˜è®¤å‚æ•° max_tokens=-1ã€‚\n",
    "    # [ä½œç”¨] å°è£…æ•°æ®åŠ è½½é€»è¾‘ã€‚max_tokens ç”¨äºé™åˆ¶åŠ è½½çš„æ•°æ®é‡ï¼ˆ-1 è¡¨ç¤ºä¸é™åˆ¶ï¼ŒåŠ è½½å…¨éƒ¨ï¼‰ï¼Œæ–¹ä¾¿è°ƒè¯•å°æ ·æœ¬ã€‚\n",
    "    \"\"\"è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„æ ‡è®°ç´¢å¼•åˆ—è¡¨å’Œè¯æ±‡è¡¨\"\"\"\n",
    "    \n",
    "    # [è¯­æ³•] å‡½æ•°è°ƒç”¨ï¼ˆå‡è®¾ read_time_machine æ˜¯å¤–éƒ¨å®šä¹‰å¥½çš„å‡½æ•°ï¼‰ã€‚\n",
    "    # [ä½œç”¨] è¯»å–åŸå§‹æ–‡æœ¬æ–‡ä»¶ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ–‡æœ¬è¡Œçš„åˆ—è¡¨ï¼ˆå¦‚ [\"Line 1 text\", \"Line 2 text\"]ï¼‰ã€‚\n",
    "    lines = read_time_machine()\n",
    "    \n",
    "    # [è¯­æ³•] è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ tokenize å‡½æ•°ï¼Œä¼ å…¥ 'char' å‚æ•°ã€‚\n",
    "    # [ä½œç”¨] è¿›è¡Œå­—ç¬¦çº§åˆ†è¯ã€‚å°†æ–‡æœ¬è¡Œè½¬æ¢ä¸ºå­—ç¬¦åˆ—è¡¨çš„åˆ—è¡¨ï¼ˆå¦‚ [['H', 'e', 'l', 'l', 'o'], ['W', 'o', 'r', 'l', 'd']]ï¼‰ã€‚\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    \n",
    "    # [è¯­æ³•] å®ä¾‹åŒ– Vocab ç±»ã€‚\n",
    "    # [ä½œç”¨] æ„å»ºè¯è¡¨ã€‚ç»Ÿè®¡æ‰€æœ‰å­—ç¬¦çš„é¢‘ç‡ï¼Œåˆ†é…å”¯ä¸€çš„ç´¢å¼•ï¼ˆ0, 1, 2...ï¼‰ï¼Œå¹¶å¤„ç†æœªçŸ¥å­—ç¬¦ <unk>ã€‚\n",
    "    vocab = Vocab(tokens)\n",
    "    \n",
    "    # [è¯­æ³•] åµŒå¥—çš„åˆ—è¡¨æ¨å¯¼å¼ (Nested List Comprehension)ã€‚\n",
    "    # [é€»è¾‘å±•å¼€] ç­‰ä»·äºï¼š\n",
    "    # corpus = []\n",
    "    # for line in tokens:      # éå†æ¯ä¸€è¡Œ\n",
    "    #     for token in line:   # éå†è¡Œä¸­çš„æ¯ä¸€ä¸ªå­—ç¬¦\n",
    "    #         corpus.append(vocab[token]) # å°†å­—ç¬¦æŸ¥è¡¨è½¬æ¢ä¸ºæ•°å­—ç´¢å¼•\n",
    "    # [ä½œç”¨] æ ¸å¿ƒè½¬æ¢æ­¥éª¤ï¼šå°†äººç±»å¯è¯»çš„å­—ç¬¦ï¼ˆtokenï¼‰å…¨éƒ¨è½¬æ¢ä¸ºæœºå™¨å¯è¯»çš„æ•´æ•°ç´¢å¼•ï¼ˆindexï¼‰ï¼Œå¹¶å°†äºŒç»´åˆ—è¡¨å±•å¹³ä¸ºä¸€ç»´é•¿åˆ—è¡¨ã€‚\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    \n",
    "    # [è¯­æ³•] if æ¡ä»¶åˆ¤æ–­ã€‚\n",
    "    # [ä½œç”¨] æ£€æŸ¥æ˜¯å¦éœ€è¦æˆªæ–­æ•°æ®ã€‚é€šå¸¸ç”¨äºå¿«é€Ÿæµ‹è¯•æ¨¡å‹ï¼Œé¿å…æ•°æ®é‡å¤ªå¤§è·‘ä¸åŠ¨ã€‚\n",
    "    if max_tokens > 0:\n",
    "        # [è¯­æ³•] åˆ—è¡¨åˆ‡ç‰‡ (List Slicing) [:n]ã€‚\n",
    "        # [ä½œç”¨] ä¿ç•™ corpus åˆ—è¡¨çš„å‰ max_tokens ä¸ªå…ƒç´ ï¼Œä¸¢å¼ƒåé¢çš„ã€‚\n",
    "        corpus = corpus[:max_tokens]\n",
    "        \n",
    "    # [è¯­æ³•] return è¯­å¥è¿”å›å…ƒç»„ã€‚\n",
    "    # [ä½œç”¨] å°†å¤„ç†å¥½çš„â€œè¯­æ–™åº“ç´¢å¼•åºåˆ—â€å’Œâ€œè¯è¡¨å¯¹è±¡â€ä¸€èµ·è¿”å›ï¼Œä¾›åç»­æ¨¡å‹è®­ç»ƒä½¿ç”¨ã€‚\n",
    "    return corpus, vocab\n",
    "\n",
    "# ================= è°ƒç”¨éƒ¨åˆ† =================\n",
    "\n",
    "# [è¯­æ³•] å‡½æ•°è°ƒç”¨ + å…ƒç»„è§£åŒ… (Tuple Unpacking)ã€‚\n",
    "# [ä½œç”¨] æ‰§è¡Œæ•°æ®åŠ è½½ï¼Œå°†è¿”å›çš„ä¸¤ä¸ªç»“æœåˆ†åˆ«èµ‹å€¼ç»™å˜é‡ corpus å’Œ vocabã€‚\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "\n",
    "# [è¯­æ³•] åˆ›å»ºä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸¤ä¸ª len() å‡½æ•°çš„è¿”å›å€¼ã€‚\n",
    "# [ä½œç”¨] æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼š\n",
    "# 1. len(corpus): è¯­æ–™åº“çš„æ€»å­—ç¬¦æ•°ï¼ˆå³æ ·æœ¬æ€»é•¿åº¦ï¼‰ã€‚\n",
    "# 2. len(vocab): è¯è¡¨çš„å¤§å°ï¼ˆå³æœ‰å¤šå°‘ä¸ªä¸é‡å¤çš„å­—ç¬¦ç±»åˆ«ï¼‰ã€‚\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'd',\n",
       " 'j',\n",
       " 's',\n",
       " 'a',\n",
       " 'k',\n",
       " 'l',\n",
       " 'j',\n",
       " ' ',\n",
       " 's',\n",
       " 'j',\n",
       " 'd',\n",
       " 'k',\n",
       " 'l',\n",
       " 'j',\n",
       " 's',\n",
       " ' ',\n",
       " 's',\n",
       " 'j',\n",
       " 'd',\n",
       " 's',\n",
       " 'k',\n",
       " 'j',\n",
       " ' ',\n",
       " 's',\n",
       " 'j',\n",
       " 'd',\n",
       " 'k',\n",
       " ' ',\n",
       " 's',\n",
       " 'k',\n",
       " 'd',\n",
       " 'j',\n",
       " 'd',\n",
       " 's',\n",
       " 'k',\n",
       " ' ',\n",
       " 's',\n",
       " 'j',\n",
       " 'd',\n",
       " 'k',\n",
       " 's',\n",
       " 'j',\n",
       " ' ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ' djsaklj sjdkljs sjdskj sjdk skdjdsk sjdksj '\n",
    "list(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "2041.23px",
    "left": "45.9792px",
    "top": "56px",
    "width": "301.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
